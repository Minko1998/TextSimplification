{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57e61a9",
   "metadata": {},
   "source": [
    "Written by Mink Spronk (GitHub: Minko1998) for SimpleText @ CLEF 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"simpletext_task2_train.csv\")\n",
    "df.head()\n",
    "df_without_dups = df.drop_duplicates(subset = ['source_snt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable used to temporarily store count of simpletext_task2_test.csv\n",
    "word_count_test = word_count.copy()\n",
    "len(word_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_without_dups.source_snt.to_list()\n",
    "word_freq_line = []\n",
    "word_length = {}\n",
    "doc_freq = {}\n",
    "last_doc = 0\n",
    "word_count = {}\n",
    "for line in text:\n",
    "    line = line.split(' ')\n",
    "\n",
    "    for word in line:\n",
    "        # strip everything except hyphens\n",
    "        word = word.strip('., \"\" \\' \\' \\] \\[ \\} \\{ \\ ?')\n",
    "        word = word.lower()\n",
    "        if word not in doc_freq and line != last_doc:\n",
    "            doc_freq[word] = 1\n",
    "        else:\n",
    "            doc_freq[word] += 1\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "        \n",
    "        if len(word) not in word_length:\n",
    "            word_length[len(word)] = 1  \n",
    "        else:\n",
    "            word_length[len(word)] += 1\n",
    "#         if word == 'extensive':\n",
    "#             print('a')\n",
    "#             print(word_count['extensive'])\n",
    "    word_freq_line.append(word_count)\n",
    "    last_doc = line\n",
    "word_count = dict(sorted(word_count.items(), key=lambda item: item[1], reverse = True))\n",
    "word_length = dict(sorted(word_length.items(), key=lambda item: item[1], reverse = True))\n",
    "\n",
    "\n",
    "# word_length_norm = {k: v / total for total in (sum(word_length.values()),) for k, v in word_length.items()}\n",
    "# word_length_norm\n",
    "doc_freq\n",
    "word_count['extensive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "scores = Counter(word_count_test) + Counter(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a65437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "pos_score = {}\n",
    "for i, line in enumerate(df_without_dups.source_snt):\n",
    "    pos_line = sp(line)\n",
    "    line = line.split(' ')\n",
    "    for j, word in enumerate(line):\n",
    "        word = word.lower().strip('., \"\" \\' \\' \\] \\[ \\} \\{ \\ ?')\n",
    "        pos_word = pos_line[j].pos_\n",
    "        if word not in pos_score:\n",
    "            pos_score[word] = 0\n",
    "        \n",
    "        # Nouns are considered most complicated in general)\n",
    "        if pos_word == \"NOUN\":\n",
    "            pass\n",
    "        # Followed by verbs\n",
    "        elif pos_word == \"VERB\":\n",
    "            pos_score[word] += 2\n",
    "\n",
    "        # Ending with adjectives\n",
    "        elif pos_word == \"ADJ\":\n",
    "            pos_score[word] += 5\n",
    "        else:\n",
    "            # Since \n",
    "            pos_score[word] += 1\n",
    "\n",
    "# # adjusting for the times each word is used\n",
    "for word in pos_score:\n",
    "\n",
    "    # normalize pos_score over collection\n",
    "    pos_score[word] = pos_score[word] // word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e31a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "final_score = Counter(pos_score) + Counter(scores)\n",
    "# final_score\n",
    "# print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.displot(final_score, kind = 'kde')\n",
    "ax.set(title=\"Distribution of the final scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words to return\n",
    "run_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "algorithm = []\n",
    "counter = 0\n",
    "\n",
    "# iterate over all line in df\n",
    "for line in df_without_dups.iterrows():\n",
    "    \n",
    "    # set list with default incredibly high score\n",
    "    top_five = [('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000), ('a', 1000000)]\n",
    "    \n",
    "    # analyze any word in line\n",
    "    for word in line[1].source_snt.strip().split():\n",
    "        \n",
    "        # strip of all signs except hyphens\n",
    "        word = word.lower().strip('., \"\" \\' \\' \\] \\[ \\} \\{ \\ ?')\n",
    "        \n",
    "        \n",
    "        i = run_number\n",
    "        # if final_score word is lower than item in list\n",
    "        while final_score[word] < top_five[i-1][1] and i > 0:\n",
    "            i -= 1\n",
    "        \n",
    "        if i < run_number:\n",
    "            # insert at proper location\n",
    "            top_five.insert(i, (word, final_score[word])) \n",
    "            \n",
    "            # reduce length list to wanted size\n",
    "            top_five = top_five[:run_number]\n",
    "    counter = 1\n",
    "    guesses = []\n",
    "    \n",
    "    \n",
    "    for choice in top_five:\n",
    "        guesses.append(choice[0])\n",
    "    algorithm.append(guesses)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, item in enumerate(top_five):\n",
    "        if item[1] == 'a':\n",
    "            continue\n",
    "            \n",
    "        # we consider there to be no more complicated words\n",
    "        if item[1] > 600:\n",
    "            break\n",
    "        \n",
    "        # assign values to \n",
    "        item = {\"run_id\": \"UAms_TASK2_Automatic\", \"manual\": 1, \"snt_id\" : line[1].snt_id, 'term' : item[0], \"term_rank_snt\": i,\\\n",
    "               'score_5': 5 if item[1] < 250 else (4 if item[1] < 500 else(3 if item[1] < 1000 else 1)), 'score_3': 3 if item[1] < 300 else 2}\n",
    "        results.append(item)\n",
    "algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'UAms_TASK2_AutomaticRunAlgorithm{run_number}.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# results = []\n",
    "# for line in df_without_dups.iterrows():\n",
    "#     cnt2 = 0\n",
    "#     for j in random_lst[cnt]:\n",
    "#         item = {\"run_id\": \"UAms_TASK2_Automatic\", \"manual\": 1, \"snt_id\" : line[1].snt_id, 'term' : j, \"term_rank_snt\": cnt2,\\\n",
    "#                'score_5': 5, 'score_3': 3}\n",
    "#         results.append(item)\n",
    "#     cnt += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53916252",
   "metadata": {},
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5000226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(r\"C:\\Users\\mink-\\Documents\\huiswerk\\KI\\Scriptie\\TextSimplification\\dblp1.json\", 'rb') as f:\n",
    "#     file = json.load(f)\n",
    "final_score['decision-making']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c53b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_guessed_random = []\n",
    "half_correct_random = []\n",
    "\n",
    "for i, guesses in enumerate(random_lst):\n",
    "    for guess in guesses:\n",
    "        if guess not in right[i]:\n",
    "            isHalfCorrectRandom = False\n",
    "            for answer in right[i]:\n",
    "                words = answer.split()\n",
    "                if guess in words:\n",
    "                    half_correct_random.append(guess)\n",
    "                    isHalfCorrectRandom = True\n",
    "    if not isHalfCorrectRandom:\n",
    "        not_guessed_random.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7df8d",
   "metadata": {},
   "source": [
    "Baseline: Choosing randomly a set of words combining concepts to be inspected as complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "answers = pd.read_csv(\"simpletext_task2_decorated_run.csv\")\n",
    "answers_without_dups = answers.drop_duplicates(subset = ['snt_id'])\n",
    "# in this list the random unigrams are selected\n",
    "random_lst = []\n",
    "\n",
    "last_snt = df.snt_id[0]\n",
    "right = []\n",
    "last_snt_id = 0\n",
    "\n",
    "# \n",
    "for i in df.index:\n",
    "    result = set()\n",
    "\n",
    "    # if it is a different paper\n",
    "    if df.snt_id[i] != last_snt_id:\n",
    "        \n",
    "        # add the right answers to a list called 'right'\n",
    "        right_answers = answers.loc[answers['snt_id'] == df.snt_id[i]].term.to_list()\n",
    "        right_answers = [answer.lower() for answer in right_answers]\n",
    "        right.append(right_answers)\n",
    "        \n",
    "        # guess random words from the source_snt\n",
    "        for j in range(10):\n",
    "            result.add(random.choice(df.source_snt[i].lower().split()).strip('\\{ ,.\\[ \\] \\}'))\n",
    "        random_lst.append(result)\n",
    "        \n",
    "    last_snt_id = df.snt_id[i]\n",
    "# random_lst\n",
    "# answers.iloc[-3:]\n",
    "# len(df_without_dups)\n",
    "# print(len(random_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show which words are not picked up by the algorithm\n",
    "\n",
    "not_guessed_algo = []\n",
    "for line in right:\n",
    "    for word in line:\n",
    "        if word not in correct:\n",
    "            words = []\n",
    "            if len(word.split(\" \")) > 1:\n",
    "                words.extend(word.split(\" \"))\n",
    "                for _ in words:\n",
    "                    if _ not in half_correct:\n",
    "                        not_guessed_algo.append(word)\n",
    "print(len(not_guessed_algo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e044d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pos_score['sequencing'])\n",
    "# print()\n",
    "# print(algorithm[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9493366",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "half_correct = []\n",
    "# iterate over the guesses of the algorithm\n",
    "print(len(algorithm))\n",
    "for i, item in enumerate(algorithm):\n",
    "    # foreach guess\n",
    "    for guess in item:\n",
    "        print(guess)\n",
    "        # compare word with the answersheet\n",
    "        if guess[0] in right[i]:\n",
    "            correct.append(guess[0])\n",
    "            \n",
    "        gesplit = []\n",
    "        for a in right[i]:\n",
    "            if len(a.split(' ')) > 1:\n",
    "                gesplit.extend(a.split(' '))\n",
    "        if guess[0] in gesplit:\n",
    "            half_correct_algo.append(guess[0])\n",
    "\n",
    "for i in right:\n",
    "    for a in i:\n",
    "        if a not in correct:\n",
    "            pass\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = 0\n",
    "unigram = []\n",
    "for a in right:\n",
    "    for b in a:\n",
    "        if isinstance(b, str):\n",
    "            if len(b.split(' ')) == 1:\n",
    "                unigrams += 1\n",
    "                unigram.append(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dacf53",
   "metadata": {},
   "source": [
    "Accuracy calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_lst contains the random guesses for complicated words\n",
    "# algorithm consists the algorithm's guesses\n",
    "# right is the list with the correct words\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "amount_multiple_words = []\n",
    "for line in right:\n",
    "    for word in line:\n",
    "        if len(word.split(' ')) > 1:\n",
    "            amount_multiple_words.append(word)\n",
    "\n",
    "overlap = []\n",
    "# guesses that actually were in the answerlist\n",
    "for i, element in enumerate(random_lst):\n",
    "#     print(right)\n",
    "    overlap.append(element.intersection(set(right[i])))\n",
    "flatten = list(chain(*overlap))\n",
    "print(f\"the amount of right guesses the random algorithm did is: {len(flatten)}\")\n",
    "# amount of guesses done by the algorithm\n",
    "tries = sum(len(a) for a in algorithm)\n",
    "print(f'the amount of guesses the algorithm did was: {tries}')\n",
    "# amount of answers in the answerlist\n",
    "answersheet = sum(len(a) for a in right)\n",
    "print(f'the amount answers in the answerslist is: {answersheet}')            \n",
    "print(f'the amount of words that consist of multiple words is: {len(amount_multiple_words)}')\n",
    "print(f'the amount of right guesses by the algorithm was: {len(correct)}')\n",
    "print(f'in {len(half_correct_algo )} cases the algorithm only picked up one word of a multiple words concept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_length = {}\n",
    "for answers in right:\n",
    "    for answer in answers:\n",
    "        answer = answer.split()\n",
    "        for word in answer:\n",
    "            if len(word) not in answer_length:\n",
    "                answer_length[len(word)] = 1\n",
    "            else:\n",
    "                answer_length[len(word)] += 1\n",
    "answer_length_norm = {k: v / total for total in (sum(answer_length.values()),) for k, v in answer_length.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = .4\n",
    "rect1 = plt.bar(word_length_norm.keys(), word_length_norm.values(), width = width)\n",
    "rect2 = plt.bar([a + width for a in answer_length_norm.keys()], answer_length_norm.values(), width = width)\n",
    "# rects1 = ax.bar(ind, menMeans, width, color='royalblue', yerr=menStd)\n",
    "plt.title(\"Distribution of the amount of letters per word\")\n",
    "plt.ylim(0,.2)\n",
    "plt.xlim(0,20)\n",
    "plt.xticks([0,4,8,12,16,20])\n",
    "plt.yticks([.1,.2], ['10%', '20%'])\n",
    "plt.legend((rect1, rect2), ('all words', 'correct words'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c971a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually comparing the output of the algorithm vs. the answersheet\n",
    "for i in range(len(algorithm)):\n",
    "    print(f'answer: {right[i]}')\n",
    "    print(' vs. guess: [', end = '')\n",
    "    for j in range(3):\n",
    "        \n",
    "        print(f'{algorithm[i][j]}, ', end = '')\n",
    "    print(']')\n",
    "len(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unigrams)\n",
    "\n",
    "# F1 score calculations\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "items_random = sum(len(a) for a in random_lst)\n",
    "items_algo = sum(len(a) for a in algorithm)\n",
    "items_right = sum(len(a) for a in right)\n",
    "\n",
    "# the not_guessed of the algorithm\n",
    "items_half_correct = len(half_correct_algo)\n",
    "items_not_guessed_algo = len(not_guessed_algo)\n",
    "\n",
    "# the not_guessed of the random\n",
    "items_not_guessed_random = len(not_guessed_random)\n",
    "\n",
    "# scores_random = f1_score()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8bb362d9a07b19d5fc33e341507d9e6d06a3189cb624271d17c74198151cd44"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
