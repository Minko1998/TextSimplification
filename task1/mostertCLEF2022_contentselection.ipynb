{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nWritten by Femke Mostert (GitHub: Nademaaltijd) for SimpleText @ CLEF 2022\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "Written by Femke Mostert (GitHub: Nademaaltijd) for SimpleText @ CLEF 2022\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import packages \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import JSON file with topics and queries \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import json file\n",
    "f = open('SP12022topics.json')\n",
    "query_data = json.load(f)\n",
    "\n",
    "# convert file to normalised dataframe \n",
    "df = pd.json_normalize(query_data)\n",
    "\n",
    "# extract relevant data and combine\n",
    "topic = df['topic_id']\n",
    "query_id = df['query_id']\n",
    "query_txt = df['query_text']\n",
    "\n",
    "queries = zip(topic, query_id, query_txt)\n",
    "queries = list(queries)\n",
    "\n",
    "\n",
    "# testing query text extraction \n",
    "#for i in range(len(queries)):\n",
    "    #print(queries[i][2])\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 694: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8624/1855325011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'flattexttopics.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 694: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import JSON file with article texts, indexed with G01, T01 etc \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "f = open('flattexttopics.json')\n",
    "topics = json.load(f)\n",
    "\n",
    "print(topics)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the twentieth century, it became clear that national economies can make progress only by investing in human capital. Knowledge increases human productivity and creativity, and society can prosper only through human self awareness, understanding of the world around them and by increasing the quality of their lives. In the information age, progress was made through information and technology, with particular emphasis on critical thinking. But in the conceptual economy of the 21st century, influenced by three key developments; over-supply, outsourcing, and automation, it has become clear that critical thinking, even with the help of high technology, is no longer sufficient to make a living or to be competitive in the labor market. In the u0027Value age,u0027 these capabilities need to be complemented by highly conceptual and highly sensitive traits. The integration of critical and creative thinking is a fundamental factor in personal productivity and the essential condition for achieving sustainable economic development. The self assessment of Algebra University College students was designed to investigate the extent to which students integrated critical and creative thinking into their own circumstances and to explore whether there is a correlation between the type of thinking they are more inclined to and their study programs.  \n",
      "\n",
      "The predicted exponential growth of mobile data traffic demands suitable solutions for coping with the high amount of data. Increasing number of Wi-Fi enabled smartphones and Wi-Fi access becoming more widely available is opening the possibility of offloading a large chunk of data traffic from 3G/ 4G mobile networks to Wi-Fi IEEE 802.11. However, the quality of experience must be preserved no matter which network layer is currently engaged. Therefore, the scope of this paper was to determine the usability of the free of charge Wi-Fi available networks and to compare it with the legacy 3G\\4G networks.  \n",
      "\n",
      "This paper explains appliance of social network analysis and data visualization techniques in analysis of information propagation. Context of information (news) propagation through social network is an extremely dynamic and complex area to study. Due to topic actuality and a very small number of works on the similar topic this paper required a comprehensive and systematic approach. Thus, for practical reasons this work is based on the usage of Social Network Analysis (SNA) and visualization of social networking data obtained through Facebook covering 145 + public pages linked to 2.6 million fans. The main hypothesis is based on the premise whether is possible to find any similarities between the real-life social, economic and political entities/processes and online information propagation. The process consists of the development of the underlying model, the retrieval of data, data processing and consequential analysis u0026 visualization which has been elaborated in detail along with the comments related to the methods of application.  \n",
      "\n",
      "We construct an explicit minimal strong Grobner basis of the ideal of vanishing polynomials in the polynomial ring over Z/m for mu003e=2. The proof is done in a purely combinatorial way. It is a remarkable fact that the constructed Grobner basis is independent of the monomial order and that the set of leading terms of the constructed Grobner basis is unique, up to multiplication by units. We also present a fast algorithm to compute reduced normal forms, and furthermore, we give a recursive algorithm for building a Grobner basis in Z/m[x\"1,x\"2,...,x\"n] along the prime factorization of m. The obtained results are not only of mathematical interest but have immediate applications in formal verification of data paths for microelectronic systems-on-chip.  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'guacamole.univ-avignon.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "The 'no-check-certificate' link as described in the GUI API documentation is used, \n",
    "which causes a warning. The DBLP data is still accessible despite the SSL warning. \n",
    "    \n",
    "There are several search parameters you can use: size, text queries, and boolean \n",
    "queries. Below are a few example queries from the API documentation, remove hash \n",
    "before query to use (make sure to only have one query active). \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# example of a text query\n",
    "query = {'q':'algebra', 'size':'4'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves doc with id 1494645067\n",
    "# query = {'q':'_id:1494645067', 'size':'1'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves docs which refer to 1584898773\n",
    "# query = {'q':'references:1584898773', 'size':'1'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves all documents which title \n",
    "# contains geometric or with a field of subject name equal to \"\n",
    "# Computer science\" and another including algebra but none equal to Graph\n",
    "# query = {'q':'(( fos.name: \"Computer science\" AND fos.name:*algebra* AND NOT fos.name:Graph ) OR title:*geometric* )', 'size':'10'}\n",
    "\n",
    "# request response from GUI API\n",
    "url = 'https://inex:qatc2011@guacamole.univ-avignon.fr/dblp1/_search?'\n",
    "corpus = requests.get(url, params=query, verify=False)\n",
    "dump = corpus.json()\n",
    "\n",
    "\n",
    "# extract abstracts from ElasticSearch hits\n",
    "hits = dump['hits']['hits']\n",
    "abstracts = []\n",
    "\n",
    "for hit in hits:\n",
    "    print(hit['_source']['abstract'], \"\\n\")\n",
    "\n",
    "\n",
    "#print('Data frame: \\n', df)\n",
    "\n",
    "#print('Dump: \\n', dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
