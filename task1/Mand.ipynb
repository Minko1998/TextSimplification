{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a78a987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mink-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'guacamole.univ-avignon.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "Written by Femke Mostert (GitHub: Nademaaltijd) for SimpleText @ CLEF 2022\n",
    "\n",
    "\n",
    "The 'no-check-certificate' link as described in the GUI API documentation is used, \n",
    "which causes a warning. The DBLP data is still accessible despite the SSL warning. \n",
    "    \n",
    "There are several search parameters you can use: size, text queries, and boolean \n",
    "queries. Below are a few example queries from the API documentation, remove hash \n",
    "before query to use (make sure to only have one query active). \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# example of a text query\n",
    "query = {'q':'genetic algorithm', 'size':'100'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves doc with id 1494645067\n",
    "# query = {'q':'_id:1494645067', 'size':'1'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves docs which refer to 1584898773\n",
    "# query = {'q':'references:1584898773', 'size':'1'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves all documents which title \n",
    "# contains geometric or with a field of subject name equal to \"\n",
    "# Computer science\" and another including algebra but none equal to Graph\n",
    "# query = {'q':'(( fos.name: \"Computer science\" AND fos.name:*algebra* AND NOT fos.name:Graph ) OR title:*geometric* )', 'size':'10'}\n",
    "\n",
    "url = 'https://inex:qatc2011@guacamole.univ-avignon.fr/dblp1/_search?'\n",
    "with requests.get(url, params=query, verify=False) as f:\n",
    "    dump = f.json()\n",
    "# convert entries to dataframe (NOT FINISHED, NEED TO FLATTEN NESTED LIST)\n",
    "\n",
    "df = pd.DataFrame(dump)\n",
    "# print('Data frame: \\n', df)\n",
    "papers = dump['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13210c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1978514531  It has been shown many times that the evolutionary online learning XCS classifier system is a robustly generalizing reinforcement learning system, which also yields highly competitive results in data mining applications. The XCSF version of the system is a real-valued function approximation system, which learns piecewise overlapping local linear models to approximate an iteratively sampled function. While the theory on the binary domain side goes as far as showing that XCS can PAC learn a slightly restricted set of k-DNF problems, theory for XCSF is still rather sparse. This paper takes the theory from the XCS side and projects it onto the real-valued XCSF domain. For a set of functions, in which fitness guidance is given, we even show that XCSF scales optimally with respect to the population size, requiring only a constant overhead to ensure that the evolutionary process can locally optimize the evolving structures. Thus, we provide foundations concerning scalability and resource management for XCSF. Furthermore, we reveal dimensions of problem difficulty for XCSF - and local linear learners in general - showing how structural alignment, that is, alignment of XCSFu0027s solution representation to the problem structure, can reduce the complexity of challenging problems by orders of magnitude. \n",
      "1: 2125861761  This paper presents an approach to analyze population evolution in classifier systems using a symbolic representation. Given a sequence of populations, representing the evolution of a solution, the method simplifies the classifiers in the populations by reducing them to their \"canonical form\". Then, it extracts all the subexpressions that appear in all the classifier conditions and, for each subexpression, it computes the number of occurrences in each population. Finally, it computes the trend of all the subexpressions considered. The expressions which show an increasing trend through the course of evolution are viewed as building blocks that the system has used to construct the solution. \n",
      "2: 2545219809  We break the long standing cubic time bound of \\(O(n^3)\\) for the Minimum Weight Polygon Triangulation problem by showing that the well known dynamic programming algorithm, reported independently by Gilbert and Klincsek, can be optimized with a faster algorithm for the \\((min,+)\\)-product using look-up tables. In doing so, we also show that the well known Floyd-Warshall algorithm can be optimized in a similar manner to achieve a sub-cubic time bound for the All Pairs Shortest Paths problem without having to resort to recursion in the semi-ring theory. \n",
      "3: 2470547003  We study the precoding for the high-rank MIMO in the LTE-A Pro systems. Unlike the low-rank precoding, layer mapping has a relatively large impact on the performance for high-rank precoding. First, we construct a model on the relationship between layer mapping and precoding. Then we derive the system throughput with the resulting model, so that the performance of the layer mapping and precoding can be evaluated. Further, we operate a sub-space optimization for the codebook-based precoding to maximize the system throughput. Simulation results show that after optimizing layer mapping, high-rank precoding achieves much better performance. \n",
      "4: 2810905956  In this work, we present an adaptive motion planning approach for impedance-controlled robots to modify their tasks based on human physical interactions. We use a class of parameterized time-independent dynamical systems for motion generation where the modulation of such parameters allows for motion flexibility. To adapt to human interactions, we update the parameters of our dynamical system in order to reduce the tracking error (i.e., between the desired trajectory generated by the dynamical system and the real trajectory influenced by the human interaction). We provide analytical analysis and several simulations of our method. Finally, we investigate our approach through real world experiments with a 7-DOF KUKA LWR 4+ robot performing tasks such as polishing and pick-and-place. \n",
      "5: 2172121104  This paper presents a simple real-coded estimation of distribution algorithm (EDA) design using x-ary extended compact genetic algorithm (XECGA) and discretization methods. Specifically, the real-valued decision variables are mapped to discrete symbols of user-specified cardinality using discretization methods. The XECGA is then used to build the probabilistic model and to sample a new population based on the probabilistic model. The effect of alphabet cardinality and the selection pressure on the scalability of the real-coded ECGA (rECGA) method is investigated. The results show that the population size required by rECGA-to successfully solve a class of additively- separable problems-scales sub-quadratically with problem size and the number of function evaluations scales sub-cubically with problem size. The proposed rECGA is simple, making it amenable for further empirical and theoretical analysis. Moreover, the probabilistic models built in the proposed real- coded ECGA are readily interpretable and can be easily visualized. The proposed algorithm and the results presented in this paper are first step towards conducting a systematic analysis of real-coded EDAs and towards developing a design theory for development of scalable and robust real-coded EDAs. \n",
      "6: 2405328360  Music classification helps to manage song collections, recommend new music, or understand properties of genres and substyles. Until now, the corresponding approaches are mostly based on less interpretable low-level characteristics of the audio signal, or on metadata, which are not always available and require high efforts for filtering the relevant information. A listener-friendly approach may rather benefit from high-level and meaningful characteristics. Therefore, we have designed a set of high-level audio features, which is capable to replace the baseline low-level feature set without a significant decrease of classification performance. However, many common classification methods change the original feature dimensions or create complex models with lower interpretability. The advantage of the fuzzy classification is that it describes the properties of music categories in an intuitive, natural way. In this work, we explore the ability of a simple fuzzy classifier based on high-level features to predict six music genres and eight styles from our previous studies. \n",
      "7: 1588279470  Recently, we presented a new practical method for upward crossing minimization [6], which clearly outperformed existing approaches for drawing hierarchical graphs in that respect. The outcome of this method is an upward planar representation (UPR), a planarly embedded graph in which crossings are represented by dummy vertices. However, straight-forward approaches for drawing such UPRs lead to quite unsatisfactory results. In this paper, we present a new algorithm for drawing UPRs that greatly improves the layout quality, leading to good hierarchal drawings with few crossings. We analyze its performance on well-known benchmark graphs and compare it with alternative approaches. \n",
      "8: 1995439618  In this work the modeling and calibration method of reciprocity error in a coherent TDD coordinated multi-point (CoMP) joint transmission (JT) system are addressed. The modeling includes parameters such as amplitude gains and phase differences of RF chains between the eNBs. The calibration method used for inter-cell antenna calibration is based on precoding matrix indicator (PMI) feedback by UE. Furthermore, we provide some simulation results for evaluating the performance of the calibration method in different cases such as varying estimation-period, cell-specific reference signals (CRS) ports configuration, signal to noise ratio (SNR), phase difference, etc. The main conclusion is that the proposed method for inter-cell antenna calibration has good performance for estimating the residual phase difference. \n",
      "9: 2135482966  In this paper, we present a novel algorithm to evaluate the quality of ECG recordings. Our algorithm is designed to help clinicians in rapid selection of good quality ECG segments from long recordings collected by an ECG monitoring device such as a 12-lead bedside monitor. With some adjustments, we used the Computing in Cardiology Challenge 2011 database in order to compare the performance of our algorithm to the published results. The challenge was aimed to develop near real-time algorithms in mobile phones and provide feedback on quality of the ECGs for interpretation to the users who are mostly laypersons with little knowledge of ECG interpretation. Our algorithm generates a noise score which is a combination of two parameters: a high-frequency noise measure which accounts for the muscle noise and other fast changing artifacts, and a baseline wander noise measure quantifying the low-frequency noise. The training dataset (set A) with reference quality assessments was used to determine an optimum threshold on the ROC curve for classification of acceptable and unacceptable segments. The algorithm was then evaluated on the test dataset (set B) with undisclosed annotations. Our method achieved maximum accuracy of 93.9% on the training dataset and an accuracy of 90.2% on the test dataset, placing itself among the top 10 performers who participated in the challenge. \n",
      "10: 1543999319  In large and continuous state-action spaces reinforcement learning heavily relies on function approximation techniques. Tile coding is a well-known function approximator that has been successfully applied to many reinforcement learning tasks. In this paper we introduce the hyperplane tile coding, in which the usual tiles are replaced by parameterized hyperplanes that approximate the action-value function. We compared the performance of hyperplane tile coding with the usual tile coding on three well-known benchmark problems. Our results suggest that the hyperplane tiles improve the generalization capabilities of the tile coding approximator: in the hyperplane tile coding broad generalizations over the problem space result only in a soft degradation of the performance, whereas in the usual tile coding they might dramatically affect the performance. \n",
      "11: 1909867152  The complete design of a circuit typically includes the tasks of creating the circuitu0027s placement and routing as well as creating its topology and component sizing. Design engineers perform these four tasks sequentially. Each of these four tasks is, by itself, either vexatious or computationally intractable. This paper describes an automatic approach in which genetic programming starts with a high-level statement of the requirements for the desired circuit and simultaneously creates the circuitu0027s topology, component sizing, placement, and routing as part of a single integrated design process. The approach is illustrated using the problem of designing a 60 decibel amplifier. The fitness measure considers the gain, bias, and distortion of the candidate circuit as well as the area occupied by the circuit after the automatic placement and routing. \n",
      "12: 1602765982  Many practical applications for drawing graphs are modeled by directed graphs with domain specific constraints. In this paper, we consider the problem of drawing directed hypergraphs with (and without) port constraints, which cover multiple real-world graph drawing applications like data flow diagrams and electric  :[44],\"existing algorithms for drawing hypergraphs with port constraints are adaptions of the framework originally proposed by Sugiyama et al. in 1981 for simple directed graphs. Recently, a practical approach for upward crossing minimization of directed graphs based on the planarization method was proposed [7]. With respect to the number of arc crossings, it clearly outperforms prior (mostly layering-based) approaches. We show how to adopt this idea for hypergraphs with given port constraints, obtaining an upward-planar representation (UPR) of the input hypergraph where crossings are modeled by dummy  we :[132],\"present the new problem of computing an orthogonal upward drawing with minimal number of crossings from such an UPR, and show that it can be solved efficiently by providing a simple method. \n",
      "13: 2117411467  Recent research on evolutionary multiobjective optimization has mainly focused on Pareto fronts. However, we state that proper behavior of the utilized algorithms in decision/search space is necessary for obtaining good results if multimodal objective functions are concerned. Therefore, it makes sense to observe the development of Pareto sets as well. We do so on a simple, configurable problem, and detect interesting interactions between induced changes to the Pareto set and the ability of three optimization algorithms to keep track of Pareto fronts. \n",
      "14: 1509009144  A directed acyclic graph is upward planar if it allows a drawing without edge crossings where all edges are drawn as curves with monotonously increasing y-coordinates. The problem to decide whether a graph is upward planar or not is NP-complete in general, and while special graph classes are polynomial time solvable, there is not much known about solving the problem for general graphs in practice. The only attempt so far was a branch-and-bound algorithm over the graphu0027s triconnectivity structure which was able to solve sparse  :[85],\"this paper, we propose a fundamentally different approach, based on the seemingly novel concept of ordered embeddings. We carefully model the problem as a special SAT instance, i.e., a logic formula for which we check satisfiability. Solving these SAT instances allows us to decide upward planarity for arbitrary graphs. We then show experimentally that :[85],\"this approach seems to dominate the known alternative approaches and is able to solve traditionally used graph drawing benchmarks effectively. \n",
      "15: 2109366179  Vehicle detection and tracking is essential in traffic surveillance and traffic flow optimization. However, occlusion or overlapped vehicle tracking is difficult and remain a challenging research topic in image processing. In this paper, a conventional Markov Chain Monte Carlo (MCMC) is enhanced via Cumulative Sum (CUSUM) path plot in order to track vehicles in overlapping situation. By calculating the hairiness of CUSUM path plot, MCMC can be diagnosed as converged based on its sampling outputs. Varying sample size of MCMC provides enhancement to the tracking performance and capability of overcoming the limitation of conventional fix sample size algorithm. In addition, implementation of m-th order prior probability distribution and fusion of color and edge distance likelihood have further improved the tracking accuracy. MCMC with fixed sample size and CUSUM path plot are implemented and their corresponding performances are analyzed. Experimental results show that MCMC with CUSUM path plot has better performance where it is able to track the overlapped vehicle accurately with lesser processing time. \n",
      "16: 2260706855  In this paper, we present a parallel algorithm for the maximum convex sum (MCS) problem, which is a generalization of the maximum subarray (MSA) problem. In the MSA problem, we find a rectangular portion within the given data array that maximizes the sum in it. The MCS problem is to find a convex shape instead of a rectangular shape that maximizes the sum. For the MSA problem, O(n) time parallel algorithms are already known on an (n, n) 2D array of processors. For the MCS problem, we achieve the same time bound of O(n) on an (n, n) 2D array of processors. We provide rigorous proofs for the correctness of our parallel algorithm based on Hoare logic and also provide some experimental results of our algorithm that are gathered from the Blue Gene/P super computer. \n",
      "17: 1606852530  Recent works in evolutionary multiobjective optimization suggest to shift the focus from solely evaluating optimization success in the objective space to also taking the decision space into account. They indicate that this may be a) necessary to express the users requirements of obtaining distinct solutions (distinct Pareto set parts or subsets) of similar quality (comparable locations on the Pareto front) in real-world applications, and b) a demanding task for the currently most commonly used algorithms.We investigate if standard EMOA are able to detect and preserve equivalent Pareto subsets and develop an own special purpose EMOA that meets these requirements reliably. \n",
      "18: 2786439742  When estimating the state of a quadrotor, information about the noise, especially the noise covariances are necessary. In this paper, the autocovariance least-square (ALS) method is applied to identify the process and measurement noise covariance in the model of the quadrotor. The identified covariances are used in the EKF program to get the filtered values of the state variables. The simulation results show that the filtered values are close to the true values. \n",
      "19: 2026818368  This paper proposes a model for image representation and image analysis using a multi-layer neural network, which is rooted in the human vision system. Having complex neural layers to represent and process information, the biological vision system is far more efficient than machine vision system. The neural model simulate non-classical receptive field of ganglion cell and its local feedback control circuit, and can represent images, beyond pixel level, self-adaptively and regularly. The results of experiments, rebuilding, distribution and contour detection, prove this method can represent image faithfully with low cost, and can produce a compact and abstract approximation to facilitate successive image segmentation and integration. This representation schema is good at extracting spatial relationships from different components of images and highlighting foreground objects from background, especially for nature images with complicated scenes. Further it can be applied to object recognition or image classification tasks in future. \n",
      "20: 38932088  The features of network agricultural information (NAI) are summarized by analyzing Chinese typical, famous agricultural information websites. The features include the storage-scattered of agricultural informations, high frequency of updating, non-uniform data format and a lots of raw information existing. It is pointed out that the features have disadvantages for sharing and exploiting the NAI efficiently. Then a scheme is proposed to try to overcome the disadvantages of the features, which is mainly based on brower/server structure and information extraction technology from webpages. Also the key technologies to implement the scheme are described in the paper. In the end, an example application of the scheme is carried out to demonstrate the concrete steps to develop practical applications based on the scheme. network agricultural information, information extraction technology, brower/server structure \n",
      "21: 1532998712  This paper describes how the process of synthesizing the design of both the topology and the numerical parameter values (tuning) for a controller can be automated by using genetic programming. Genetic programming can be used to automatically make the decisions concerning the total number of signal processing blocks to be employed in a controller, the type of each block, the topological interconnections between the blocks, and the values of all parameters for all blocks requiring parameters. In synthesizing the design of controllers, genetic programming can simultaneously optimize prespecified performance metrics (such as minimizing the time required to bring the plant output to the desired value), satisfy time-domain constraints (such as overshoot and disturbance rejection), and satisfy frequency domain constraints. Evolutionary methods have the advantage of not being encumbered by preconceptions that limit its search to well-traveled paths. Genetic programming is applied to an illustrative problem involving the design of a controller for a three-lag plant with a significant (five-second) time delay in the external feedback from the plant to the controller. A delay in the feedback makes the design of an effective controller especially difficult. \n",
      "22: 2087486594  \n",
      "23: 2149107903  Traffic congestions often occur within the entire traffic network of the urban areas due to the increasing of traffic demands by the outnumbered vehicles on road. The problem may be solved by a good traffic signal timing plan, but unfortunately most of the timing plans available currently are not fully optimized based on the on spot traffic conditions. The incapability of the traffic intersections to learn from their past experiences has cost them the lack of ability to adapt into the dynamic changes of the traffic flow. The proposed Q-learning approach can manage the traffic signal timing plan more effectively via optimization of the traffic flows. Q-learning gains rewards from its past experiences including its future actions to learn from its experience and determine the best possible actions. The proposed learning algorithm shows a good valuable performance that able to improve the traffic signal timing plan for the dynamic traffic flows within a traffic network. \n",
      "24: 2119085972  The study of isolated populations is a promising approach for the identification of genes conferring susceptibility to disease. Due to the complex genealogies of such populations epidemiological studies are challenging.We present ongoing research to using a visual analytics based approach for the identification of diseases that cluster in families, risk factors and heritability patterns. \n",
      "25: 2021609393  We present a multi-player mobile game that employs fully automated music feature extraction to create ‘levels’ and thereby produce game content procedurally. Starting from a pool of songs (and their features), a self-organizing map is used to organize the music into a hexagonal board so that each field contains a song and one of three minigames which can then be played using the song as background and content provider. The game is completely asynchronous: there are no turns, players can start and stop to play anytime. A preference-learning style experiment investigates whether the user is able to discriminate levels that match the music from randomly chosen ones in order to see if the user gets the connection, but at the same time, the levels do not get too predictable. \n",
      "26: 1488445023  Several ECG features are common electrocardiographic markers for manual interpretation of early repolarization (ER) and acute pericarditis (PCARD), both confounders for acute myocardial infarction (AMI). We hypothesized these features could improve automated AMI detection in the presence of ER and PCARD. \n",
      "27: 2136809102  In this paper, a new algorithm for Automatic License Plate Localisation and Recognition (ALPR) is proposed on the basis of isotropic dilation that can be achieved using the binary image Euclidean distance transform. In a blob analysis problem, any two Region of Interest (RoIs) that is discontinuous are typically treated as separate blobs. However, the proposed algorithm combine with Connected Component Analysis (CCA) are coded to seek for RoI within a certain distance of other RoI to be treated as non-unique. This paper investigates the design and implementation of several pre-processing techniques and isotropic dilation algorithm to classify moving vehicles with different backgrounds and varying angles. A multi-layer feed-forward back-propagation Neural Network is used to train the segmented and refined characters. The results obtained can be used for implementation in the vehicle parking management system. \n",
      "28: 2592736747  Parameters extracted from ECG recordings show different noise tolerance levels. Some parameters may be slightly affected by noise, while the others could be inaccurate. Choosing a single noise threshold for all parameters may lead to adoption of invalid results or removal of valid parameters. In this study, we develop a statistical model between the Signal-to-Noise Ratio (SNR) and our Signal Quality Indicator (SQI) algorithm, and determine the noise-tolerance threshold for several ECG parameters statistically. Our dataset was based on the STAFF-III database with added physical noise recording segments from MIT-BIH database containing electrode motion, muscle artifact and baseline wander. We generated 3193 noise-added 12-lead ECG signals with SNR varying from −6dB to 24dB. For each 10-second segment, the noise level was measured by our SQI algorithm, and ECG parameters were measured by the Philips DXL ECG algorithm, allowing us to derive a noise model and the thresholds for noise tolerance of the ECG parameters. Varying thresholds suggest using parameter-specific thresholds in order to avoid reduction in accuracy. \n",
      "29: 2164371031  Misconnection of ECG lead-wires can generate abnormal ECG and erroneous diagnosis. Existing methods for detecting lead-wire interchange were designed for ECG devices using conventional lead system. In this work we developed an automatic ECG cable interchange detection algorithm and compared the algorithm performance between conventional and Mason-Likar (ML) electrode placements. The algorithm was developed based on a decision tree classifier which uses beat morphology measurements that were obtained using Philips DXL ECG algorithm. The algorithm was evaluated for detecting limb cable interchanges on an independent database which included both conventional and ML ECG recordings for each subject (total 423 subjects). There was no statistically significant difference in terms of overall sensitivity and specificity. This morphology-based cable interchange detection algorithm showed similarly high performance for maintaining a low false positive rate for both lead systems. Therefore, in practice, the same algorithm may be used with either electrode placement without a need for a special configuration. \n",
      "30: 2046287992  Pre-fetching in a memory hierarchy is known to alleviate the “memory wall” paradigm but its use is impeded because of the difficulty to estimate efficiency when used in a complex system such as a SoC (System on Chip) or NoC (Network on Chip). Therefore, some methods are needed to evaluate the benefit of pre-fetching at the earliest possible stage in a design flow to help the designer choose architectural parameters or transform the application algorithm. In this paper we show that the emulation platform implementing the nD-AP Cache (n-Dimensional Adaptive and Predictive Cache) allows to perform a platform-independent measurement of this cache efficiency. The nD-AP Cache performs pre-fetching in multidimensional arrays which are commonly used in image processing and multimedia applications. The obtained metric can be used to extrapolate the cache performance in a much broader system configuration. The method to compute this metric is the calibration process. The performed benchmarks show that the calibration process is confident. Also, we measured that the nD-AP Cache is two times faster than a standard PowerPC 2-way set associative cache in the context of an image processing kernel. \n",
      "31: 2057088440  The shape, or contour, of an object is usually stable and persistent, so it is a good basis for invariant recognition. For this purpose, two problems must be addressed. The first is to obtain clean edges, and the second is to organize those edges into a structured data form upon which the necessary manipulations and analysis may be performed. Simple cells in the primary visual cortex are specialized in orientation detection, so the neural mechanism can be simulated by a computational model, which can produce a fairly clean set of lines, and all of them in vectors rather than in pixels. Then a line-context descriptor was designed to describe geometrical distribution of lines in a local area. All lines were also recorded by a weighted graph, and its minimum spanning tree can be used to describe the topological features of an object. An iterative matching algorithm was developed by combining line-context descriptors and minimum spanning tree, and was shown to match objects of the same type but with different shapes very well. Our results suggest that key to representation efficiency of searchable trees is to apply a mid-level line-context. This once more confirms the crucial role played by simple cells in visual processing path, for its preprocessing can greatly ease the subsequent processing. \n",
      "32: 2092575453  Business processes are milestone of the information system of any companies. Their availability is a crucial aspect. We provide a solution for the high level of availability of business processes by the use of cluster of enterprise service buses (ESB). Our approach is based on the dynamic creation of the route between the business services and the migration of a runtime context from one ESB to another one. So, we insure the management of business processes over a cluster and measure the impact of such incident. Through the use of log, we also report these events which allow the administrator for preparing updates of the information system. With the use of open source software, we guarantee the reuse of our case study with other kinds of enterprise service bus, which respect open standard exchanges like XML language and REST API. \n",
      "33: 2026805440  Abstract   Recent prospective studies have suggested that useru0027s confidence in computer-aided detection (CAD) marks and their tolerance of false positive (FP) marks play a role in the benefit gained from a CAD device. In this paper, we propose to (1) introduce new metrics to improve the characterization of the latest generation of CAD algorithms and (2) to describe the algorithm performance tradeoffs with user adjustable operating points. The performance of the CAD algorithm (V8.0, R2 Technology, Sunnyvale, CA) with user adjustable operating points was assessed on a multi-institutional database of 832 consecutive, biopsy-proven cancers detected by screening mammography and 345 clinically confirmed normal (four view) cases. The standard measurements of CAD performance were made—case sensitivity for microcalcifications and masses, and false positive (FP) marker rate on normal cases. In addition, new measurements of case specificity (the percentage of normal cases with no CAD marks) and two-view sensitivity (the percentage of cancer cases with a cancer visible (n=628) and marked on both views) were used. It is likely that the case specificity measure reflects the impact of the CAD algorithm on the workflow of the radiologist. Further, a high value for the two-view sensitivity is likely to increase the radiologistu0027s confidence in the CAD tool. This new CAD system allows the user to choose an operating point to best fit their clinical requirements. \n",
      "34: 2146225475  Nowadays, vehicle tracking is a vital approach to assist and improve the road traffic control, surveillance and security systems by having the detail of the captured vehicle information. In past, many tracking techniques have been implemented and suffered from the well known u0027occlusionu0027 problems. Increasing the accuracy of the tracking algorithm has caused the computational cost due to the inflexibility to adapt the partial and fully occluded situations. Besides occlusion, appearance of new objects and background noises in the captured videos increase the difficulties of continuously tracking the labelled vehicles. In this paper, an adaptive particle filter approach has been proposed as the tracking algorithm to solve the vehicle occlusion problem. In order to solve the common particle filter degeneracy problem, the proposed particle filter is equipped with the adaptive resampling algorithm which is capable of dealing with various occlusion incidents. The experimental results show that enhancement of the particle filter via resampling algorithm has been robustly tracking the vehicles, and significantly improve the accuracy in tracking the occluded vehicles without compromising the processing time. \n",
      "35: 2794629793  Due to a significant spectral overlap between the motion artifact and underlying photoplethysmogram (PPG), reliable automated PPG analysis in real-life environment may be challenging. To evaluate the impact of motion artifact on the accuracy of automated PPG pulse detection, we designed a noise stress test (NST) in which artifact-bearing (noise-added) recordings are assembled from actual recordings by selecting intervals that contain predominantly motion artifact. To assemble the NST database, we analyzed 2000 synchronized electrocardiogram (ECG) and PPG recordings from MIMIC-II database. One-minute segments with the highest and lowest agreement between the ECG beats and the PPG pulses were selected using a semi-automated protocol. The resulting NST database included 52 artifact-free base recordings by visually selecting clean segments with normal pulse rate and rhythm, and 10 pure artifact recordings by selecting segments with negligible spectral content from the base signal. Cross combination of the base and artifact recordings, by calibrating the level of added artifact, generated 520 one-minute PPG signals for each desired signal-to-noise ratio (SNR). For each combined signal, the performance of automatic pulse detection and time-domain pulse rate variability analysis was evaluated by using the annotations from artifact-free base recordings as reference. \n",
      "36: 2611050211  Bi-directional optical flow (so-called BIO) is part of Joint Exploration Model (JEM) which explores potential coding efficiency improvement over state-of-the-art video codec. BIO allows fine motion compensation on a sample level without additional signaling, since refinement is explicitly calculated using just texture information from both reference frames under assumption the validity of optical flow equation. BIO reduces BD-rate in average by more than 2% (up to 5% for some test video), but computational complexity is rather high. Two simplifications for BIO are studied in this paper. First is redesign chain for MC prediction and gradients calculation scheme. Simplified scheme has slightly high latency but reduces amount of multiplications in bi-predicted blocks by factor 2. Another simplification is clustering samples in order to perform motion refinement in BIO not per sample but for group of samples. This allows reduction of division operation in BIO by factor 9.7 in average (up to 256 times in largest blocks). Both modifications enabled together maintain the same performance for BIO in JEM while reduce encoding and decoding run-time significantly. \n",
      "37: 1997649845  The vision system of primates could process colorful scenes very efficiently. This is because, in biological retina, there are three types of cone cells and several types of ganglion cells that possess highly complicated receptive fields. The central and the surrounding areas of a receptive field are usually composed of different types of cones. Typically, they form two classes, namely the red-green opponency and the blue-yellow opponency. In order to develop a new representation schema for colorful images, we simulated some physiological mechanisms in retina, such as the opponent color theory. Based on anatomical and electrophysiological findings of ganglion cells, we proposed a bio-inspired color processing method. We designed a neural network simulating retinal ganglion cells (GCs) and their classical receptive fields (CRF), and also raised a dynamic procedure to control receptive fieldu0027s self-adjustment according to the characteristics of an image. A great number of experiments were conducted on natural images. The results showed that this new method could reserve crucial structural information of an image and suppress trivial information at the same time. Depending on these new representations, some upcoming processing, such as image segmentation, could be improved significantly. Image segmentation is very critical to ultimate image understanding. However, actual image stimuli are a little bit far from biological studies. Our work integrated them together and explained how the physiological opponent-color theory could facilitate image processing in real applications. \n",
      "38: 2026633184  Abstract   This paper presents a study of four parallel linear system solvers implemented using the Bulk Synchronous Parallel programming model. We show that better performances are achieved by the implicit methods due both to higher exploitation of hardware capabilities and to the lower communication costs. A BSP performance model of all the algorithms has been built and the predictions obtained have been compared with the results of measurements on two different parallel systems: a cluster of workstations and a shared memory multiprocessor machine. The implicit algorithms have proved to be from 20% to 40% faster than their traditional counterparts. The high accuracy of prediction makes the BSP programming paradigm an ideal framework for developing parallel algorithms. \n",
      "39: 1588879653  A mathematical formula containing one or more free variables is “general” in the sense that it provides a solution to an entire category of problems. For example, the familiar formula for solving a quadratic equation contains free variables representing the equation’s coefficients. Previous work has demonstrated that genetic programming can automatically synthesize the design for a controller consisting of a topological arrangement of signal processing blocks (such as integrators, differentiators, leads, lags, gains, adders, inverters, and multipliers), where each block is further specified (“tuned”) by a numerical component value, and where the evolved controller satisfies user-specified requirements. The question arises as to whether it is possible to use genetic programming to automatically create a “generalized” controller for an entire category of such controller design problems — instead of a single instance of the problem. This paper shows, for an illustrative problem, how genetic programming can be used to create the design for ‘both the topology and tuning of controller, where the controller contains a free variable. \n",
      "40: 2248976353  In this paper, the dynamic channel characteristics at 23.5 GHz in an indoor scenario are investigated according to measurement and deterministic simulation. In order to obtain accurate channel realizations, the ray tracing (RT) software is calibrated on both the power delay profile and the path levels. For the measurement data, the propagation paths are identified using the non-parametric peak detection algorithm. The cluster-alike behaviors of these paths and the influence of the antenna radiation pattern are also studied through the comparison with the RT simulated paths. Subsequently, the evolutionary traces of channel with regard to the user equipment’s movement are identified by associating the samples, which have the similar parameters at adjacent locations. The features of these traces are analyzed in both statistical and individual ways. Results show that the life durations of most traces are within 5 m. The line of sight and reflected paths with significant power survive longer than the others. These observations confirm the feasibility of designing adaptive beam tracking algorithms based on the spatial consistency of the dominant propagation paths. Moreover, high correlations among the variations of different parameters in the same trace are revealed. \n",
      "41: 1996360777  The shape or contour of an object is usually stable and persistent, so it is a good basis for invariant recognition. For this purpose, two problems must be handled. The first is obtaining clean edges and the other is organizing those edges into a structured form so that they can be manipulated easily. We apply a bio-inspired orientation detection algorithm because it can output a fairly clean set of lines, and all lines are in the form of vectors instead of pixels. This line representation is efficient. We decompose them into several slope-depended layers and then create a hierarchical partition tree to record their geometric distribution. Based on the similarity of trees, a rough classification of objects can be realized. But for an accuracy recognition, we design a moment-based measure to describe the detail layout of lines in a layer, and then re-describe image by Huu0027s moment invariants. The experimental results suggest that the representation efficiency enabled by simple cellu0027s neural mechanism and that applying multi-layered representation schema can simplify the complexity of the algorithm. This proves that line-context representation greatly eases subsequent shape-oriented recognition. \n",
      "42: 2152135248   :[0],\"increasing availability and diversity of omics data in the post-genomic era offers new perspectives in most areas of biomedical research. Graph-based biological networks models capture the topology of the functional relationships between molecular entities such as gene, protein and small compounds and provide a suitable framework for integrating and analyzing omics-data. The development of software tools capable of integrating data from different sources and to provide flexible methods to reconstruct, represent and analyze topological networks is an active field of research in bioinformatics. \n",
      "43: 2293281440  This paper presents a comparative study of classification performance in automatic audio chord recognition based on three chroma feature implementations, with the aim of distinguishing effects of frame size, instrumentation, and choice of chroma feature. Until recently, research in automatic chord recognition has focused on the development of complete systems. While results have remarkably improved, the understanding of the error sources remains lacking. In order to isolate sources of chord recognition error, we create a corpus of artificial instrument mixtures and investigate (a) the influence of different chroma frame sizes and (b) the impact of instrumentation and pitch height. We show that recognition performance is significantly affected not only by the method used, but also by the nature of the audio input. We compare these results to those obtained from a corpus of more than 200 real-world pop songs from The Beatles and other artists for the case in which chord boundaries are known in advance. \n",
      "44: 2122422997  This paper presents a monocular approach of partially recovering three-dimensional information from rectangles and circles with proper prior knowledge. Taking image-formation process as projective transform, we first work out the focal length of an image with a common perspective rectangle and further obtain normal vectors of the supporting planes of rectangles and circles and relative depths of each point on the planes with respect to camera coordinate system. The absolute depths and other information about lengths and distances are also acquirable with reasonable estimation or measurement of side lengths and radius. The experimental results suggest good feasibility and acceptable precision of our approach. \n",
      "45: 2303195069  This paper proposes a new way of managing the cache by exploiting the difference of behavior in the memory system between read-only data and read-write data. A division of the existing cache-based memory hierarchy is proposed in order to create a dedicated data path for read-only data. In order to justify this approach, an analysis performed on a set of benchmarks shows that read-only data count for significant part of the working set and are less reused than read-write data. A transparent solution is proposed based on specific compilation support to separate automatically the memory accesses of read-only data at L1-level. This organization exploits the properties of the different sub-workloads in order to increase the overall data locality and data reuse. Simulated in a multicore environment, the evaluation of the new memory organization shows reduction of L1 misses up to 28.5%. Moreover, the messages issued on the interconnection network can be reduced up to 14.7% without any penalty on the performance. \n",
      "46: 2127497482  Summary: An efficient tool for mining complex inbred genealogies that identify clusters of individuals sharing the same expected amount of relatedness is described. Additionally it allows for the reconstruction of sub-pedigrees suitable for genetic mapping in a systematic way. Availability: http://www.jenti.org Contact: mario.falchi@kcl.ac.uk A promising approach to dissect the genetics of complex traits is to focus on isolated populations with small number of founders. In these isolates the expected number of phenotype-influencing variants is likely to be reduced and the shared environment among individuals is more uniform compared with outbred populations (Wright et al. 1999; Peltonen et al 2000, Shifman and Darvasi 2001). The value of genetic isolates is often enriched by the availability of extensive historical and archival records that allow tracking the inheritance pattern of extant individuals through generations. Despite the potential advantages of exploiting the knowledge of these relationships to increase the efficiency of the studies, the study design and the statistical analysis should be carefully planned, keeping in mind the peculiarity of a sample of individuals mostly related to each other through multiple lines of descent. The main issues to deal with are the non-independence of subjects’ genotype in population-based designs with resulting biased association results due to linkage, and the complexity of these large pedigrees that often prohibits using them entirely in family-based designs. Here, we describe Jenti, a user-friendly tool that assists the user in the selection of sub-samples suitable for genetic studies based on genealogical information. Given the genealogical connections between two individuals a and b, their genetic relatedness might be described by the kinship coefficient (or coefficient of consanguinity) Φa,b representing the chance that a randomly chosen pair of alleles, one from each individual, is the inherited copy of the same ancestral allele (Malecot 1948). The kinship coefficient is related to the expected amount of alleles shared identical-by-descent among individuals in the genome (e.g. Glaubitz et al 2003). All pairwise connections between the individuals of a genealogy, expressed by the kinship coefficient, can be exploited to cluster optimal sub-group(s) of individuals whose members share a given range of genetic relatedness. The sub-sample of individuals sharing each with the other the lower degree of kinship is likely to be representative of the allele frequencies distribution in the population, since the bias due to the relationship among individuals is minimized. This sample would \n",
      "47: 2036003211   :[0],\"is a reference database of eukaryotic repetitive DNA, which includes prototypic sequences of repeats and basic information described in annotations. Repbase already has software for entering new sequence families and for comparing the useru0027s sequence with the database of consensus sequences. \n",
      "48: 2076911240  Summary: In order to obtain an accurate description of the protein interior, we describe a simple and fast algorithm that measures the depth of each atom in a protein (dpx), defined as its distance (˚ A) from the closest solvent accessible atom. The program reads a PDB file containing the atomic solvent accessibility in the B-factor field, and writes a file in the same format, where the Bfactor field now contains the dpx value. Output structure files can be thus directly displayed with molecular graphics programs like RASMOL, MOLMOL, Swiss-PDB View and colored according to dpx values. Availability: The algorithm is implemented in a standalone program written in C and its source is freely available at ftp.icgeb.trieste.it/pub/DPX or on request from the authors. \n",
      "49: 2914887876  Use of peak-picked ECG is generally not recommended for heart rate variability (HRV) calculations because the apparent R-wave peak moves around within the QRS due to the peak-picking operation. This study tests an algorithm for recovering high resolution RR intervals from peak-picked ECG. Two databases were used for testing, simulated ECG and Holter ECG from end stage renal disease (ESRD) patients on and off hemodialysis. Twenty minute samples of single lead ECG (n=1000) were generated by the Physionet HRV ECG simulator. Each ECG record had a random combination of heart rate standard deviation and additive noise. 12-lead, 48 hour Holter ECG from 51 ESRD patients was split into 20 minute segments for HRV analysis. ECG was decimated to 250sps from 1000sps and then peak-picked to the final sample rate of 125sps. High resolution RR interval recovery was based on up-sampling and template matching to align beats. HRV standard deviation of RR intervals (SDNN) was calculated for each 20 min. segment. Bland-Altman analysis was used to assess bias of SDNN error across the SDNN range. The bias of SDNN error at low SDNN values was pronounced for RR intervals defined by R-wave peaks. RR correction by template matching reduced (1) the impact of noise and (2) the SDNN error at low levels of SDNN. \n",
      "50: 2792249286  Hyperbolic geometry appears to be intrinsic in many large real networks. We construct and implement a new maximum likelihood estimation algorithm that embeds scale-free graphs in the hyperbolic space. All previous approaches of similar embedding algorithms require at least a quadratic runtime. Our algorithm achieves quasi-linear runtime, which makes it the first algorithm that can embed networks with hundreds of thousands of nodes in less than one hour. We demonstrate the performance of our algorithm on artificial and real networks. In all typical metrics, such as log-likelihood and greedy routing, our algorithm discovers embeddings that are very close to the ground truth. \n",
      "51: 2096142265  The Bayesian optimization algorithm (BOA) uses Bayesian networks to learn linkages between the decision variables of an optimization problem. This paper studies the influence of different selection and replacement methods on the accuracy of linkage learning in BOA. Results on concatenated m-k deceptive trap functions show that the model accuracy depends on a large extent on the choice of selection method and to a lesser extent on the replacement strategy used. Specifically, it is shown that linkage learning in BOA is more accurate with truncation selection than with tournament selection. The choice of replacement strategy is important when tournament selection is used, but it is not relevant when using truncation selection. On the other hand, if performance is our main concern, tournament selection and restricted tournament replacement should be preferred. These results aim to provide practitioners with useful information about the best way to tune BOA with respect to structural model accuracy and overall performance. \n",
      "52: 2183459402  Propagation channel model is crucial for the design of high-frequency communication systems. Measurement-based stochastic modeling approach which was commonly adopted in low frequencies is no more efficient or flexible due to the limitation of antenna gain and sounding efficiency. In this contribution, we propose a hybrid method which utilize both the measurements and ray-based deterministic simulations to study the channel characteristics in specified indoor environments. First, the ray-tracing tool is evaluated by using a small amount of measured data. And then we adopt the validated tool to generate massive samples of channel impulse responses. Fianlly, statistical models are constructed based on the proposed approach. The results show good compatibility with the recommended model suggested by METIS. Furthermore, the importance of diffuse scattering at high-frequency is also investigated. \n",
      "53: 2165663969  Summary: PedVizApi is a Java API (application program interface) for the visual analysis of large and complex pedigrees. It provides all the necessary functionality for the interactive exploration of extended genealogies. While available packages are mostly focused on a static representation or cannot be added to an existing application, PedVizApi is a highly flexible open source library for the efficient construction of visual-based applications for the analysis of family data. An extensive demo application and a R interface is provided. \n",
      "54: 1993991039  We present a new sorting algorithm, called Splitsort which adapts to existing order within the input sequence. The algorithm is optimal with respect to several known measures of presortedness, including the number of inversions, for which no such simple and space efficient algorithm was known before. The amount of extra space needed is only n + O(log n) pointers. Splitsort uses a simple data structure and is easy to code. In the worst case Splitsort performs 2.5 n log2n comparisons, but if the input is presorted according to some of the measures it completes the sorting task considerably faster. We also show how a variant of the algorithm can be implemented to run in-place. \n",
      "55: 2091874610  \n",
      "56: 2004849335  This paper presents a non-player character (NPC, bot) for the strategy game Diplomacy. The bot is able to communicate with other players and thus shows a human-like behavior. We investigate how far the playing abilities can be improved without corrupting the human-like behavior. Is there a trade-off at all or do these skills complement one another? Different versions of the bot are tested against other bots and humans which requires means to automatically measure believability. We derive such a measure after a general approach and apply it for monitoring the believability criterion while improving the playing strength of our bot. \n",
      "57: 92580448  We describe a system, ECStar, that outstrips many scaling aspects of extant genetic programming systems. One instance in the domain of financial strategies has executed for extended durations (months to years) on nodes distributed around the globe. ECStar system instances are almost never stopped and restarted, though they are resource elastic. Instead they are interactively redirected to different parts of the problem space and updated with up-to-date learning. Their non-reproducibility (i.e. single “play of the tape” process) due to their complexity makes them similar to real biological systems. In this contribution we focus upon how ECStar introduces a provocative, important, new paradigm for GP by its sheer size and complexity. ECStar’s scale, volunteer compute nodes and distributed hub-and-spoke design have implications on how a multi-node instance is managed. We describe the set up, deployment, operation and update of an instance of such a large, distributed and long running system. Moreover, we outline how ECStar is designed to allow manual guidance and re-alignment of its evolutionary search trajectory. \n",
      "58: 2148014043  The Gibbs phenomenon was recognized as early as 1898 by Michelson and Stratton. Gibbs oscillations occur during the reconstruction of discontinuous functions from a truncated periodic series expansion, such as a truncated Fourier series expansion or a truncated discrete Fourier transform expansion. Recent theoretical results have shown that Gibbs oscillations can be removed from the truncated Fourier series representation of a function that has discontinuities. This is accomplished by a change of basis to the set of orthogonal polynomials called the Gegenbauer polynomials. In this correspondence, a straightforward numerical procedure for the denoising of piecewise polynomial signals is developed. Examples using truncated Fourier series and discrete Fourier transform (DFT) series demonstrate the effectiveness of the numerical procedure. \n",
      "59: 1994169614  \n",
      "60: 2017158542  With the fast development of mobile technologies and wireless Internet access, video streaming share grows rapidly. New HEVC (High Efficiency Video Codec) standard was introduced by ITU-T in 2013. It increases compression rate and, in the same time, computational efforts. Thus development of efficient decoding systems for mobile platforms becomes actual problem. This paper is dedicated to analysis of possible issues which arise during design of the important decoderu0027s component - task scheduler. In particular, opportunities of HEVC decoding parallelization on mobile devices using GPGPU (General Purpose Graphics Processing Unit) technique are considered. \n",
      "61: 1905847227  Many, if not most, optimization problems have multiple objectives. Historically, multiple objectives have been combined ad hoc to form a scalar objective function, usually through a linear combination (weighted sum) of the multiple attributes, or by turning objectives into constraints. The genetic algorithm (GA), however, is readily modified to deal with multiple objectives by incorporating the concept of Pareto domination in its selection operator, and applying a niching pressure to spread its population out along the Pareto optimal tradeoff surface. We introduce the Niched Pareto GA as an algorithm for finding the Pareto optimal set. We demonstrate its ability to find and maintain a diverse \"Pareto optimal population\" on two artificial problems and an open problem in hydrosystems. u003e \n",
      "62: 2045961514  Abstract   In this paper we discuss two parallel forward chaining models together with their implementations on multiprocessor systems. The forward chaining models use a dynamic scheduling strategy, and are for a rule-based expert system. All the models are domain-independent. To support the use of these models, a ‘rulebase compiler’ has been built to translate a rule base in text format into the data structure needed by the system. \n",
      "63: 2109686016  Biological evolution endows human vision perception with an \"optimal\" or \"near optimal\" structure while facing a large variety of visual stimuli in different environment. Mathematical principles behind the sophisticated neural computing network facilitate these circuits to accomplish computing tasks sufficiently as well as at a relatively low energy consumption level. In other words, human visual pathway, from retina to visual cortex has met the requirement of \"No More Than Needed\" (NMTN). Therefore, properties of this \"nature product\" might cast a light on the machine vision. In this work, we propose a biological inspired computational vision model which represents one of the fundamental visual information -- orientation. We also analyze the efficiency trade-off of this model. \n",
      "64: 10366557  Advances in human genetics beg the integration of genomic/genetic data into clinical practice. The future will include the use of the detailed genetic characteristics of an individual to identify predispositions, to design prophylactic measures, and to provide the most effective treatments. However, in the present, and for patients and populations for whom the expense of genetic characterization will preclude its general use, a thorough family history of disease, of health-related traits, and of responses to treatments and medications could be used efficiently to guide medical decision  :[86],\"risks for many diseases, based on a positive family history, have been recognized for centuries. Positive family history remains one of the most significant risk factors for many diseases. However, the use of the family history to predict risk and to plan medical care is still sporadic at best. There is no health care system uniformly collecting such data, nor using it effectively to help guide medical  :[154],\"computerized family health history resource useful for such clinical application requires accurate and complete genealogical and medical data. The resources that might begin to allow such application exist today in two places in the world. More than 30 years ago, and almost simultaneously in the Utah and Iceland populations, computerization of the genealogies of the population founders and their descendants began. The Utah genealogy was record linked to statewide cancer and death certificates dating back decades, and continues to be linked to these, and now additional Utah hospital data. In Iceland, record linking of medical data from collaborating physicians began in the  :[257],\"descriptions of the genetic nature of many diseases, as well as the identification of multiple disease predisposition genes, has been accomplished from both of these resources. Accomplishments using the Utah genealogy resource include identification of the BRCA1 breast cancer gene, identification of the p16 melanoma gene, identification of a heritable component underlying death from influenza using 100 years of death certificate data, and a recent description of relative :[86],\"risks for cancers of all sites among relatives of prostate cancer  significant :[337],\"contributions from these 2 limited population resources supplies a vision of much wider scope for genealogical resources linked to medical data. We envision creation of the genealogy of the U.S., extending back to ancestors beyond the U.S. (and someday a genealogy of all users of the same alphabets). Genealogy is the second most popular hobby in the world and genealogies extending back tens of generations are commonly found on the Internet or published in books. In Utah we have created tools to gather and link publicly available genealogical data. A pilot genealogy of 14 million individuals for Utah and the surrounding states, with extensions back to European ancestors has been built. We limited the size of this pilot genealogy by requiring a Utah record; an increase in scale is possible. Were a U.S. genealogical resource to be built, national medical data repositories such as the Veterans Administration data and the Social Security Death Index could provide initial nationwide phenotype data to be linked. State birth and death certificates and disease registries could be  :[512],\"opportunities to understand the familial and genetic contribution to human health and disease from such a resource are almost unlimited. Analysis of these data could form the basis of risk estimation models which could be used to determine need for genetic testing, appropriate disease screening schedules, suggested lifestyle changes, and preferred treatments and medications for individuals and physicians willing to utilize public data to enhance and inform family health histories. The success of the UPDB and the Iceland resource in appropriately handling such data for over 30 years supports the potential of such an  the Utah nor the Iceland resource has :[607],\"yet been used to assess an individual’s familial risk of disease. The extension to such use requires dealing with many significant issues, including availability of genealogical data, availability of medical history, quality of these data, computerization and record linking of these data, privacy and security issues surrounding the data, and liability implied in their use for medical decision-making, among others. \n",
      "65: 2122593529  Summary: GCHap quickly finds maximum likelihood estimates (MLEs) of frequencies of haplotypes given genotype information on a random sample of individuals. It uses the gene counting method but by excluding haplotypes with zero MLE at an early stage, this implementation uses many orders of magnitude less space and time than naive implementations. A second program, ApproxGCHap, is provided to give alternate estimates for data sets with large numbers of loci or large amounts of missing genotypes. Availability: The Java classes and Javadocs pages for GCHap can be obtained from bioinformatics.med.utah.edu/ \n",
      "66: 1495188608  This paper reports a genetic algorithm (GA) for individual assignment using multi-locus microsatellite genotyping. Its performance has been compared with existing frequency, Bayesian and distance-based methods using simulated as well as actual data. Simulated data has been generated with SIMCOAL program. Actual data has been generated from genotypes of four cattle breeds from India. The GA showed lower accuracy while assigning individuals from simulated data. Its performance was comparable to that of existing methods using actual data. \n",
      "67: 2169587965  This paper proposes a chromosome compression scheme which represents subsolutions by the most expressive schemata. The proposed chromosome compression scheme is combined with the dependency structure matrix genetic algorithm and the restricted tournament replacement to create a scalable optimization tool which optimizes problems via hierarchical decomposition. One important feature of the proposed method is that at the end of the run, the problem structure obtained from the proposed method is comprehensible to human researchers and is reusable for larger-scale problems. The empirical result shows that the proposed method scales sub-quadratically with the problem size on hierarchical problems and is able to capture the problem structures accurately. \n",
      "68: 2168797010   :[0],\"scale understanding of complex and dynamic alterations in cellular and subcellular levels during cancer in contrast to normal condition has facilitated the emergence of sophisticated systemic approaches like network biology in recent times. As most biological networks show modular properties, the analysis of differential modularity between normal and cancer protein interaction networks can be a good way to understand cancer more significantly. Two aspects of biological network modularity e.g. detection of molecular complexes (potential modules or clusters) and identification of crucial nodes forming the overlapping modules have been considered in this regard. \n",
      "69: 2003272551  We propose a new algorithm that solves the Steiner tree problem on graphs with vertex set V to optimality in O(B\"t\"w\"+\"2^[email protected][email protected]?|V|) time, where tw is the [email protected]?s treewidth and the Bell numberB\"k is the number of partitions of a k-element set. This is a linear-time algorithm for graphs with fixed treewidth and a polynomial algorithm for [email protected]?O(log|V|/loglog|V|). While being faster than the previously known algorithms, the coloring scheme used in our algorithm can be extended to give new, improved algorithms for the prize-collecting Steiner tree as well as the k-cardinality tree problems with similar runtime bounds. \n",
      "70: 75330223  Biological vision systems have become highly optimized over millions of years of evolution, developing complex neural structures to represent and process stimuli. Moreover, biological systems of vision are typically far more efficient than current human-made machine vision systems. The present report describes a non-task-dependent image representation schema that simulates the early phase of a biological neural vision mechanism. We designed a neural model involving multiple types of computational units to simulate ganglion cells and their non-classical receptive fields, local feedback control circuits and receptive field dynamic self-adjustment mechanisms in the retina. We found that, beyond the pixel level, our model was able to represent images self-adaptively and rapidly. In addition, the improved representation was found to substantially facilitate contour detection. We propose that this improvement arose because ganglion cells can resize their receptive fields, enabling multiscale analysis functionality, a neighborhood referring function and a localized synthesis function. The ganglion cell layer is the starting point of subsequent diverse visual processing. The universality of this cell type and its functional mechanisms suggests that it will be useful for designing image processing algorithms in future. \n",
      "71: 2115692554  Summary: The scale of genetic-variation datasets has increased enormously and the linkage equilibrium (LD) structure of these polymorphisms, particularly in whole-genome association studies, is of great interest. The significant computational complexity of calculating single- and multiple-marker correlations at a genome-wide scale remains challenging. We have developed a program that efficiently characterizes whole-genome LD structure on large number of SNPs in terms of single- and multiple-marker  :[65],\"LdCompare is licensed under the GNU General Public License (GPL). Source code, documentation, testing datasets and precompiled executables are available for download at:  :[89],\"ke_hao@affymetrix.com \n",
      "72: 2130093497  In this paper, a novel model-based approach is proposed for generating a set of image feature maps (or primal sketches). For each type of feature, a piecewise smooth parametric model is developed to characterize the local intensity function in an image. Projections of the intensity profile onto a set of orthogonal Zernike-moment-generating polynomials are used to estimate model-parameters and, in turn, generate the desired feature map. A small set of moment-based detectors is identified that can extract various kinds of primal sketches from intensity as well as range images. One main advantage of using parametric model-based techniques is that it is possible to extract complete information (i.e., model parameters) about the underlying image feature, which is desirable in many high-level vision tasks. Experimental results are included to demonstrate the effectiveness of proposed feature detectors. \n",
      "73: 1823536187  The longest common subsequence(LCS) problem is one of the classical and well-studied problems in computer science. The computation of the LCS is a frequent task in DNA sequence analysis, and has applications to genetics and molecular biology. In this paper we define new variants, introducing the notion of gap-constraints in LCS problem and present efficient algorithms to solve them. \n",
      "74: 2085335356  In this paper we discuss the results of the parallel implementation of a new scheme for the solution of large and dense linear systems: an implicit version of the well known Gauss-Jordan method. We present a complete computational analysis of the method and we explain the superiority of implicit schemes on the basis of the number of accesses to the problem data. A new pivoting strategy for the implicit methods is presented. We prove that this strategy gives better results than the classical partial pivoting used with Gaussian Elimination. \n",
      "75: 2094592259  SBASE (http://www.icgeb.trieste.it/sbase) is an on-line collection of protein domain sequences and related computational tools designed to facilitate detection of domain homologies based on simple database search. The 10th ‘jubilee release’ of the SBASE library of protein domain sequences contains 1 052 904 protein sequence segments annotated by structure, function, ligand-binding or cellular topology, clustered into over 6000 domain groups. Domain identification and functional prediction are based on a comparison of BLAST search outputs with a knowledge base of biologically significant similarities extracted from known domain groups. The knowledge base is generated automatically for each domain group from the comparison of within-group (‘self’) and out-of-group (‘non-self’) similarities. This is a memory-based approach wherein group-specific similarity functions are automatically learned from the database. \n",
      "76: 2016071741  Biological vision systems have become highly optimized over millions of years of evolution, developing complex neural structures to represent and process stimuli. Moreover, biological systems of vision are typically far more efficient than current human-made machine vision systems. The present report describes a non-task-dependent image representation schema that simulates the early phase of a biological neural vision mechanism. We designed a neural model involving multiple types of computational units to simulate ganglion cells and their non-classical receptive fields, local feedback control circuits and receptive field dynamic self-adjustment mechanisms in the retina. We found that, beyond the pixel level, our model was able to represent images self-adaptively and rapidly. A series of statistical analyses revealed that this model not only produces compact and abstract approximations of images, but also retains their primary visual features. In addition, the improved representation was found to substantially facilitate contour detection and image segmentation. We propose that this improvement arose because ganglion cells can resize their receptive fields, enabling multi-scale analysis functionality, a neighborhood referring function and a localized synthesis function. The ganglion cell layer is the starting point of subsequent diverse visual processing. The universality of this cell type and its functional mechanisms suggests that it will be useful for designing image processing algorithms in future. \n",
      "77: 2065646054   :[0],\"evidence is accumulating that dinucleotide steps other than AA/TT affect DNA flexure of AnTm (m + n greater than = 4) containing fragments. However, it is not clear whether macroscopic DNA flexure without AA/TT steps might occur. In this paper we demonstrate the anomaly in electrophoretic mobility of non AA/TT repetitive DNA sequences which is a function of sequence phasing. Therefore, our results show that PyPu (TA) and AG/CT steps, angulary separated by close to 180 degrees from Pu/Py (GC) and GG/CC steps, bend DNA, even in the absence of AnTm tracts. \n",
      "78: 2035403830  The HARQ enables the error correction of the unsuccessfully decoded transport block, but it requires redundancy retransmissions and delay in receiving an acknowledgement. In order to provide the flexibility of scheduling, the DL of LTE system uses the asynchronous HARQ so that retransmissions occur at any time. Generally in the FDD, the HARQ round trip time (RTT) which is time interval between initial transmission and retransmission is fixed. In the TDD, UL/DL subframe is separated in time domain by UL-DL configuration, so the HARQ RTT is different according to each subframe and that would be longer than the FDD. Especially when retransmissions occur, if the HARQ RTT increases, the QoS requirement of the retransmitted packet would not be satisfied by increased end-to-end latency. In this paper, we suggest the method of implementing DL asynchronous HARQ in LTE TDD system that the eNB can apply by UL-DL configuration as minimizing the HARQ RTT, and analyze in aspect of the HARQ RTT. \n",
      "79: 1971995096  The broad region outside the classical receptive field (CRF) of a vision neuron, known as the non-classical receptive field (nCRF), exerts a robust modulatory effect on the responses to visual stimuli presented within the CRF, and plays an important role in visual information processing. One possible role for the nCRF is the extract object contours from disorderly background textures. In this study, a multi-scale integration based contour extraction model, inspired by the inhibitory and disinhibitory interactions between the CRF and the nCRF is presented. Unlike previous models, our model not only includes both the simple and complex cell mechanisms but also introduces pre-processing of the external information by the retinal ganglion cells at an early stage. The multi-scale representation of a physical scene acquired through such pre-processing was filtered through Gabor filters, and then inhibited or disinhibited at different spatial locations on different scales until a final response was obtained. Our results show that by introducing this kind of mechanism into the model, numbers of non-meaningful texture elements can be removed significantly, while at the same time, the object contours can be detected effectively. In addition to the superior contour detection performance in comparison to other contour detection models, our model provides a better understanding of the role of the nCRF and a novel approach for computer vision and pattern recognition. \n",
      "80: 1573992779  We define and investigate a structure called transversal edge-partition related to triangulations without non empty triangles, which is equivalent to the regular edge labeling discovered by Kant and He. We study other properties of this structure and show that it gives rise to a new straight-line drawing algorithm for triangulations without non empty triangles, and more generally for 4-connected plane graphs with at least 4 border vertices. Taking uniformly at random such a triangulation with 4 border vertices and n vertices, the size of the grid is almost surely $\\frac{11}{27}n \\times \\frac{11}{27}n$up to fluctuations of order $\\sqrt{n}$, and the half-perimeter is bounded by n–1. The best previously known algorithms for straight-line drawing of such triangulations only guaranteed a grid of size $(\\lceil n/2\\rceil - 1)\\times \\lfloor n/2 \\rfloor$. Hence, in comparison, the grid-size of our algorithm is reduced by a factor $\\frac{5}{27}$, which can be explained thanks to a new bijection between ternary trees and triangulations of the 4-gon without non empty triangles. \n",
      "81: 2560771977  The rapidly increasing amount of data on human genetic variation has resulted in a growing demand to identify pathogenic mutations computationally, as their experimental validation is currently beyond reach. Here we show that alpha helices and beta strands differ significantly in their ability to tolerate mutations: helices can accumulate more mutations than strands without change, due to the higher numbers of inter-residue contacts in helices. This results in two patterns: a) the same number of mutations causes less structural change in helices than in strands; b) helices diverge more rapidly in sequence than strands within the same domains. Additionally, both helices and strands are significantly more robust than coils. Based on this observation we show that human missense mutations that change secondary structure are more likely to be pathogenic than those that do not. Moreover, inclusion of predicted secondary structure changes shows significant utility for improving upon state-of-the-art pathogenicity predictions. \n",
      "82: 2002220863  Detecting object in unseen images is an challenging task because of the strong clutter background, various scale of object and the deformation of class. In this paper, we present a shape-based object detection model using scale-invariant fragment feature which is approximated by conjunctive short straight segments. This is a novel shape descriptor for object detection by bypassing estimation of scale of object in natural scene. Utilizing those local and consistent segments, we improve the robustness of model to natural background and deformation of object. We experiment our model on two texture-less image datasets, INRIA horses dataset and Weizmann horses dataset. The results demonstrate our model outperform those state-of-the-art methods. \n",
      "83: 2743747259  These days, a lot number of elderly people need health care which may cause huge financial costs, especially in formal case. Machine Learning and the profound achievements in sensing technology provide the opportunities to monitor people living independently at home and can detect a distress situation affordably. Although there are some approaches to do recognize activities for this purpose, but there has not been any game-theoretic approach in order to select the most efficient sensors to reduce the systemu0027s overhead by decreasing the number of features. In this paper, we present a new classifier to recognize activities in a smart environment that is based on selection of most efficient sensors by cooperative game theory. The sensors are selected in which provide more information about the target classes. We show the performance of our algorithm by simulation. \n",
      "84: 2098292002  \n",
      "85: 2128594755  This paper describes how genetic programming was used as an invention machine to automatically synthesize complete designs for six optical lens systems that duplicated the functionality of previously patented lens systems. The automatic synthesis was done \"from scratch\"-that is, without starting from a pre-existing good design and without pre-specifying the number of lenses, the physical layout of the lenses, the numerical parameters of the lenses, or the non-numerical parameters of the lenses. One of the six genetically evolved lens systems infringed a previously issued patent; three contained many of the essential features of the patents, without infringing; and the others were non-infringing novel designs that duplicated (or improved upon) the performance specifications contained in the patents. One of the six patents was issued in the 21st-century. The six designs were created in a substantially similar and routine way, suggesting that the approach used may have widespread utility. The genetically evolved designs are instances of human-competitive results produced by genetic programming in the field of optical design. \n",
      "86: 1500419802  We investigate the adaptability of optimization algorithms for the real-valued case to concrete problems via tuning. However, the focus is not primarily on performance, but on the tuning potential of each algorithm/problem system, for which we define the empirical tuning potential measure (ETP). It is tested if this measure fulfills some trivial conditions for usability, which it does. We also compare the best obtained configurations of 4 adaptable algorithms (2 evolutionary, 2 classic) with classic algorithms under default settings. The overall outcome is quite mixed: Sometimes adapting algorithms is highly profitable, but some problems are already solved to optimality by classic methods. \n",
      "87: 2078564087  ABSTRACT In this paper a parallel Romberg Integration procedure for the numerical evaluation of single and double integrals U described. The process is achieved by the use of a sub-rnanager to facilitate the data partitioning procedure. \n",
      "88: 1505200862  In this paper, we revisit the much studied problem of Pattern matching with Swaps (Swap Matching problem, for short). We first present a new graph-theoretic approach to model the problem, which opens a new and so far unexplored avenue to solve the problem. Then, using the model, we devise an efficient algorithm to solve the swap matching problem. The resulting algorithm is an adaptation of the classic shiftor algorithm. For patterns having length similar to the word-size of the target machine, the algorithm runs in O(n log m) time, where n and m are the length of the text and the pattern respectively. \n",
      "89: 2104789942   :[0],\"is a reference database of eukaryotic repetitive DNA, which includes prototypic sequences of repeats and basic information described in annotations. Updating and maintenance of the database requires specialized tools, which we have created and made available for use with Repbase, and which may be useful as a template for other curated databases. \n",
      "90: 5859415  In this paper, we utilize the physiological mechanism of non-classical receptive field and design a hierarchical network model for image representation based on neurobiology. It is different from the contour detection, edge detection, and other practices using the classical receptive fields, simulating the non-classical receptive fields physiological mechanism which can be dynamically adjusted according to stimulation for image local segmentation and compression based on image neighborhood region similarity, thus to realize the inner image representation in neural representation level and convenient for extract the semanteme further. \n",
      "91: 2170800974  Two related problems with TORCS car racing competition controllers are approached here. At first, we demonstrate how to handle the 10% artificial sensor noise that made proper track segment recognition quite difficult for some controllers in 2010 (when the noise was introduced). This is successfully dealt with by a combination of averaging and regression. The presented solution copes well with the natural antagonism between accuracy and the produced time lag, meaning that controllers are enabled to use full sensor information despite noise. Secondly, we suggest a solution for the problem of selecting a minimal set of controller parameter configurations suitable for several different tracks by applying principles of multi-objective optimization. While a full multi-objective approach is unfeasible here, we investigate the conflict potential between objectives (in this case tracks) in order to remove the ones that are less problematic. Naturally, the second problem is much more interesting but can only be tackled if the first one is resolved. \n",
      "92: 2238930303   :[0],\"which have been subject to Post-transcriptional exon shuffling (PTES), have an exon order inconsistent with the underlying genomic sequence. These have been identified in a wide variety of tissues and cell types from many eukaryotes, and are now known to be mostly circular, cytoplasmic, and non-coding. Although there is no uniformly ascribed function, several have been shown to be involved in gene regulation. Accurate identification of these transcripts can, however, be difficult due to artefacts from a wide variety of sources. \n",
      "93: 2277693607  Upon publication of the original article [1] it has been found that author David J. Elliott has been accidentally misspelt. The correct spelling is David J. Elliott rather than David J. Elliot. This has been herewith corrected with this erratum and also updated in the original article. \n",
      "94: 1561847687  Despite the existence of a number of procedures for real-parameter optimization using evolutionary algorithms, there is still a need of a systematic and unbiased comparison of different approaches on a carefully chosen set of test problems. In this paper, we develop a steady-state, population-based optimization algorithm which allows the main search principles to be independently designed. The algorithm so developed is applied to a set of 25 test problems and results on 10 and 30 dimensions are presented. Although the proposed procedure cannot find the exact optimum within the specified number of function evaluations, in most problems, the algorithm shows steady progress towards the optimum. Moreover, it is also observed that the performance of the algorithm does not get affected by the rotation of the functions, discontinuity and embedded noise in function description \n",
      "95: 2045875756  This paper has two aspects. First, it describes the use of genetic programming to automatically synthesize a solution to the challenge problem posed at an international competition held every four years in the field of optical design. In 2002, the competition at the International Optical Design Conference attracted 42 entries from 39 well-known optical designers, commercial consultants, and patent holders from many of the fieldu0027s most prominent companies, universities, and research institutions. The 39 human contestants spent an average of 34.1 hours working on their entries. Virtually all entries were considered good solutions to the challenge problem. Genetic programming automatically synthesized a design \"from scratch\" - that is, without starting from a pre-existing human-created design and without pre-specifying the number of lenses, the physical layout of the lenses, or the numerical or non-numerical parameters of the lenses. The run of genetic programming did not employ any knowledge base of design techniques or principles from the field of optical design and did not entail any human intervention during the run. The genetically evolved optical lens system would have ranked in the middle (21st) if it had been entered into the 2002 competition and is therefore an instance of a \"human-competitive\" result produced by genetic programming. Second, this paper presents a mutation operation for numerical constants that is especially appropriate for problems in which the to-be-designed structure contains a large number of non-linearly interrelated numerical values and for problems in which the topology of the solution is to be automatically created. \n",
      "96: 2126940406  \n",
      "97: 2035234463  A new algorithm, the double-bordering algorithm, for the solution of linear systems of equations is derived, and adapted to enable it to perform matrix inversion in parallel. The resultant algorithm was implemented on a four-processor MIMD computer, and its performance demonstrated to be superior to the performance of the Gauss-Jordan algorithm. \n",
      "98: 1578446700  This paper discusses the underlying pressures responsible for code growth in genetic programming, and shows how an understanding of these pressures can be used to use to eliminate code growth while simultaneously improving performance. We begin with a discussion of two distinct components of code growth and the extent to which each component is relevant in practice. We then define the concept of resilience in GP trees, and show that the buildup of resilience is essential for code growth. We present simple modifications to the selection procedures used by GP that eliminate bloat without hurting performance. Finally, we show that eliminating bloat can improve the performance of genetic programming by a factor that increases as the problem is scaled in difficulty. \n",
      "99: 2905425488  \n"
     ]
    }
   ],
   "source": [
    "# The abstract in the paper are needed to be accessed in order to compare the difficulty of the words used\n",
    "\n",
    "abstracts = []\n",
    "for index, paper in enumerate(papers):\n",
    "#         f.write(paper['_source']['abstract'])\n",
    "#         f.write('\\n')\n",
    "    print(f\"{index}: {paper['_source']['id']}  {paper['_source']['abstract']}\")\n",
    "    abstracts.append(paper['_source']['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a05297ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mink-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'guacamole.univ-avignon.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "url = 'https://inex:qatc2011@guacamole.univ-avignon.fr/dblp1/_search?'\n",
    "query = {'size':'1'}\n",
    "with requests.get(url, params=query, verify=False) as f:\n",
    "    dump = f.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14d6ffc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
