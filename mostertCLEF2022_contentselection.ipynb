{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nWritten by Femke Mostert (GitHub: Nademaaltijd) for SimpleText @ CLEF 2022\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "Written by Femke Mostert (GitHub: Nademaaltijd) for SimpleText @ CLEF 2022\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import packages \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import JSON file with topics and queries \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import json file\n",
    "f = open('SP12022topics.json')\n",
    "query_data = json.load(f)\n",
    "\n",
    "# convert file to normalised dataframe \n",
    "df = pd.json_normalize(query_data)\n",
    "\n",
    "# extract relevant data and combine\n",
    "topic = df['topic_id']\n",
    "query_id = df['query_id']\n",
    "query_txt = df['query_text']\n",
    "\n",
    "queries = zip(topic, query_id, query_txt)\n",
    "queries = list(queries)\n",
    "\n",
    "\n",
    "# testing query text extraction \n",
    "#for i in range(len(queries)):\n",
    "    #print(queries[i][2])\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G01': 'Digital assistants like Siri and Alexa entrench gender biases, says UN Kevin Rawlinson Assigning female genders to digital assistants such as Apple’s Siri and Amazon’s Alexa is helping entrench harmful gender biases, according to a UN agency. Research released by Unesco claims that the often submissive and flirty responses offered by the systems to many queries – including outright abusive ones – reinforce ideas of women as subservient. “Because the speech of most voice assistants is female, it sends a signal that women are obliging, docile and eager-to-please helpers, available at the touch of a button or with a blunt voice command like ‘hey’ or ‘OK’,” the report said. “The assistant holds no power of agency beyond what the commander asks of it. It honours commands and responds to queries regardless of their tone or hostility. In many communities, this reinforces commonly held gender biases that women are subservient and tolerant of poor treatment.” The publication was entitled I’d Blush if I Could; a reference to the response Apple’s Siri assistant offers to the phrase: “You’re a slut.” Amazon’s Alexa will respond: “Well, thanks for the feedback.” The paper said such firms were “staffed by overwhelmingly male engineering teams” and have built AI systems that “cause their feminised digital assistants to greet verbal abuse with catch-me-if-you-can flirtation”. It added: “The subservience of digital voice assistants becomes especially concerning when these machines – anthropomorphised as female by technology companies – give deflecting, lacklustre or apologetic responses to verbal sexual harassment. “This harassment is not, it bears noting, uncommon. A writer for Microsoft’s Cortana assistant said that ‘a good chunk of the volume of early-on enquiries’ probe the assistant’s sex life.” It cited research by a firm that develops digital assistants that suggested at least 5% of interactions were “unambiguously sexually explicit” and noted the company’s belief that the actual number was likely to be “much higher due to difficulties detecting sexually suggestive speech”. Saniye Gülser Corat, Unesco’s director for gender equality, said: “The world needs to pay much closer attention to how, when and whether AI technologies are gendered and, crucially, who is gendering them.” Unesco said the relatively recent introduction of such technology provided an opportunity to develop less damaging norms in its application. It called for digital assistants not to be made female by default and said technology firms should explore the feasibility of developing a neutral machine gender that is neither male nor female. It added that they should programme such technology to discourage gender-based insults and abusive language, as well as designing assistants to be interchangeable across devices and defining them as “non-human at the outset of interactions with human users”.', 'G02': \"Apple contractors 'regularly hear confidential details' on Siri recordings Alex Hern Apple contractors regularly hear confidential medical information, drug deals, and recordings of couples having sex, as part of their job providing quality control, or “grading”, the company’s Siri voice assistant, the Guardian has learned. Although Apple does not explicitly disclose it in its consumer-facing privacy documentation, a small proportion of Siri recordings are passed on to contractors working for the company around the world. They are tasked with grading the responses on a variety of factors, including whether the activation of the voice assistant was deliberate or accidental, whether the query was something Siri could be expected to help with and whether Siri’s response was appropriate. Apple says the data “is used to help Siri and dictation … understand you better and recognise what you say”. But the company does not explicitly state that that work is undertaken by humans who listen to the pseudonymised recordings. Apple told the Guardian: “A small portion of Siri requests are analysed to improve Siri and dictation. User requests are not associated with the user’s Apple ID. Siri responses are analysed in secure facilities and all reviewers are under the obligation to adhere to Apple’s strict confidentiality requirements.” The company added that a very small random subset, less than 1% of daily Siri activations, are used for grading, and those used are typically only a few seconds long. A whistleblower working for the firm, who asked to remain anonymous due to fears over their job, expressed concerns about this lack of disclosure, particularly given the frequency with which accidental activations pick up extremely sensitive personal information. Siri can be accidentally activated when it mistakenly hears its “wake word”, the phrase “hey Siri”. Those mistakes can be understandable – a BBC interview about Syria was interrupted by the assistant last year – or less so. “The sound of a zip, Siri often hears as a trigger,” the contractor said. The service can also be activated in other ways. For instance, if an Apple Watch detects it has been raised and then hears speech, Siri is automatically activated. The whistleblower said: “There have been countless instances of recordings featuring private discussions between doctors and patients, business deals, seemingly criminal dealings, sexual encounters and so on. These recordings are accompanied by user data showing location, contact details, and app data.” That accompanying information may be used to verify whether a request was successfully dealt with. In its privacy documents, Apple says the Siri data “is not linked to other data that Apple may have from your use of other Apple services”. There is no specific name or identifier attached to a record and no individual recording can be easily linked to other recordings. Accidental activations led to the receipt of the most sensitive data that was sent to Apple. Although Siri is included on most Apple devices, the contractor highlighted the Apple Watch and the company’s HomePod smart speaker as the most frequent sources of mistaken recordings. “The regularity of accidental triggers on the watch is incredibly high,” they said. “The watch can record some snippets that will be 30 seconds – not that long but you can gather a good idea of what’s going on.” Sometimes, “you can definitely hear a doctor and patient, talking about the medical history of the patient. Or you’d hear someone, maybe with car engine background noise – you can’t say definitely, but it’s a drug deal … you can definitely hear it happening. And you’d hear, like, people engaging in sexual acts that are accidentally recorded on the pod or the watch.” The contractor said staff were encouraged to report accidental activations “but only as a technical problem”, with no specific procedures to deal with sensitive recordings. “We’re encouraged to hit targets, and get through work as fast as possible. The only function for reporting what you’re listening to seems to be for technical problems. There’s nothing about reporting the content.” As well as the discomfort they felt listening to such private information, the contractor said they were motivated to go public about their job because of their fears that such information could be misused. “There’s not much vetting of who works there, and the amount of data that we’re free to look through seems quite broad. It wouldn’t be difficult to identify the person that you’re listening to, especially with accidental triggers – addresses, names and so on. “Apple is subcontracting out, there’s a high turnover. It’s not like people are being encouraged to have consideration for people’s privacy, or even consider it. If there were someone with nefarious intentions, it wouldn’t be hard to identify [people on the recordings].” The contractor argued Apple should reveal to users this human oversight exists – and, specifically, stop publishing some of its jokier responses to Siri queries. Ask the personal assistant “are you always listening”, for instance, and it will respond with: “I only listen when you’re talking to me.” That is patently false, the contractor said. They argued that accidental triggers are too regular for such a lighthearted response. Apple is not alone in employing human oversight of its automatic voice assistants. In April, Amazon was revealed to employ staff to listen to some Alexa recordings, and earlier this month, Google workers were found to be doing the same with Google Assistant. Apple differs from those companies in some ways, however. For one, Amazon and Google allow users to opt out of some uses of their recordings; Apple offers no similar choice short of disabling Siri entirely. According to Counterpoint Research, Apple has 35% of the smartwatch market, more than three times its nearest competitor Samsung, and more than its next six biggest competitors combined. The company values its reputation for user privacy highly, regularly wielding it as a competitive advantage against Google and Amazon. In January, it bought a billboard at the Consumer Electronics Show in Las Vegas announcing that “what happens on your iPhone stays on your iPhone”.\", 'G03': 'Alexa, Siri... Elsa? Children drive boom in smart speakers James Tapper Voice assistants such as Alexa and Siri will become common in children’s bedrooms, according to a new report from Internet Matters, the online safety body, which says it is critical for parents to spend more time understanding new technology. The pandemic has accelerated the adoption of new technology at home by “three or four years”, the researchers said, and families in the UK will become much more reliant on voice-enabled devices over the next five years. The report’s author, Lynne Hall, professor of computer science at the University of Sunderland, said we would even see the emergence of a range of celebrity voice assistants. “You’d have Elsa from Frozen,” Hall said. “You can imagine that with every Disney film that came out there would be a new voice skin.” Amazon has already launched a novelty Samuel L Jackson voice for its Alexa devices, although it does not enable all voice commands. The “Living The Future” report, based on interviews and surveys of parents and academics, says that 42% of families have been using tech together more often over the last three months – playing online games together (34%), watching tutorials (51%), streaming videos (41%) or online shopping (36%). Only 7% of parents said they wanted to return to the workplace full time, which is likely to further fuel a boom in home tech controlled by voice assistants, the report said. “The home is becoming less and less private and we need to think about what data is being shared,” said Carolyn Bunting, the chief executive of Internet Matters. “We need to make sure we’re not sleepwalking into a world where we’re just giving away all of this information without thinking about where it’s going, who’s holding it or how it’s being used.” Although voice assistants usually process commands using voice recognition, Apple, Amazon and Google have all used call-centre staff to check recordings and some have heard children sharing the home address and phone number, according to a Bloomberg report last year. Hall said her research showed that children were encountering voice assistants almost from the day they are born. “Women are buying a voice assistant for their child’s bedroom when they’ve had the baby,” she said. “If you’ve got your hands full, it’s much easier to say, ‘Alexa, ring my partner’.” Children do not see voice assistants as their friends, Hall said. “They don’t anthropomorphise. It’s very much a tool. But voice assistants are starting to learn how little children speak. Children try to teach the voice assistant if it doesn’t understand – they give it more information.” Since parents play a vital role in children’s acquisition of language, there are unanswered questions about how hearing a mother or father giving orders to voice assistants might affect their development. Australian academics Yolande Strengers and Jenny Kennedy have voiced concerns in their book The Smart Wife that the devices reinforce gender stereotypes since they are usually voiced by young women, take orders and are linked to domestic responsibilities. There are potential safety benefits to using voice assistants, Bunting said, since children using them for information will usually accept the first answer. Bunting called for better regulation. The government’s online harms bill, obliging social media companies to prevent children seeing harmful content, is likely to be published this winter, “but they are struggling with these softer issues,” she said. She said that tech firms also needed to design an internet for children: “It’s wrong that in the Covid pandemic we saw six or seven-year-olds up to secondary school age children being thrust into Houseparty and Whatsapp video chats. None of those services are designed for little people. We need safe playgrounds for children online.”', 'G04': \"Drug companies look to AI to end 'hit and miss' research Julia Kollewe The hunt for new medicines has often been more like a game of roulette than high-end science. But now the pharmaceutical sector is on the cusp of a transformation, as it delves into cutting-edge technology to come up with new treatments for diseases such as cancer, rheumatoid arthritis and Alzheimer’s. Artificial intelligence (AI) is set to improve the industry’s success rates and speed up drug discovery, potentially saving it billions of dollars, a recent survey by the analytics firm GlobalData has found. AI topped a list of technologies seen as having the greatest impact on the sector this year. Almost 100 partnerships have been struck between AI specialists and large pharma companies for drug discovery since 2015. AI uses automated algorithms – sets of instructions that computers follow – to perform tasks previously done by humans. It can sift quickly through large datasets (from clinical studies and scientific literature) to detect hidden patterns, and perform tasks within seconds that would usually take months. A study in the Lancet found AI software could identify breast cancers that were missed by doctors in mammograms. In a process known as machine learning, AI systems run through millions of possibilities, improving each time, until they are able to perform acceptably. The output of that training is an algorithm. “Drug discovery is being transformed through the use of AI, which is reducing the time it takes to mine the vast amounts of scientific data to enable a better understanding of disease mechanisms and identify new potential drug candidates,” says Karen Taylor, director of the Centre for Health Solutions at accounting and consultancy group Deloitte. “Traditional drug discovery has been very fragmentary, very hit and miss,” she adds. Taylor says the rapid progress of Covid-19 vaccines and potential treatments has been aided by the use of AI techniques. “It allows you to cross-reference a lot of published literature with other data within seconds.” Kitty Whitney, director of thematic research at GlobalData, says the Covid-19 crisis could be a “tipping point” for widespread adoption across the pharma industry. About 90% of large pharmaceutical firms initiated AI projects last year, according to the US research firm Trinity Life Sciences. AstraZeneca and GSK, Britain’s two biggest drugmakers, committed in November to a five-year partnership with Cambridge University to fund the Cambridge Centre for AI in Medicine. The 15-strong team will develop AI and machine-learning technologies to improve clinical trials, personalised medicine and drug discovery. GSK had previously opened a £10m AI research base in King’s Cross, central London, near Google’s DeepMind AI lab. Its global team of AI experts has grown to 50 people, which it wants to double to 100. Functional genomics – a new area of science that looks at why small changes in a person’s genetic make-up can increase the risk of diseases – deals with huge datasets. Each person has about 30,000 genes, which can be combined with others, as Hal Barron, GSK’s chief scientific officer, explains. “You start to realise you’re dealing with trillions and trillions of data points, even per experiment, and no human can interpret that, it’s just too complicated.” Large pharmaceutical companies have been criticised for being slow to embrace technological advances. Drug discovery has a woefully low success rate – of 10 drugs in development, nine will typically fail; it takes 10-12 years on average, and comes with a high cost, of more than $2bn, to take a medicine through research and development and regulatory approval. Conventional drug discovery has been compared to a “molecular casino” by Alex Zhavoronkov, an expert in the use of AI for developing new drugs, who runs Hong Kong-based Insilico Medicine. GSK’s Barron reckons the use of AI technologies could at least double the success rate to 20%, which would save billions of dollars spent on drug development. Others, like Zhavoronkov, hope the success rate could improve much more, potentially to 50%. All of the world’s top 10 drugmakers – the Swiss firms Novartis and Roche; the US companies Pfizer, Johnson & Johnson, Merck, AbbVie and Bristol Myers Squibb; France’s Sanofi; and the UK’s AstraZeneca and GSK – are now investing in AI, mainly through collaborations, or by acquiring technologies. Kim Branson, GSK’s global head of AI and machine learning, says AI is being used in the search for treatments for infectious diseases, as well as for diseases that are harder to crack such as cancer, rheumatoid arthritis and autoimmune disorders such as Crohn’s. Alzheimer’s– “the hardest of the hard targets” – is on GSK’s radar but will be tackled at a later stage. Zhavoronkov says the problem with Alzheimer’s and the brain disorder Parkinson’s disease is that there is not enough data available to study them, hence the large number of drug failures to date. Zhavoronkov and Barron have expressed confidence that a major breakthrough in one of the harder-to-research diseases can be achieved with AI technologies. Barron compares the potential to having a new microscope. “Within the next year or two we might find a target that really can make a difference.”\", 'G05': 'After the Nobel, what next for Crispr gene-editing therapies? Philip Ball When last year’s Nobel prize for chemistry was awarded to biochemist Jennifer Doudna and microbiologist Emmanuelle Charpentier for their work in developing the technique of gene editing known as Crispr-Cas9 (pronounced “crisper”), headlines hailed their discovery as “molecular scissors” that would allow us to “rewrite the book of life” – with all the complicated ethical questions that ability raises. But much of the excitement has nothing to do with visions of designer babies. The real promise of Crispr is for treating diseases caused by genetic mutations, from muscular dystrophy to congenital blindness, and even some cancers. The first human trials of Crispr therapies are happening already, and researchers hope that they are on the brink of reaching the clinic. “The speed at which Crispr research has progressed has been truly astonishing,” says Doudna from the University of California at Berkeley. Many common diseases, including heart conditions, Alzheimer’s and diabetes, are partly caused by genes: people who inherit the “wrong” variants of certain genes are more vulnerable. For many of these conditions the genetic component is complicated: many genes are involved. Other diseases, such as cystic fibrosis, might be caused by the malfunction of just one or a few genes. In that case, the disease might be cured entirely by gene editing: replacing the faulty genes with the healthy variant. This “gene therapy” approach has been a goal ever since scientists first began learning how to edit genes in the 1970s. But it has never yet lived up to the hype, because editing one gene among about 21,000 others in the DNA of each of our cells is hard. It requires very accurate tools for finding the gene, snipping the DNA at that point, and then stitching in a new gene (or fragment of one) in its place. Biologists have been able to make such edits for decades, but not precisely enough for safe clinical use. If editing is too messy or inadvertently alters other genes too, the consequences could be dire – in particular, an unintended mutation could trigger cancer. Crispr changed all that. The technique uses an enzyme molecule called Cas9, first found in bacteria, which can be reliably programmed to find its target. It carries with it a piece of genetic material called RNA, similar to DNA, which holds the sequence of the target site. When the enzyme finds the DNA sequence matching that on its RNA reference strand, it snips the DNA double helix in two. Other enzymes can then insert another piece of DNA – encoding the “healthy” sequence, say – into the break. When the Crispr system was first reported in 2012 by Doudna, Charpentier and other researchers, the unprecedented accuracy of gene-editing it permitted quickly began to transform the possibilities for tailoring a genome – the sum of an organism’s DNA – to order. The roles and effects of genes could be deduced by cutting them out or modifying them. Crispr also made gene-editing more viable for medicine. The first diseases researchers are looking at, Doudna says, are those that require “a simple change in a single gene and in a cell or tissue that we can target easily”. As it’s a new and expensive approach, she adds, it makes sense to prioritise diseases for which no other treatments exist. Some blood disorders, such as sickle-cell anaemia and beta thalassemia, fit the bill. In sickle-cell disease, a mutation in the gene for haemoglobin (the oxygen-carrying protein in red blood cells) changes the cells’ shape, causing problems with blood flow. In a procedure developed by a hospital in Tennessee, last year a Mississippi woman named Victoria Gray became the first person to receive an experimental Crispr treatment for sickle-cell anaemia. Blood-forming stem cells from her bone marrow were collected and treated outside her body to alter a gene involved in haemoglobin production, before being transfused back. So far the treatment seems to be successful: Gray has not needed the regular blood transfusions or hospitalisations her condition previously necessitated. She is now taking part in trials on Crispr treatments of both sickle-cell disease and beta thalassemia conducted in Boston by Crispr Therapeutics in collaboration with Vertex Pharmaceuticals. Doudna warns, however, that the early therapies are going to be quite expensive. Lowering the cost is one of the key aims of her Innovative Genomics Institute at Berkeley. “Having a cure for sickle-cell disease that few people can afford is not a solution to the problem,” she says. One great attraction of Crispr, says Niren Murthy, a bioengineer at Berkeley, is that it could be a one-shot affair. You have the treatment and the gene is fixed for good, rather than you having to return to the doctor every few months. What’s more, the gene-editing doesn’t have to be particularly efficient to work. “With sickle-cell disease, it appears that correcting the mutation in just 5% of a patient’s stem cells would be enough to have a positive clinical effect,” says Doudna. “We’re aiming for much higher than that, of course – the more you can target your treatment, the higher the efficiency.” One key advantage in treating these diseases is that it’s easy to get the Crispr system to the right place: the blood. For editing other tissues, the challenge is to cross the barrier between the bloodstream, where a drug would be introduced, and the cells of the tissue. If you just inject the molecular components into the blood, they get quickly degraded by the body’s immune system. It’s better to load them into some tiny vehicle or “vector” such as synthetic particles or disabled viruses (that’s how the active ingredients of Covid vaccines are delivered). But these tend to be too large to get through membranes and into tissues. “The delivery problem is very large,” Murthy says. “If someone was able to solve it, that would open up a lot more therapeutic opportunities.” Some researchers hope that Crispr can combat cancer. One approach would use gene-editing to boost our immune system so that it is better at destroying tumour cells. Such cancer immunotherapy is already showing great promise, but “Crispr could make it more efficient or effective,” says Doudna. “The basic concept is to edit a patient’s T-cells [a type of white blood cell central to the immune response] and reintroduce them to the bloodstream so that they can recognise and attack cancer cells.” The first human trial for Crispr-boosted (lung) cancer immunotherapy happened in China in 2016. There have also been efforts to treat some types of blood and bone cancers this way. But it’s too early to say how effective the treatments are, Doudna says. Another option is to use Crispr to disable cancer cells themselves – but again, the challenge is getting the gene-editing machinery into tumours. For blood cancers such as leukaemia, Murthy points out, this delivery problem doesn’t arise. Atherosclerosis (a cause of stroke and heart disease) is another important target. Some people have a genetic vulnerability to it because their cells produce too much of a protein called PCSK9, which stops a molecule called LDL cholesterol from being broken down. High levels of LDL cholesterol can create hardening of the arteries, which in turn may induce heart failure. Cholesterol breakdown takes place in the liver, which is one of the few tissues for which good drug-delivery vehicles have been developed. That makes PCSK9-related atherosclerosis an ideal target for Crispr therapy. Last year, the US biotech startup Verve, based in Cambridge, Massachusetts, began trialling this approach, using artificial nanoparticles made from fatty lipids to ferry the gene-editing molecules to the liver. Cambridge-based Intellia, meanwhile, is exploring Crispr therapies for sickle-cell, haemophilia and some rare genetic heart conditions. Yet another Cambridge-based gene-editing company, Editas, has begun a trial in collaboration with Dublin-based Allergan that uses Crispr to treat the most common form of inherited childhood blindness, called LCA10. Unlike the earlier sickle-cell and cancer treatments, this one introduces Crispr directly into the body – in this case by injecting it, inside a virus, into the eye. The eye is a good target, Doudna says, because it has certain characteristics that make genome-editing less likely to have unwanted side-effects. “We’ll learn a lot from this trial”, she adds, “and I’m excited to see the results.” Murthy is working on a Crispr treatment for Duchenne muscular dystrophy, one of the most common and severe forms. It is caused by mutations of a gene that produces dystrophin, which is involved in building muscles, and results in the wasting away of muscle fibres, leading to disability and death. But he suspects that Crispr therapy may first see wide clinical use for neurological genetic conditions such as Huntington’s disease, because brain tissue turns out to be easier to edit than muscle. Treating different diseases might demand different kinds of gene-editing. The simplest approach is to just mess up a gene so it doesn’t work. When Cas9 snips a DNA strand, the cell’s DNA-repair machinery doesn’t just stitch it together again; typically it shaves a bit off the strands, as if cleaning up the ragged ends. The rejoined gene is then generally useless – and sometimes that’s all you need. Some editing jobs call for a more precise molecular scalpel, however. “For most genetic diseases, precise gene correction, rather than disruption, is needed to benefit patients,” says David Liu of the Broad Institute of the Massachusetts Institute of Technology and Harvard University. Over the past few years, he has developed a way of using Cas9 to make precise changes to just a single one of the molecular units – called bases – that encode genetic information. Sometimes, as in sickle-cell disease, that’s all it takes to make a mutation dangerous. Liu’s so-called base editors use a modified version of Cas9 that can target DNA in a programmed way but doesn’t cut it, in conjunction with other molecules that then swap a single base at the target site. Liu and his colleagues are using their base editors to treat a devastating condition called progeria, which causes very rapid ageing and eventually death in children born with a mutation to a gene called lamin A. This too is caused by a single base change, but the mutant protein it produces can damage nearly all the cells in the body. It’s not enough to just damage mutant lamin A, since the uncontrolled mixture of products that results could still be lethally toxic. You need instead to precisely correct the lone rogue base. Liu’s team has done this in mice genetically altered to carry the human form of mutant lamin A. They treated the animals 14 days after birth – equivalent to about age five in humans – and found that the mice lived until the beginning of “old age” for normal mice. “As we realised the extent of the disease rescue was well beyond what had been achieved before, we started freaking out,” says Liu. “Five years ago, the prospect of correcting a single base pair in a living animal that causes a fatal genetic disease, with a one-time treatment of an engineered molecular machine, seemed like science fiction,” he says. His team is now working with Beam Therapeutics (also in Cambridge, MA) and with Verve in Cambridge to develop these tools for clinical applications in humans; Verve is using base editors for its work on atherosclerosis. Although Murthy says that widespread clinical use of Crispr therapies is still five to 10 years down the line, Doudna admits to being “constantly amazed at how quickly Crispr genome-editing has been adopted by researchers around the world”. Usually, clinical trials can take a long time, she says. So the fact that, thanks to Crispr, “we have people today who appear to be cured of sickle-cell disease is surprising in the best way”.', 'G06': \"Elon Musk: regulate AI to combat 'existential threat' before it's too late Samuel Gibbs Tesla and Space X chief executive Elon Musk has pushed again for the proactive regulation of artificial intelligence because “by the time we are reactive in AI regulation, it’s too late”. Speaking at the US National Governors Association summer meeting in Providence Rhode Island, Musk said: “Normally the way regulations are set up is when a bunch of bad things happen, there’s a public outcry, and after many years a regulatory agency is set up to regulate that industry. “It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.” Musk has previously stated that AI is one of the most pressing threats to the survival of the human race, and that his investments into its development were made with the intention of keeping an eye on its development. “AI is the rare case where I think we need to be proactive in regulation instead of reactive. Because I think by the time we are reactive in AI regulation, it’ll be too late,” Musk told the meeting. “AI is a fundamental risk to the existence of human civilisation.” Elon Musk interviewed as part of the NGA Ahead of the Curve initiative. While Musk has repeatedly shared his worries over AI and its development that is seen as inevitable in some regard, words appeared to hit home with multiple governors of the 32 taking part in the meeting, with follow-up questions looking for suggestions for how to go about regulating AI’s development. Musk suggested that the first stage would be to “learn as much as possible” to better understand the problem. Musk also talked about electric and self-driving cars, saying that at some stage having a non-autonomous vehicle intended for travel rather than recreation would be considered strange and that the biggest threat to autonomous cars would be a hack of the software to take control of a fleet of connected vehicles.\", 'G07': 'Misinformation runs rampant as Facebook says it may take a week before it unblocks some pages Josh Taylor Facebook may wait up to a week before unblocking some of the pages of hundreds of non-media organisations caught up in its news ban, while anti-vaccination content and misinformation continues to run rampant on the social media platform. Content designated as news was blocked on Facebook in Australia on Thursday morning in response to the federal government’s news media code, which would require the tech giant to negotiate with news publishers for payment for content. The decision continued to make waves globally on Friday, with leaders at the virtual G7 summit discussing the issue and US legislators setting out plans for a series of related antitrust bills, starting with one that would make it easier for small news organisations to negotiate with tech giants by allowing them to work as a group. In Europe, the European Commission president urged the US to join the EU in creating “a digital economy rulebook that is valid worldwide”. Ursula von der Leyen cited the storming of the US capitol as “a turning point for our discussion on the impact social media have on our democracies”, adding: “We just cannot leave decisions, which have a huge impact on our democracies, to computer programmes without any human supervision or to the board rooms in Silicon Valley. The latest decision of Facebook regarding Australia is just another proof for that.” In the UK, Facebook’s Vice-President for Europe, the Middle East and Africa Nicola Mendelsohn defended the decision, telling LBC that the move would establish “an unworkable precedent” and arguing that “publishers choose to put their stories on our news feed because it allows them to sell more subscriptions, it allows them to grow their audiences, and ultimately to increase advertising revenue.” She emphasised that no similar moves were likely in the UK. But the social network was under pressure over the continued blocks on public service content affected by its action. It has blamed the situation on the government’s broad definition of what is considered to be “news” in the code. Hundreds of other pages have been barred from posting content, including health department and emergency services pages, family violence support pages, Western Australia opposition leader Zak Kirkup’s page, and even a page for mums in Sydney’s north shore. Greg Inglis, the managing director of funeral business Picaluna, told Guardian Australia that Facebook had “killed off” his business’s page yesterday, just after he had paid for a marketing campaign on the platform. “We’re just at the very beginning of what for us is quite a big campaign where we’re going to spend quite a bit of money on Facebook,” he said. “And the irony is that they’re cutting off the hand that feeds them. It’s just crazy so it took me two hours down a rabbit hole of trying to find somewhere on Facebook’s website where you can actually contact them.” Inglis eventually found a live chat on Facebook where he had to explain his company was not a news business. “I spent the first 20 minutes of that live chat trying to explain that we’re a small to medium enterprise, we are not a media organisation. He kept coming back and saying ‘yes but you published stories’. I said ‘but we’re not a publisher they’re stories about funerals, we’re a funeral business’.” Inglis was told it could be 72 hours or more before someone would respond to the case lodged by Facebook support. Some other pages were restored on Thursday and Friday, but Guardian Australia understands it could be up to a week before many of the pages are even reviewed. Tim Hanslow, head of social at Preface Social Media and who also helps run the Australian Community Managers group on Facebook, told Guardian Australia he had heard from a couple of community managers who had been contacted by their Facebook representatives and were told an appeals process would be put in place for people to plead their case. He said in a post, shared with Guardian Australia, Facebook had applied the definition of news as per the definition in the code’s legislation. “But they are aware some pages have been incorrectly brought down by the ban. It’s clearly been done automatically. They’re compiling a list of pages incorrectly pulled down,” he said. “An appeals process for the ban will launch on Feb 25 and you can request your page be assessed as outside the news ban. All of the government pages/sites caught up in this should be reinstated.” Guardian Australia has sought comment from Facebook. Australian news sites recorded a steep decline in traffic as a result of the block. Audience tracking company Nielsen reported total sessions for news content declined 16% on Thursday compared with the last six Thursdays, while total time spent declined 14%. The company said 22% of the audience of Australian media publishers in 2020 accessed their content via the Facebook app. Social tracking website Chartbeat also reported overnight that Australian news sites recorded a decline of more than 20% in traffic due to Facebook cutting off news sites. Prior to the change, about 15% of visits to sites within Australia were being driven by Facebook, but after the change, that had dropped to less than 5%, the company said. Although satire sites Betoota Advocate and The Chaser were also initially hit by the ban, they managed to have the blocks removed, and as a result accounted for nine out of the Top 10 posts by Australian pages on engagement on Thursday. The top-performing link posts by Australian Facebook pages on 18 Feb were from: 1. The Chaser2. Betoota Advocate3. Betoota Advocate4. Betoota Advocate5. Ozzy Man Reviews6. The Chaser7. Betoota Advocate8. Betoota Advocate9. The Chaser10. Betoota Advocate — FacebookTop10AU (@FacebookTop10AU) February 18, 2021 The Chaser said on its Facebook page on Friday the attention had brought down the website. “Our first foray into real journalism has been so popular it completely crashed our website. We’re beginning to understand why the Herald-Sun never publishes proper journalism.” Also immune was the YouTube satirist Jordan Shanks, who operates under the moniker Friendlyjordies. Despite promoting himself as a comedian, Shanks has worked with the independent Australian journalist Michael West on stories about New South Wales deputy premier John Barilaro, and would probably fall under Facebook’s application of the government’s proposed definition of “news”. West’s page was hit by the news ban. Separately, dozens of pages and groups dedicated to promoting conspiracy theories, anti-vaccination misinformation and the alt-right have continued to operate unhindered by the company’s broad-brush ban on news content. In most cases, the groups were able to continue posting misinformation via YouTube and websites which escaped Facebook’s definition of news, profoundly reshaping how Australians consumed information on the social media behemoth. One 7,000-member group, which is dedicated to promoting the baseless conspiracy that the 1996 Port Arthur massacre was staged, posted an edited video which uses a 2015 speech by former Liberal party senator Bill Heffernan to falsely allege the existence of a widespread pedophilia network in Australian politics. The conspiracy theory, which has become a key tenet of Australia’s QAnon community, has been shared twice since the ban was introduced. In another Australian group, which is dedicated to vaccination misinformation, links to websites pushing that group’s agenda continued to be visible after the ban, as did posts containing false information about the soon-to-be-released Covid-19 vaccine. Facebook’s ban also missed alt-right operators such as Avi Yemini, who has previously been banned from Facebook for hate speech and has become a key part of the growing conspiracy movement in Australia by promoting a steady stream of content linked to and shared by the anti-lockdown and anti-vaccination groups. Despite calling himself a “journalist”, Yemini and his associated pages continued to post unencumbered to his more than 100,000 followers on Friday. Other far-right pages, including one identified by the Guardian’s hate factory investigation, which uncovered a global network of far-right hate operating for profit through Facebook, also remained unhindered. The Australian government is trying to resolve issues with Facebook, but there is no clear timeline on when or if news media will be restored. Federal treasurer Josh Frydenberg held his second meeting in two days with Facebook chief executive Mark Zuckerberg about the company’s ongoing issues with the code, and said on Friday the pair would talk again over the weekend. Today I had a further conversation with Facebook’s Mark Zuckerberg. We talked through their remaining issues & agreed our respective teams would work through them immediately. We’ll talk again over the weekend. I reiterated Australia remains committed to implementing the code. — Josh Frydenberg (@JoshFrydenberg) February 18, 2021 The legislation for the code passed the House of Representatives on Wednesday night, and is expected to be debated in the Senate as soon as next week. The prime minister, Scott Morrison, repeated that the government would not be backing down from the code. “I would just say to Facebook: this is Australia, if you want to do business here, you work according to our rules. That is a reasonable proposition. We’re happy to listen to them on the technical issues of this, just like we listened to Google and came to a sensible arrangement,” he said. “But the idea of shutting down the sorts of sites they did yesterday, as some sort of threat, I know how Australians react to that and I thought that was not a good move on their part.', 'G08': \"Bitcoin's market value exceeds $1tn after price soars Joanna Partridge Bitcoin hit a new high on Friday, giving it a market value of more than $1tn for the first time. Bitcoin rose 6.4% during trading on Friday to reach an all-time high above $55,000, and was on track for a weekly gain of about 14%. The surge took its market capitalisation – the value of all bitcoin in circulation – to more than $1tn, according to the data website CoinMarketCap. On Wednesday, bitcoin burst through the $50,000 mark for the first time, as it continued to attract interest from mainstream investors and acceptance from financial institutions. Bitcoin has enjoyed a meteoric rise in 2021, gaining more than 60% during the month of February alone. Its popularity has been boosted in recent weeks by high-profile purchases by the electric car company Tesla, run by the billionaire Elon Musk, while the US bank Morgan Stanley has said its investment fund is looking at a large purchase of bitcoins. Tesla sent bitcoin’s value skywards when it revealed earlier in February it had bought $1.5bn (£1.1bn) in bitcoin and said it might soon accept payment in the cryptocurrency. Musk has regularly tweeted that he is a supporter of cryptocurrencies, which have been favoured by some investors while global interest rates remain at record lows. He wrote on Twitter on Friday that bitcoin was a “less dumb form of liquidity than cash”. Despite their growing popularity, digital currencies remain controversial. Regulators including the Bank of England are sceptical about cryptocurrencies on account of their volatility and vulnerability to theft or hacking. The governor of the Bank of England, Andrew Bailey, told an online conference hosted by the World Economic Forum in January that he did not believe any existing cryptocurrencies would last in the long term. Bitcoin has been on a rollercoaster ride in the last few years. In March 2020 it was trading below $6,000. In 2016 a single coin was worth less than $400. Neil Wilson, the chief market analyst at the financial trading platform Markets.com, noted that bitcoin had received “yet more institutional support”. He said the boss of the US investment firm DoubleLine Capital, Jeffrey Gundlach, a longtime proponent of investing in gold, “is backing bitcoin as the asset to insulate investors against the great monetary inflation”.\", 'G09': 'Forensic Architecture: the detail behind the devilry Rowan Moore In 2006 a man walked into an internet cafe in Kassel, Germany, and shot dead Halit Yozgat, a 21-year-old member of the Turkish-German family who owned it. It was the ninth in a series of racist killings by neo-Nazis, the motivation for which the police persistently refused to admit. A striking fact of Yozgat’s murder was that Andreas Temme, an intelligence agent for the state of Hessen, was in the cafe at the time, logged on to a dating website in a back room. If there’s one thing a secret agent should be able to do, you might have thought, it would be to notice a killing in the next room, but Temme claimed he did not. He took part in a police video reconstruction in which he is seen placing his payment for his internet access on the reception table, unaware of the corpse on the floor behind it. His story didn’t seem likely, but in the absence of further evidence it seemed that he would have to be taken at his word. That might have been that, were it not that Forensic Architecture investigated the case and exhibited their findings at the 2017 edition of Documenta, Kassel’s five-yearly art fair. Through creating a full-scale mock-up of the cafe interior, and analysing the sound of the two shots (loud enough, even with a silencer), the dispersal of their smoke and the sightlines of the agent – a tall man – as he put money on the table behind which the young victim was sprawled, it was demonstrated that Temme could not possibly have failed to hear, smell and see the crime. Forensic Architecture, whose work is going on show next month at the Institute of Contemporary Arts in London, is an agency based at Goldsmiths, University of London. The organisation’s founder and director is Eyal Weizman, a British-Israeli architect. Its primary mission is research, to “develop evidentiary systems in relation to specific cases”; in so doing, it acts as “an architectural detective agency”, working with NGOs and human rights lawyers to uncover facts that confound the stories told by police, military, states and corporations. “We think that architects need to be public figures,” says Weizman. “They should take positions, whatever they do. We map the most extreme and violent forms.” “We’re building a new sub-discipline of architecture,” he adds. “We just have to figure it out.” They use whatever means they can to reconstruct a hybrid of physical and virtual space – the metadata surrounding phone calls and phone-camera videos, meteorology, eyewitness accounts, reconstructions. They might scrape thousands of images of a bombing off social media and match them with material facts to fix facts in space and time, as if with the coordinates of a multidimensional map. They learn from ancient as well as modern methods, such as the memorising techniques of Roman orators and Elizabethan actors, when helping ex-prisoners reconstruct the monstrous and secret prison of Saydnaya in Syria. They are engaged in a game of wits with military and security services. Their arena is shaped by surveillance and data collection – factors that give rise to well-founded fears that they might be abused by power. Forensic Architecture aims to make these techniques benefit rather than harm human rights. In a world saturated by images, where seemingly almost everything is exposed to view, they try to make visible those things that are kept hidden. They prefer to call their activity “counter-forensics”, “forensics” being “the art of the police”. The material is harrowing: to see, for example, from several CCTV camera positions, the daily life of an Aleppo hospital in the seconds before it is obliterated by pro-regime forces. “You never get used to it,” says Weizman. The work is also compelling, both in the inventiveness, precision and patience of the processes and the crystalline outcomes. It might take a year to reconstruct a day, as it did with the events of Black Friday, 1 August 2014, when 2,000 Israeli bombs, missiles and shells were dropped on the city of Rafah, in the Gaza strip. But Forensic Architecture’s research into that day contributed to the cancellation of the “Hannibal Directive”, a classified policy whereby the Israeli military might kill their own soldiers if they are taken prisoner, rather than allow them to become hostages. This is not the point where most architecture students expect to end up. After studying at the Architectural Association in London, Weizman set up a practice in a conventional enough way for young architects in Tel Aviv. What changed everything was his decision to do a PhD on the ways in which town planning in the occupied territories was used to divide and suppress. “I was trying to show that there could be human rights violation by architecture and planning,” he says, “and that architects can be complicit.” Forensic Architecture’s reconstruction of the abduction of 43 students in Iguala, Mexico in 2014. Photograph: Forensic Architecture Asked to contribute to an exhibit of young Israeli architects in Berlin in 2002, he presented a show on settlements, which led to the Israel Association of United Architects cancelling the exhibition and destroying the catalogues. Like much censorship, it made the name of its target, and that year Weizman managed to exhibit his work at the Israeli pavilion in the Venice architecture biennale. From there he “accelerated from the slow violence of architecture and planning” to the rapid violence of warfare and displacement. He founded Forensic Architecture in 2011, their “expertise gradually growing” since. Their areas of interest expanded beyond Israel and Palestine to wherever they might be needed: Kassel, Syria, the disappearance of students in Iguala in Mexico, a lethal factory fire in Karachi, a detention centre in Cameroon where torture and executions took place with the apparent connivance of US personnel based there. They are “on the side of civil society” and won’t take commissions from government or corporations, but don’t take political sides. Which has given them a wide range of enemies and detractors who, while not equivalent in the outrageousness of their regimes, have some attitudes in common. Forensic Architecture have been dismissed by Germany’s ruling CDU party as factually challenged artists, by Assad as Qatari stooges, by the Kremlin-backed RT TV network as supporters of Islamic State. In Israel they get called “Pallywood”, as in Palestinian Hollywood. “The bastards’ last line of defence is to call it ‘fake news’,” says Weizman. “The minute they revert to this argument is when they’ve lost all the others.” It is indeed notable that Forensic Architecture use arts venues such as Documenta and the ICA among their means of getting their messages out. This might seem a bit dilettante, in relation to the hard facts of human rights disputes and war crimes cases, but Weizman argues that the means of representation, the ways in which research is communicated to the public, are vital. The proof is in Forensic Architecture’s work, which stands up in court, gets ministers to recant their previous statements, and changes lives and, sometimes, policy. Their theoretical explorations seem to help rather than hinder the factual aspects of their work. A reconstruction of the altercation of search and rescue operations in the central Mediterranean on 6 November 2017. Photograph: Forensic Oceanography and Forensic Architecture Weizman also still considers Forensic’s activities to be a way of practising architecture, wielding the profession’s potential for synthesising the contributions of multiple other disciplines. He is quick to pay tribute to the affiliated research project Forensic Oceanography, who investigated with Forensic Architecture claims that NGOs rescuing migrants off the coast of Libya were colluding with people smugglers. Forensic Architecture’s work might require knowledge of the law or munitions, on which they collaborate with the appropriate experts. Members of the agency who are not architects include an investigative journalist, a programmer, a film-maker and a preservationist. They borrow from the speculative end of the architectural profession, with surprisingly useful results. An example is parametricism, a fad that swept through architecture schools in the first decade of this millennium, which tried to design buildings by feeding data into sophisticated software that then generated exotic but intractable forms. “As a design strategy,” says Weizman, “parametricism is futile and decorative.” Forensic Architecure instead applied its techniques to analysing the clouds caused by the bombing of Rafah. Forensic Architecture founder and director Eyal Weizman. Photograph: Justin Tallis/AFP/Getty Images “A bomb cloud is everything a building was,” he says, “in gas form: plaster, concrete, wood, flesh. It’s horrible, horrible, devilish dust.” But you can “reconstruct its force fields out of its form”. Every cloud has a “fingerprint”, a moving one, which means you can pinpoint the place and time from which a photograph was taken from the shape of the cloud. By triangulating and coordinating all the information thus gained, Forensic Architecture could locate particularly large and lethal explosions. They could also calculate the dimensions of the bombs – and hence their weight of explosives – caught in one of the photographs before they hit. In Rafah their evidence made a convincing case that the object of the onslaught was to kill an Israeli officer who had been captured that morning, who the military believed was in an underground tunnel. That there was huge collateral damage to civilians didn’t seem to be much regretted. It was a particularly aggressive interpretation of the Hannibal Directive that was in due course cancelled or at least clarified – a triumph of which Forensic was part, although as Weizman points out one that is conditional on whatever takes the directive’s place. Clear-cut victories in this business seem to be rare – it’s still not known, for example, why Temme was in the fatal cafe and why he claimed not to have noticed the shooting. But in the constant struggle to protect truth from becoming a casualty, Forensic’s version of architecture is a powerful weapon.', 'G10': 'Robots on the rise as Americans experience record job losses amid pandemic Lauren Aratani They can check you in and deliver orange juice to your hotel room, answer your questions about a missing package, whip up sushi and pack up thousands of subscription boxes. And, perhaps most importantly, they are completely immune to Covid-19. While people have had a hard time in the coronavirus pandemic, robots are having a moment. The Covid-19 pandemic has left millions of Americans unemployed – disproportionately those in the service industries where women and people of color make up the largest share of the labor force. In October, 11 million people were unemployed in the US, compared with about 6 million people who were without a job during the same time last year. And as humans are experiencing record job losses and economic uncertainty, robots have become a hot commodity. Multiple technology manufacturers have reported increased demand for their bots over the course of the pandemic, from drone-like machines that can roam hallways to make deliveries and AI-powered customer service software to increased use of self-service checkouts at supermarkets. A recent report from the World Economic Forum predicted that by 2025 the next wave of automation – turbocharged by the pandemic – will disrupt 85m jobs globally. New jobs will be created but “businesses, governments and workers must plan to urgently work together to implement a new vision for the global workforce”. The hospitality industry, which has been one of the hardest-hit by the pandemic, has seen a clear uptick in the adoption of new technology during the pandemic. Hotels are allowing guests to use kiosks to check themselves in, apps to control the television and light switches in their room and a few use delivery bots to send to guests’ room when they want a refreshment. Ron Swidler, chief information officer of the Gettys Group, a hotel design and development consultancy firm, said more hotels are experimenting with new technology during the pandemic. Swidler leads the Hotel of Tomorrow, a consortium of hospitality leaders that was re-upped in the middle of the pandemic to think of ways to innovate the industry. The group came up with five “big ideas” on how the industry needs to change, and new technology – including robots – are a core part of the equation. “The cost [of automation] is coming down, the technology is getting better and we are seeing innovation working effectively in other parts of the world that we can transfer here,” Swidler said, citing Alibaba’s FlyZoo hotel that is staffed nearly entirely by technology, from check-in to room service. While the idea of being serviced by a BB-8 lookalike in a hotel may seem strange, Swidler said permanent job losses in the industry will be a reality as hotels adopt new technologies to try to save on labor costs. It is unclear whether the increased demand for new technology has directly caused job losses during the pandemic, but a discussion paper published by the Federal Reserve Bank of Philadelphia in September found that “automatable” jobs – occupations that could be replaced by technology that is in development or is already available – lost 4.2 more jobs per every 100 than occupations that are less at risk for automation. Occupations that are considered automatable include hotel desk clerks, shuttle drivers and retail salespeople, according to the paper. The paper’s authors raise the widely shared concern that the automation undertaken during the pandemic will be a permanent replacement for jobs. “The longer time it takes to fully control the virus, the higher the probability that the labor-saving technology will become permanent,” said Lei Ding, senior economic advisor at the Federal Reserve Bank of Philadelphia and co-author of the paper. “Job losses will become permanent losses.” Currently, there are only anecdotal examples of permanent job loss due to an uptick in automation brought on by the pandemic, but the layoffs of hundreds of Pennsylvania toll booth workers provides one clear example of how labor-saving technology can sweep away jobs. In June, the Pennsylvania Turnpike Commission laid off about 500 toll collectors in the state when it switched to all-electronic toll collecting. For years, the commission had talked about replacing toll booth workers with automated collectors, and they finally gave workers a timeline. Per a union agreement, workers were supposed to be kept on payroll until at least October 2021, with final layoffs happening by January 2022. When the pandemic arrived, collectors were sent home in March and were promised that the commission would still uphold the October 2021 date. But in June, the commission permanently laid off all workers, over a year before the agreed date. “We understand the safety of employees is the most important thing, but for them to have safety mean the elimination of their jobs … It’s been devastating,” said Jock Rowe, principal officer for Teamsters Local 77, the union representing 300 of the laid-off toll workers. Rowe cited other toll-collecting agencies that brought back toll workers with enhanced safety measures, including the Port Authority of New York and New Jersey. The impact of a recession on the growth of automation has been well-documented by economists and has shown that automation does not grow steadily, but rather happens in bursts. Businesses are more likely to automate after experiencing economic shocks, when they have strong incentives to save on labor. For a study published in 2016, researchers from the University of Rochester combed through 87m job postings online from before and after the Great Recession. They found that employers in cities that were hit hardest by the recession were replacing workers with labor-saving technology and more skilled workers. A report published by the Century Foundation found that “robot intensity” increased in 2009, in the immediate wake of the Great Recession, particularly in the manufacturing industry. While an increase in automation can be good for educated workers and help to stimulate the economy, studies have also shown that new technology tends to leave low-wage workers behind. “Automation has been a major driver in the increase in inequality,” said Daron Acemoglu, an economist at the Massachusetts Institute of Technology. Acemoglu co-authored a study published in May that showed automation creates a “prosperity gap” that benefits high-skilled workers at the sake of lower-skilled workers. Low-wage workers are not only more susceptible to job loss and wage depression due to automation, but they also experienced the most job losses due to shutdowns. Higher-wage workers are more likely to be able to work from home during the pandemic, while lower-wage workers – a disproportionate number of whom are Black or Hispanic – were more susceptible to layoffs due to shutdown orders. An important caveat many roboticists will point out is that artificial intelligence technology is not quite smart enough to cause mass waves of layoffs due to robots. New AI technology can take a lot of money, time and resources to set up, something that many businesses do not have during the pandemic. “You should definitely not worry about losing your job to an AI-enabled robot right now. If you’re going to lose your job to automation, it’s going to be … some proven, well-known automation that is more than 10 or 15 years old,” said Matt Beane, an assistant professor at the University of Santa Barbara’s Technology Management Program. AI has “tremendous potential for making humans more productive” without replacing humans, Acemoglu said, if society takes a human-centric approach to technological advances. But without the political will to make sure those who do lose jobs are taken care of, by training them for new jobs, for example, the impact of automation may be devastating and a pandemic that has already hit those workers hardest could be leave a lasting legacy of unemployment. “I’m not saying automation is terrible … What I’m saying is it would be terrible if we put all the eggs in the automation basket,” Acemoglu said. “We have to a large extent done so over the last 30 years. [The pandemic] will just exacerbate that.”', 'G11': 'UK wants new drones in wake of Azerbaijan military success Dan Sabbagh The UK’s military is expected to embark on a new armed drone programme in response to Azerbaijan’s controversial use of the technology in its victory over Armenia in the Nagorno-Karabakh conflict. Defence officials believe that Azerbaijan’s use of cheaper Turkish drones in the six-week autumn war was crucial in defeating the Armenians, and forcing them to cede control of territory in the disputed Caucasus region. Ministry of Defence sources added that the UK wanted to procure its own cheaper drones as part of the five-year defence review due to be unveiled early in 2021, despite warnings about the risks of the proliferation of deadly unmanned aircraft. Earlier this month Ben Wallace, the UK defence secretary, said that Turkish TB2 drones were an example of how other countries were now “leading the way”. The drones, he added, have “been responsible for the destruction of hundreds of armoured vehicles and even air defence systems”, although there is video evidence that suggests they also killed many people in the Nagorno-Karabakh war. In mid-October, graphic footage in black and white emerged from Clash Report, a Twitter and Telegram account closely linked to the Turkish military, appearing to show a string of TB2 strikes targeting Armenian positions against a backdrop of jingoistic music. Other graphic footage posted by Azerbaijan’s defence ministry in October shows what are said to be TB2 drones picking out Armenian forces and using the information to call in deadly rocket fire from elsewhere. Manufactured by Baykar Makina, the TB2 drones cost as little as $1m to $2m each according to analyst estimates, far less than the near $20m per drone paid by the British military for a fleet of 16 high-end, next-generation Protector drones manufactured by US specialist General Atomics. The TB2 drones have a much shorter operating range of up to 150km, but are able to loiter in the air for up to 24 hours. Because they are cheaper, military forces can afford to lose some in action. Turkey’s TB2 drones have been rapidly altering the military balance in the region, and have been heavily used in strikes against Kurdish opposition both inside and outside the country and in Libya, in the country’s civil war. In the summer, on the eve of the conflict, Azerbaijan purchased TB2s from Turkey – two dozen on some estimates – and deployed them so quickly and effectively it is widely believed they were operated by Turkish pilots. Drone footage was also broadcast on digital billboards in the Azerbaijani capital of Baku. “There’s a huge PR element to this,” said Rob Lee, a doctoral student at King’s College London’s war studies department, who has been closely following the conflict. “In an environment where there is not much independent information this helped the Azeri government to control the narrative.” A Russian-brokered truce between the two sides was signed on 9 November. Azerbaijan kept the territory it had gained while Armenia was forced to withdraw from land it had controlled adjacent to Nagorno-Karabakh. Around 5,000 soldiers from both sides were killed but analysts on the Oryx Blog, relying on publicly available pictures and video, estimated that Armenia lost 224 tanks compared with 36 from Azerbaijan. “The Azeris use of drones was decisive,” added Prof Michael Clarke, a distinguished fellow at the Royal United Services Institute (Rusi), a military thinktank. Several TB2 drones were downed by Armenian forces, revealing how Baykar is able to make them relatively cheaply. A report released by the Armenian National Committee of America in November included photos of components used in the damaged drone, including a navigation system from Garmin. Garmin said its product was “not designed or intended for military use” and said it was asking its dealers to halt sales to Baykar. In October, Canada also suspended exports to Turkey of targeting gear made by Ontario-based Wescam – a subsidiary of US firm L3Harris – after they were found in a TB2 drone. The document also lists two possible British components, a fuel pump made by Hampshire firm Andair, with the firm’s name clearly marked, and a bomb rack missile release system that was first developed in the UK by another L3Harris subsidiary, Brighton-based EDO MBM Technology – although Baykar said it had developed its own version since. Andair did not reply to a request for comment. The UK complies with a 1992 arms embargo relating to all weapons that could be used in Nagorno-Karabakh. When asked specifically about the components cited by the Armenian document, the Department for International Trade said: “We have not issued licences contrary to the arms embargo.” One expert said the UK’s drone plans would legitimise a technology that could promote conflict in disputed areas. Chris Coles, director of NGO Drone Wars UK, said: “Civil society groups have been warning for some time that because drones lower the cost of warfare, they are likely to fuel this type of bitter, lethal conflict between neighbouring states.”', 'G12': 'Patient data from GP surgeries sold to US companies Toby Helm Data about millions of NHS patients has been sold to US and other international pharmaceutical companies for research, the Observer has learned, raising new fears about America’s growing ambitions to access lucrative parts of the health service after Brexit. US drugs giants, including Merck (referred to outside the US and Canada as MSD, Merck Sharp and Dohme), Bristol-Myers Squibb and Eli Lilly, have paid the Department of Health and Social Care, which holds data derived from GPs’ surgeries, for licences costing up to £330,000 each in return for anonymised data to be used for research. Campaigners working to protect the privacy of patients’ medical histories said they were concerned at the lack of transparency that surrounded the sale of licences and a lack of clarity about what the data was being used for. The most recent accounts of the government organisation that issues the licences, Clinical Practice Research Datalink or CPRD, reveal it received more than £10m in revenue last year. “Patients should know how their data is used. There should be no surprises. While legitimate research for public health benefit is to be encouraged, it must always be consensual, safe and properly transparent,” said Phil Booth, coordinator of medConfidential, which campaigns for the privacy of health data. “Do patients know – have they even been told by the one in seven GP practices across England that pass on their clinical details – that their medical histories are being sold to multinational pharma companies in the US and around the world?” On Saturday, Dr June Raine, chief executive of the Medicines and Healthcare products Regulatory Agency (MHRA), a government body that includes CPRD, said the sale of data under licence to commercial organisations, as well as research bodies such as universities, had been fully compliant with “ethical, information governance, legal and regulatory requirements”. “Rigorous processes” were in place to ensure the privacy of patients, she added. “Ethically conducted research using CPRD patient data sets has brought enormous benefits to patient care, including providing evidence for the National Institute for Health and Care Excellence (NICE) blood-pressure targets for patients with diabetes, as well as working with universities, regulators and the pharmaceutical industry who research the safety of their medicines,” Raine said. Last week, a leak of secret government papers about private discussions between UK and US officials over a post-Brexit trade deal showed that the “free flow of data” was a “top priority” for the US. America appears to be pressing for unrestricted access to Britain’s 55 million health records, which are estimated to have a total value of £10bn a year. A minute of one of the meetings says: “On data flows, the critical element highlighted by the US was agreement that no parties will restrict information.” Another US demand is for “data localisation” to be ruled out, meaning the data of NHS patients could be stored on cloud servers abroad. US ambitions to access UK health markets have become a major issue in the general election campaign, with Labour’s leader, Jeremy Corbyn, repeatedly accusing the Tories of preparing to open up the NHS to US businesses during talks on a post-Brexit trade deal with Washington. Boris Johnson has rejected the claims, saying the NHS is not, and never will be, for sale. Labour has vowed to review the sale of health data if it wins the election. Corbyn said on Saturday that the revelations proved there was “nothing the Conservatives won’t sell to keep US big business happy”, adding that a government that sold “our most private data can’t be trusted with our NHS”. “Boris Johnson’s toxic trade deal with Trump will mean the NHS being cut up and quietly sold off, bit by bit,” he said. “This election will decide the future of our health service. While the Tories will use our NHS as a bargaining chip, Labour will pass a binding law to ensure that the NHS is never on the table and give it the funding it desperately needs.” Booth said there was a lack of transparency over the sale of data, with information about the companies buying it – and the uses to which they put it – not displayed clearly on the relevant government agency sites. He said there was evidence on CPRD’s website that US companies additional to those featured on a list provided to the Observer (after an official request was made) were using UK data. “Our detailed examination of CPRD’s approved studies shows multibillion-dollar companies like Optum, Pfizer and Sanofi named in multiple studies over the past year, but not on the list of licensed commercial organisations its officials provided to the Observer. “Such omissions and lack of transparency are deeply concerning. What else don’t we know?', 'G13': 'Baffled by digital marketing? Find your way out of the maze Don’t know your SERPs (search engine results pages) from your PPC (pay per click)? Not sure when you should be favouriting on Instagram or pinning on Pinterest? Don’t worry, you’re not the only one. Digital marketing is an umbrella term that refers to the promotion of products and brands via electronic media. It’s how a brand uses its social profiles, websites and apps to promote itself. So what are the different types of digital media you might use and what can you get from them? These tools can be split into often interchangeable sectors, including search engine optimisation (SEO), content marketing, social media marketing, display ads and email marketing. Email marketing You’re probably already familiar with this – it’s those emails you keep getting from brands with quirky titles that are trying to sell you something. And it’s not as simple as throwing email addresses and some offers into an email distribution tool (such as Mailchimp) and hoping for the best. The people who do this well segment their audience to make every email relevant to the group that receives it. They’re experts at data collection (generally getting people to sign\\xad up for things). And they know lots about heat maps, which track the percentage of people who have interacted with different parts of a webpage, and open rates, which show what percentage of the people who received your email have opened it. Display advertising Display ads are part of the paid branch of digital marketing. They are image and, increasingly, audio-\\xadled ads that pop up on web pages and often stay there a while. They can be effective but need to be used carefully – you don’t want to annoy your customers. Social media marketing Social media marketing can take two distinct forms: paid and organic. Paid marketing includes Facebook promoted posts and ads. Facebook can be a particularly effective tool as you can be very specific with your targetting, choosing who sees your ads or promoted posts on their Facebook feed based on their age, gender, interests and the type of device they use. Most social media platforms have some form of paid promotion like this. Organic social promotion is simply posting to your existing social audience. It’s free, but certain social platforms have started to limit your organic reach so that updates don’t go out to all your followers – you have to pay for that. For example, rather than displaying all posts chronologically, Facebook now gives each story a score based on users’ previous interactions, the format of the post and its age. There’s no secret sauce when it comes to improving organic reach, but it is possible to improve your score by publishing content that adds value for the reader and includes an image or a video. Twitter is also getting in on the act, recently announcing changes to its algorithm that let users choose whether they want to see chronological posts or the “best” – posts with lots of likes and retweets from people they follow. Improving organic reach really comes down to posting content that attracts engagement – posts that entertain, inform or add value. Content marketing This can be done using paid media such as display ads, social ads, Google ads, earned media (relationships you build with others who help promote your content) and owned media (your website, app and social media channels). The content can take many forms – text, images, gifs, video – and the form it takes will play a huge part in where you place it. A florist may use a visual channel such as Instagram or Pinterest to promote their business, or a video to show potential customers how to create the perfect bouquet. A blogpost might work better for a marketing company – it might create detailed guides to content marketing, which are then posted on networks such as Facebook or Twitter. Search engine optimisation SEO is probably the most misunderstood element of digital marketing. In the past it has always been interchangeable with content marketing (or what was known as link building). This is still the case to an extent, but the two are starting to separate. SEO is optimising your site to rank as highly as possible in search engines. Google, in determining where your site ranks, takes into account 200\\xad or so factors, including site speed, backlinks (links pointing to your site), social media signals and site usability. Link building is a key part of SEO but must be carefully used to ensure impact and avoid penalisation by search engines. Link building falls into three types: from companies that naturally link to your content and require no effort – on relevant sites as part of an outreach effort – and non-editorial links on forums, blog comments and user profiles. The latter is considered spammy and should be used with caution. As long as links are relevant and are not being used to cheat the system, link building can be very effective. The sheer number of factors Google considers for site ranking goes some way to illustrate the complexity of SEO. However, a well-structured, user-friendly website with high quality, regularly updated content will always be beneficial to search engine rankings. James Cotton is CEO of Onespacemedia', 'G14': 'End-to-end encryption protects children, says UK information watchdog Dan Milmo The UK data watchdog has intervened in the debate over end-to-end encryption, warning that delaying its introduction puts “everyone at risk” including children. The Information Commissioner’s Office said strongly encrypting communications strengthens online safety for children by reducing their exposure to threats such as blackmail, while also allowing businesses to share information securely. The watchdog was responding to the launch of a government-backed campaign that said social media platforms would be “willingly blindfolding” themselves to child abuse if they pushed ahead with end-to-end encryption for private messaging. “E2EE [end-to-end encryption] serves an important role both in safeguarding our privacy and online safety,” said Stephen Bonner, the ICO’s executive director for innovation and technology. “It strengthens children’s online safety by not allowing criminals and abusers to send them harmful content or access their pictures or location.” Child safety campaigners have warned that the encryption plans would prevent law enforcement, and tech platforms, from seeing messages by ensuring that only the sender and recipient can view their content – a process known as end-to-end encryption. Mark Zuckerberg’s Facebook Messenger and Instagram apps are preparing to introduce end-to-end encryption and their plans were strongly criticised last year by the home secretary, Priti Patel, who described them as “not acceptable”. Bonner said accessing encrypted content was not the only way to catch abusers. Other methods used by law enforcement include infiltrating abuse rings, listening to reports from children targeted by abusers and using evidence from convicted abusers. He added: “Until we look properly at the consequences, it is hard to see any case for reconsidering the use of E2EE – delaying its use leaves everyone at risk, including children.” The ICO, which oversees the protection of people’s data in the UK, believes that end-to-end encryption is one of the most reliable ways of protecting the data of people who use large messaging platforms. Bonner said encryption protects children by preventing criminals and abusers from accessing their pictures – which could expose them to the risk of blackmail – and their location. Meta, the owner of Messenger and Instagram, said in November last year it would delay its end-to-end encryption plans by a year to 2023. Its WhatsApp messaging service already uses end-to-end encryption. Damian Hinds, the home office minister, said the government-backed campaign, called No Place to Hide, was calling for a “more balanced debate” on the issue. Writing in the Times, he added: “There is a risk that end-to-end encryption, without the right safety capabilities, blinds companies and law enforcement, taking us backwards. Neither this government, nor society as a whole could accept that.” Responding to the ICO, the NSPCC said end-to-end encryption offered privacy benefits but put children at risk if it was poorly implemented. “That’s why the NSPCC wants companies to risk assess end-to-end encryption and balance the privacy and safety requirements of all users, including young people, to ensure it is rolled out in the best interests of the child,” said Andy Burrows, head of child safety online policy at the charity.', 'G15': 'What is GDPR and why does the UK want to reshape its data laws? Alex Hern The government has announced plans to reshape the UK’s data laws such as GDPR requirements in an effort, it claims, to boost growth and increase trade post-Brexit. The digital, media and culture secretary, Oliver Dowden, says the UK wants to shape data laws based on “common sense, not box-ticking”. What is GDPR? The General Data Protection Regulation was a replacement for the EU’s 1995 Data Protection Directive, which had until then set the minimum standards for processing data in the bloc. GDPR significantly strengthened a number of rights: individuals found themselves with more power to demand companies reveal or delete the personal data they hold; regulators were able to work in concert across the EU for the first time, rather than having to launch separate actions in each jurisdiction; and their enforcement actions had real teeth, with higher maximum fines for breaches. Why does GDPR matter if we’ve left the EU? As a European “regulation”, GDPR became UK law the second it was put into effect, on 25 May 2018. If the government had left it at that, it would have ceased to take effect on 1 January 2021, when the UK’s exit from the EU was finalised. But the 2018 Data Protection Act, introduced by Theresa May’s government under the then media and culture secretary Matt Hancock, rewrote the UK’s own data protection laws to mirror GDPR, so there would be no conflict between British and European law. This meant that when Britain left the EU, the Data Protection Act continued to apply rules that were functionally equivalent to GDPR – but it is now in the government’s power to alter those rules. What is stopping the government from ripping up the rulebook entirely? International transfers of data rest on what are called “adequacy agreements”. People cannot transfer data internationally unless their government agrees that data protection rights in that country are at least as good as their own. Those agreements are crucial. The EU, for instance, has spent years tussling with the US over whether the country provides adequate safeguards for EU citizens’ data, particularly when it comes to protection from government surveillance. The Edward Snowden revelations torpedoed the previous “safe harbour” finding that the US was good enough, and the resulting ramifications are still being felt today. If the government goes too far in changing the rules, it would run a similar risk. It knows adequacy is important: alongside Thursday’s announcement was a promise that the UK would seek such an agreement with six countries, including the US, South Korea and Australia, as well as the confirmation that the probable next information commissioner, the New Zealand privacy commissioner, John Edwards, has “vital” experience bringing his own country in line with the EU’s requirements. But what about the cookie banners? Despite being the public-facing image of GDPR, cookie banners have little to do with the regulation. In fact, they predate GDPR itself, going back to the EU’s 2002 ePrivacy directive. But the government could, as part of its overhaul of the UK’s data protection rules, strip away the requirement for websites to ask permission for low-impact uses of personal data, Dowden has suggested. Less clear is whether removing that requirement would have much effect in practice. Websites will continue to need to implement cookie banners for European users and similar legislation applies in California. Many may consider it easier to simply continue to ask British users for their consent to tracking, even if they no longer have to.', 'G16': 'Algorithms are more like puppies than monsters, they want to please you Matt McAlister Algorithms are often characterised as dark and scary robotic machines with no moral code. But when you open them up a little and look at their component parts, it becomes apparent how human-powered they are. Last month, Google open sourced a tool that helps make sense of language. Giving computers the power to understand what people are saying is key to designing them to help us do things. In this case, Google’s technology exposes what role each word serves in a sentence. The technical jargon for it is natural language processing (NLP). There is mathematics cooked into the tools, but knowing what sine and cosine mean is not a prerequisite to understanding how they work. When you give Google’s tool or any NLP system some text, it uses what it has been told to look for to decipher what it is looking at. If the creators taught it parts of speech then it will find nouns, verbs, etc. If the creators taught it to look for people’s names then it will identify word pairs and match them against lists they were given by trainers. The computer then processes the things it found and provides results. As the one asking for results, users have to decide things like whether to exclude words with relevance scores at certain thresholds or maybe to only match words in a whitelist they have provided. Different tools provide different types of results. Maybe the tool is designed to look for negative or positive sentiment in the text. Maybe it’s designed to identify mentions of city streets. Maybe it’s designed to find all the articles in a large data set that are talking about the same subject. Many startups today are using NLP to inform artificial intelligence systems that assist with everyday tasks such as x.ai’s calendar scheduler. Over time we are going to see more startups using these tools. But there are many practical things that media companies can do with NLP today, too. They might want to customise emails or cluster lots of content. I can imagine publishers creating ad targeting segments on the fly using simple algorithms powered by NLP systems. It’s worth noting that Google is actually late to the game, as many solutions already exist. IBM’s AlchemyAPI will look at text users supply it and then return data about relationships between people, places and things. There is an open source solution called OpenNLP from the Apache Foundation. Apache is also where you find Lucene, a popular search service used by companies such as Elasticsearch that can solve similar problems that NLP systems solve. At their most basic level, these technologies essentially automate decisions at scale. They take a lot of information in; they work out what answers resolve certain kinds of questions based on what people teach them; and then they spit out an answer or lots of answers. But every step of the way, people have told the computers what to do. It is people who provide the training data. It is people who instruct the algorithms to make the decisions they want made on their behalf. It is people who apply the results the algorithm returns. These tools are particularly powerful when they are given the authority to make lots of decisions quickly that could never be done by hand. And that is also where problems emerge. Lots of small errors in judgment can turn into an offensive or even threatening force. Sometimes adverse effects are infused accidentally. Sometimes they are not. And sometimes unwanted behaviour is really just an unintended consequence of automating decisions at scale. Like any new technology we don’t yet have a clear model for understanding and challenging what people are doing with algorithms. The Tow Center’s recent book on algorithms dives into the issues and poses important questions about accountability. How has the algorithm been tuned to benefit certain stakeholders? What biases are introduced through the data used to train them? Is it fair and just, or discriminatory? It’s a great piece of research that begins to expose the implications of this increasingly influential force in the world that effectively amplifies commercial and government power. The key, according to the Tow report, is “to recognise that [algorithms] operate with biases like the rest of us”. Algorithms aren’t monsters. Think of them more like puppies. They want to make you happy and try to respond to your instructions. The people who train them all have their own ideas of what good behaviour means. And what they learn when they are young has a profound effect on how they deal with people when they’re grown up. As policy folks get their heads around what’s going on they are going to need some language to deal with it. Perhaps we already know how to talk about accountability and liability when it comes to algorithms. California’s dog laws state that the owner of the dog is liable if it hurts someone. It reinforces that by saying the owner is liable even if they didn’t know their dog could or would hurt someone. Finally, being responsible for the dog’s behaviour also means the owner must do what a court decides which may include “removal of the animal or its destruction if necessary.” A policy with similar foundations for algorithms would encourage developers to think carefully about what they are training their machines to do for people and to people. Maybe it’s overkill. Maybe it’s not enough. But let there be no confusion about who is creating these machines, teaching them what to do, and putting them to work for us. It is us. And it is our nature that is being reflected, multiplied and amplified through them. Tools like natural language processing are merely paint brushes used by the artists who dream them up.', 'G17': 'The next giant leap: why Boris Johnson wants to ‘go big’ on quantum computing Dan Milmo The technology behind everyday computers such as smartphones and laptops has revolutionised modern life, to the extent that our day-to-day lives are unimaginable without it. But an alternative method of computing is advancing rapidly, and Boris Johnson is among the people who have noticed. He will need to push the boundaries of his linguistic dexterity to explain it. Quantum computing is based on quantum physics, which looks at how the subatomic particles that make up the universe work. Last week, the prime minister promised the UK would “go big on quantum computing” by building a general-purpose quantum computer, and secure 50% of the global quantum computing market by 2040. The UK will need to get a move on though: big steps have been taken in the field this year by the technology superpowers of China and the US. Peter Leek, a lecturer and quantum computing expert at Oxford University, says “classical” computing (the common term for computing as we know it) has been an incredible 20th-century achievement, but “the way we process information in computers now still doesn’t take full advantage of the laws of physics as we know them”. Work on quantum physics, however, has given us a new and more powerful way of processing information. “If you can use the principles of quantum physics to process information then you can do a range of types of calculations that you cannot do with normal computers,” says Leek. Classical computers encode their information in bits – represented as a 0 or a 1 – that are transmitted as an electrical pulse. A text message, email or even a Netflix film streamed on your phone is a string of these bits. In quantum computers, however, the information is contained in a quantum bit, or qubit. These qubits – encased in a modestly sized chip – are particles such as electrons or photons that can be in several states at the same time, a property of quantum physics known as superposition. This means qubits can encode various combinations of 1s and 0s at the same time – and compute their way through vast numbers of different outcomes. “If you compared a piece of memory in a normal computer, it is in a unique state of ones and zeroes, ordered in a specific way. In a quantum computer that memory can be simultaneously in all possible states of ones and zeroes,” says Leek. To really harness this power requires an “entanglement” of pairs of qubits: if you double the number of qubits the computing power increases exponentially. Link these entangled qubit pairs together and you get a very powerful computer that can crunch through numbers at unprecedented speed, provided there is a quantum algorithm (the set of instructions followed by the computer) for the calculation you want to do. Jay Gambetta, a VP of quantum computing at IBM, which last week unveiled the world’s most powerful quantum processor, says: “The combined system has a computational power that is much more than the individual systems.” The computer firm’s US-made Eagle quantum processor – a type of computer chip – strings together 127 qubits compared with the 66 achieved recently by the University of Science and Technology of China (USTC) in Hefei. Gambetta stresses that the practical applications of quantum computers are not there yet, but theoretically they could have exciting uses like helping design new chemicals, drugs and alloys. Quantum computing could result in a much more efficient representation of chemical compounds, says Gambetta, predicting accurately what a complex molecule might do and paving the way for new drugs and materials. “It gives us a way to model nature better,” he adds. There are ways in which quantum computing could help combat global heating, too, says Gambetta, by more efficiently separating carbon dioxide into oxygen and carbon monoxide, reducing the amount of CO2 in the atmosphere. Alternatively, quantum computing could help understand how we can make fertiliser by using much less energy. Last year, IBM teamed up with German carmaker Daimler, the parent of Mercedes-Benz, to use quantum computing to model new lithium batteries. Renewable energy, pharmaceuticals, electric cars, fertiliser: if these are just some of the products that can be enhanced by quantum computing, then the UK understandably wants to be at the forefront of the market. Once quantum computing reaches the 1,000 qubit level it should be able to achieve what IBM calls “quantum advantage”, where a quantum computer consistently solves problems faster than a classical computer. IBM is hoping to reach 1,000 qubits via its Condor processor in 2023. The UK’s strong university system – and long history of innovation, epitomised by Alan Turing in computing and Paul Dirac in quantum mechanics – gives the country some hope of achieving Johnson’s goal. But Gambetta’s IBM colleague Bob Sutor says that for the UK and other countries ambitious in making advances in quantum computing, educations and skills are key – at university level and below, including schools. “The more people working on it, the faster we will get there.”', 'G18': 'New crowdsourced recruitment tool aims to get more women into tech Zofia Niemtus Heidi Harman has a succinct way of describing the tech industry’s issues with diversity. “It’s Steve, hiring Steve, hiring Steve,” she says. (It is. A recent analysis from the Center for Investigative Reporting found that, despite concerted efforts to address the issue, white men are still massively overrepresented in Silicon Valley jobs.) Now the former UX designer and researcher – who launched her first startup, RunAlong, in 2009 – is working to dismantle that hall of mirrors with her latest venture, a recruitment tool called Included Works. And she’s doing it with one of tech world’s most beloved principles: crowdsourcing. “It allows anyone to recommend the right person for a job, with full transparency,” she explains. “Let’s say Fatima is a great developer and you recommend her to a company, vouching for her work ethic and skill. The company hires her, they pay us and we share that money with you.” So Fatima gets a new job and the company gets an employee from outside of what Harman refers to as “the pool of usual suspects”. Companies can advertise their roles on the site for free, and only pay a fee of 20% of the candidate’s first-year salary once they are hired. Each recommendation is scored by the hiring manager on an Amazon-style rating system, allowing other companies to check out the trustworthiness of recommenders. The site has been beta-testing since April and the results so far have been overwhelmingly positive, Harman says. “The hiring managers are really happy that we have experts recommending experts, because a developer can actually vouch for another developer with much greater accuracy than most recruiters,” she explains. Heidi Harman, founder of Geek Girl Meetup, a conference and networking group for women in technology. Photograph: Daniel Kennett And the developers are similarly pleased to be cutting out the traditional middlemen. Throughout her tech career, Harman continues, she has often been chased down by recruiters who don’t understand the skills required for a role, ending up “wasting their time and my time”. Instead, she says, Included Works recognises the effort that goes into making a good recommendation, and rewards successful ones with a finder’s fee of €2,000 (£1,750). Harman was first struck with the idea after being repeatedly called on to give recommendations for female developers, eventually to an unsustainable level. So, she says, she decided to create a tool that would “open it up to my network”. That network, however, happens to be a huge one, because Harman is also the co-founder of Geek Girl Meetup. The now-global movement began in Sweden in 2008, when Harman and a friend found themselves frustrated at the lack female participation in tech conferences. The pair decided to create an “unconference”, where women of all ages could network, share knowledge and ideas, and hear talks from female tech role models. The idea snowballed: there are now monthly Geek Girl Meetups in 27 cities and 17 countries around the world. Each one looks different, tailored to the local population. For some that means late-night networking, while for others, such as London, it’s breakfast meetings. Another fundamental element, she explains, is drawing on the skills of the local community. “We say: ‘Tell me what you guys are amazing at and we’ll grow the intellectual knowledge here,’” Harman says. “We don’t have to fly in somebody from somewhere else. We take the local talent that’s there and elevate them and network them.” The huge growth has been entirely organic, she continues, thanks to a model that is “completely designed for self-organisation”. New chapters are added when people approach and say that they would like to set one up. That means there are now Geek Girl Meetups in places that Harman hasn’t even visited. “They have them in Singapore and Honduras now and I’ve never been. I want to go!” she laughs. But she’s also busy heading up her next project to improve diversity in recruitment: Progress Data. It’s a tool for companies to analyse the profile of their staff and see where they may be “overly-Steved”. It’s not just about gender, Harman explains, but age and cultural background too. “It’s like Google Analytics for diversity,” she explains. “They put in all their existing data, we send out surveys for areas they are missing and every time a new person is hired, we add that to the data set. It means the hiring manager can get a one-click report on what the diversity is actually like at the company.” Companies are then offered research-based suggestions on improving in these areas, as compiled by the team’s expert researcher. The product doesn’t launch until January, but Harman is enthusiastic about its potential. Her hope for it, and for all of her work, she explains, is that she will be able to use her position and privilege to “push things forward”. And she’s cheered by the enormous success of the Geek Girl Meetups as evidence of the enthusiasm that’s out there for a fairer tech landscape. “It’s such a shame that it’s needed,” she says, “but it’s a wonderful thing.”', 'G19': \"International Space Station attacked by ‘virus epidemics’ Samuel Gibbs Malware made its way aboard the International Space Station (ISS) causing “virus epidemics” in space, according to security expert Eugene Kaspersky. Kaspersky, head of security firm Kaspersky labs, revealed at the Canberra Press Club 2013 in Australia\\xa0that before the ISS switched from Windows XP to Linux computers, Russian cosmonauts managed to carry infected USB storage devices aboard the station spreading computer viruses to the connected computers. The damage done by the malware to the computer systems of the ISS is unknown. However, Kaspersky said virus epidemics took hold of the space-based computers, including dozens of laptops. The Windows XP-based laptops on the ISS were infected with a virus called W32.Gammima.AG\\xa0in 2008, after a cosmonaut brought a compromised laptop aboard which spread the malware to the networked computers. It's not a frequent occurrence, but this isn't the first time,\\xa0a Nasa spokesperson\\xa0said at the time. In May, the United Space Alliance, which oversees the running of if the ISS in orbit, migrated all the computer systems related to the\\xa0ISS over to Linux\\xa0for security, stability and reliability reasons.\", 'G20': \"Open data is a force for good, but not without risks David Hand The government's open data initiative, outlined in its white paper on 27 June, is a move towards making government data publicly available, to encourage its reuse. A wide range of data, of all types, is being released, from the financial transactions of government departments through to information on how bumblebees respond to different flowers, and is available at data.gov.uk. There are many potential gains from this initiative. Open data enables accountability: it is difficult to conceal something if the facts are there for all to see. Open data empowers communities: the truth about crime rates, educational achievement, social services and so on is laid bare. Open data even drives economic growth: more small companies are springing up that extract useful information from data. Open data may even lead to more accurate conclusions and better decisions, as a wider variety of interested parties have the opportunity to examine the facts. Open data also alleviates the force of Goodhart's law, which says that if attention is focused on a particular outcome then that outcome becomes useless as a measure of performance, as people game to optimise it (think of hospitals manipulating waiting times). With open data, people can explore the impact of policies on a wider range of indicators. However, the open data initiative also raises some concerns. The potential threat to privacy is probably the foremost risk. Reducing this is tough, as has been recognised by the government. In 2011, Cabinet Office minister Francis Maude said: It is my intention that no personal data will be shared with any third party as part of this initiative. It is questionable whether this can be achieved, partly due to the jigsaw effect – the use of multiple sources of data, which can be combined to yield information about individuals. The fact is that no large data set that refers to human beings is perfect. This may not matter for data en masse, but it certainly matters, say, to an individual whose credit record is damaged because an address error meant bills were misdirected and went unpaid. Crime maps illustrate the sort of problems that can arise. The creation of online crime maps have been a clear public benefit, showing the police where resources should be concentrated and helping them to improve their tactics, while the public can identify risky areas to avoid and demand more police action if necessary. But there have been problems. In December 2011, for instance, Surrey Street in Portsmouth was reported as having 136 crimes, when in fact it had just two. But there are more insidious problems. A survey by Direct Line Insurance in the same year found that 11% of respondents claim to have seen but not reported an incident because they feared it would make it more difficult to rent or sell their house. In general, the open data initiative ignores such feedback effects – that the very act of publishing the data will influence the quality of future data. (Incidentally, internet surveys are notoriously unreliable because of the potential for the respondents to self-select. Again, data quality issues are raised.) This last example leads to another consideration. Data is all very well, but the ability to extract meaningful information from it requires considerable skill. Without it, there is a real danger incorrect conclusions may be drawn. No technology is without concomitant risks, but provided we tread carefully, with an awareness of the problems, the open data initiative holds immense promise for a better society.\", 'T01': \"Curve Light: A highly performing indoor positioning system Ingrid Fadelli In recent years, engineers have been trying to develop more effective sensors and tools to monitor indoor environments. Serving as the foundation of these tools, indoor positioning systems automatically determine the position of objects with high accuracy and low latency, enabling emerging Internet-of-Things (IoT) applications, such as robots, autonomous driving, VR/AR, etc. A team of researchers recently created CurveLight, an accurate and efficient light positioning system. Their technology, described in a paper presented at ACM's SenSys 2021 Conference on Embedded Networked Sensor Systems, could be used to enhance the performance of autonomous vehicles, robots and other advanced technologies. In CurveLight, the signal transmitter includes an infrared LED, covered by a hemispherical and rotatable shade, Zhimeng Yin, one of the researchers who developed the system at City University of Hong Kong, told TechXplore. The receiver detects the light signals with a photosensitive diode. When the shade is rotating, the transmitter generates a unique sequence of light signals for each point in the covered space. Recently developed positioning systems can detect the position of objects using LED lamps as landmarks (i.e., by analyzing their unique light-related characteristics). To make them easier to deploy in real-world settings, some developers did not limit their use to LED lamps, but instead designed the systems so that they collect lamp-specific information and use it as a fingerprint. While some of these systems achieved promising performances, they often require extensive sensing and computational resources. In addition, to work well, these systems need to continuously capture and analyze images, which could be a privacy concern for users. Existing solutions measure the received light intensity, compute the distances from the receiver to LED transmitters, based on the Lambertian model, and further adopt multilateral positioning for localization, Yin said. Typically, this type of positioning methods suffers from model inaccuracy, environmental noises and sensitivity to receiver's orientation. To overcome the limitations of existing positioning systems, some researchers suggested substituting LED lamps with other light sources, such as projectors. Compared to LED lamps, however, projectors could be harder to deploy in the real-world. For example, SpotLight and SmartLight exploit projectors to project dynamically changing images into the space. By detecting the changing light patterns, the receiver can compute its location, Yin said. SmartLight reports an error of 0.1 m, but the system is not easy to deploy due to the requirement of projectors. In addition, the localization latency is fairly high, making it unsuitable for real-time applications. Instead of using light, some positioning systems use other wireless signals, such as wi-fi, UWB, sound waves, geomagnetic signals and radiofrequency identification (RFID). Wi-fi signals are easier to access in real-time settings, but when used to predict the position of objects they often result in poorer accuracy and stability. On the other hand, positioning systems that use RFID technology are often very accurate, but they can be more expensive to implement. The new positioning system created by Yin and his colleagues uses light to detect objects and determine their position. Its components include a transmitter based on an infrared (IR) chip, which is fixed on the ceiling, with its base placed horizontally. The LED bead is very small in size (around 2 mm × 2 mm), so can be approximately treated as a point light source, Yin explained. To distinguish the emitted light signals from ambient light, a microcontroller (MCU) in the transmitter lets the LED flashes at a certain rate. The transmitter used by the researchers also includes a hemispherical shade that covers the LED lamp, as well as a motor that allows the shade to spin around the lamp at a fixed rate. In the initial prototype of the team's system, the shade spins at a rate of 1200 revolutions per minute (RPM). The shade consists of two types of regions: transparent regions and translucent regions, Yin said. When the LED is on, the transparent regions allow the light to pass through and result in bright regions on the ground, while the translucent regions absorb part of the light energy and create gray regions on the ground. When the shade rotates, the shade's projected image also rotates with the same rotational speed on the ground. As a result, the receiver detects a sequence of light signals with two levels of intensity for uniquely determining its location. The light positioning system created by Yin and his colleagues has numerous advantages over other systems created in the past. Firstly, it is highly accurate, with an average accuracy of 2–3cm across in typical indoor environments. Secondly, it has a low-latency, achieving an update rate of 36 Hz using a single transmitter. The system is also practical and very easy to implement. As part of their study, the researchers evaluated it in a series of tests and implemented it in several real-world environments, demonstrating its value for enhancing both autonomous driving and robotic navigation. Other than typical lab settings, we have also deployed our system in more than ten real-world environments, including autonomous driving, industrial robots in smart factories, warehouses, mines, etc., Yin said. In the paper, we report two use cases, in which CurveLight was accepted as a key part of the customer's full navigation solution. In the future, CurveLight could be used by a growing number of roboticists and developers to enhance the performance of robots, self-driving cars and other autonomous systems. Meanwhile, Yin and his colleagues will continue working on their system and evaluating its applicability in other settings. We now plan to develop accurate and scalable 3D positioning systems to serve numerous IoT applications, Yin added.\", 'T02': \"Studying RISC-V architecture to create customized systems for space computing Leah Russell When choosing a processor for space computing, there are many factors that come into play: because of the rigors of a harsh environment, developers must find the optimal balance between size, weight, power and cost. An important variable in this design is the processor architecture, which can have a significant impact on balancing performance and power consumption. Students at the University of Pittsburgh's NSF Center for Space, High-performance, and Resilient Computing (SHREC) examined the RISC-V architecture for space computing and presented their results at the 2021 IEEE Space Computing Conference. They were awarded the Best Paper Award for Research in Space Computing for their work. RISC-V is exciting because it's open-source and benefits from collaborative development, said Michael Cannizzaro, lead author on the paper and an electrical and computer engineering Ph.D. student at Pitt's Swanson School of Engineering. There is a large community, ranging from individuals to large companies, that are contributing to this development. RISC—or Reduced Instruction Set Computer—is a more efficient approach to computing that uses a simple, optimized set of instructions compared to other architectures. RISC-V, in particular, is lauded for its modularity—a unique characteristic that sets it apart from other designs and allows users to add specialized functionality to individual systems. With RISC-V, the base set of instructions essentially acts as a foundation on which a processor designer can easily develop a system that includes all the features they want, without any unnecessary extras, Cannizzaro explained. Typical architectures are proprietary and require licensing, but RISC-V's open-source structure decreases development costs and allows a wider audience of innovators to explore its applications. According to the SHREC team, RISC-V may be particularly appealing for space missions. The architecture's modularity means that different implementations of RISC-V can be used in a variety of space systems—from navigation and image processing to communications and machine learning, said Evan Gretok, an electrical and computer engineering Ph.D. student at Pitt, who also contributed to the study. However, no one can benefit from these features if the architecture itself can't perform computations in time and within the strict power consumption constraints of space—that's where our work comes in. This research is the starting point of a more in-depth investigation into a promising new architecture that may potentially lead to a space-ready RISC-V computer. We are currently working on extending this work by incorporating additional architectures, processing platforms, and benchmark tests, Cannizzaro added. These new additions will help us make the best conclusions about the RISC-V architecture and its readiness for space. RISC-V is moving forward at a very fast pace, and we're excited to see all the new systems that are developed with this technology in the near future.\", 'T03': \"RoboTurk: A crowdsourcing platform for imitation learning in robotics Ingrid Fadelli Imitation learning is a branch of machine learning that trains machines to mimic human behavior while completing particular tasks. These techniques show great promise in the field of robotics, as they tackle some of the shortcomings of reinforcement learning, such as exploration and reward specification. Despite encouraging results, imitation learning studies have so far been limited to modest-sized datasets due to difficulties in collecting large quantities of task demonstrations using existing methods. To address these limitations, a team of researchers supervised by Dr. Silvio Savarese and Dr. Fei-Fei Li at Stanford University have developed RoboTurk, a crowdsourcing platform for high-quality 6-DoF trajectory-based teleoperation using widely available smartphone devices. We wanted to create something like ImageNet for Robotics, Ajay Mandlekar, one of the researchers who carried out the study, told TechXplore. We believe that data is a key limitation in the field of robot learning. While there are plenty of methods that learn from data, such as data-driven control and reinforcement learning, most methods collect their own data. As a result, the data is often of a low quality, for instance resulting in the robot moving its arm randomly. This type of exploration can be difficult and unsafe, but we believe that humans can help. ImageNet is a renowned image database created by Dr. Li, commonly used in computer vision and object recognition research. The crowdsourcing platform developed by Stanford Vision and Learning Lab was designed to serve as a similar resource for robotics and imitation learning studies. Unlike ImageNet, such a data collection system needed to be dynamic, allowing us to collect data repeatedly, often on-demand, and perhaps even using collaborative learning, Yuke Zhu, who was also involved in the development of Roboturk, told TechXplore. This is because the data that is collected depends on what types of actions the robot takes in the environment. System diagram of ROBOTURK. A new user connects to a website to join the system, and a coordination server launches a dedicated teleoperation server for the user, as shown in (a). The coordination server then establishes direct communication channels between the user’s web browser and iPhone and the teleoperation server to start the teleoperation session. The user controls the simulated robot by moving their phone, and receives a video stream as feedback in their web browser, as shown in (b). After every successful demonstration, the teleoperation server pushes the collected data to a cloud storage system. Credit: Mandlekar et al. The researchers' ultimate goal is to train robots on advanced manipulation skills, allowing them to complete tasks within industrial settings such as packaging or assembly. They found that while imitation learning showed great potential in this context, existing datasets were very limited due to difficulties in collecting large quantities of task demonstrations. In other domains such as computer vision and natural language processing, large-scale supervision for datasets is often collected with the assistance of crowdsourcing, Mandlekar said. This enables a scalable mechanism for diverse human supervision on an extensive set of problem instances. However, collecting large amounts of data has been a challenge for robotics tasks, as they demand real-time interaction and feedback from annotators, placing difficult constraints on remote teleoperation platforms. The group at Stanford Vision and Learning Lab hence developed RoboTurk, a crowdsourcing platform that allows researchers to scale up the skills and tasks that robots can perform autonomously, through the use of scalable human supervision. Via RoboTurk, remote workers can log onto a website and collect task demonstrations, using their smartphone as a motion controller. RoboTurk is supported by a cloud-based simulation backend that streams video to a client's web browser using low-latency communication protocols, Mandlekar explained. This ensures homogenous quality of service regardless of a client's computer resources, resulting in a platform that is intuitive to use and has a low barrier to entry, which are the core requirements of a crowdsourced task. RoboTurk supports multiple robots, tasks, and simulators, and can easily be extended to support others. The researchers evaluated their platform on three manipulation tasks of varying durations, ranging from 15 to 120 seconds. They found that RoboTurk shared statistical similarities with special purpose hardware, such as virtual reality controllers. They also observed that poor network conditions did not substantially affect users' ability to perform tasks successfully on the platform. Using RoboTurk, they collected 137.5 hours of manipulation data from remote workers, with over 2200 successful task demonstrations in 22 hours of total system usage. I think that the most meaningful part of the platform is how it will enable humans and robots to interact, Animesh Garg, postdoctoral student leading the project, told TechXplore. Robots are the smart tools of the future. We should not think of them as a replacement for humans but rather as a way to extend our capabilities. This empowers humans to be more productive and focus on higher-level intelligence problems, in the same way in which the advent of computers made it easier for people to use math as a tool to solve problems of interest. RoboTurk effectively enables policy learning on multi-step manipulation tasks with sparse rewards. In addition, Mandlekar and his colleagues found that using larger quantities of demonstrations during policy learning had notable benefits, leading to better performance and greater learning consistency. In the future, RoboTurk could become a key resource in the field of robotics, aiding the development of more advanced and better performing robots. The researchers are now applying RoboTurk to real robots, while also developing algorithms that can use the data they collected to teach robots low-level skills. Robots are a very exciting technology that will enable people to be more productive and independent in all spheres of human activity, for instance providing a helping hand in the kitchen, caretakers for the senior population, and better care for patients, Garg said. One of the things that excites us is the democratization of manufacturing. This technology could enable people to make and sell custom products without the need of special purpose equipment, just as YouTube has democratized content creation and distribution, allowing anyone to create and share videos.\", 'T04': 'Artificial intelligence—parking a car with only 12 neurons Florian Aigner Computer scientists at TU Wien (Vienna) are improving artificial intelligence by drawing inspiration from biology. The new approaches achieve amazing results with surprisingly little effort. A naturally grown brain works quite differently than an ordinary computer program. It does not use code consisting of clear logical instructions, it is a network of cells that communicate with each other. Simulating such networks on a computer can help to solve problems which are difficult to break down into logical operations. At TU Wien (Vienna), in collaboration with researchers at Massachusetts Institute of Technology (MIT), a new approach for programming such neural networks has now been developed, which models the time evolution of the nerve signals in a completely different way. It was inspired by a particularly simple and well-researched creature, the roundworm C. elegans. Neural circuits from its nervous system were simulated on the computer, and then the model was adapted with machine learning algorithms. This way, it was possible to solve remarkable tasks with an extremely low number of simulated nerve cells – for example parking a car. Even though the worm-inspired network only consists of 12 neurons, it can be trained to steer a rover robot to a given spot. Ramin Hasani from the Institute of Computer Engineering at TU Wien has now presented his work at the TEDx conference in Vienna on October 20. It can be shown that these novel neural networks are extremely versatile. Another advantage is that their internal dynamics can be understood - in contrast to standard artificial neural networks, which are often regarded as a useful but inscrutable black box. Signals in branched networks Neural networks have to be trained says Ramin Hasani. You provide a specific input and adjust the connections between the neurons so that the desired output is delivered. The input, for example, can be a photograph, and the output can be the name of the person in the picture. Time usually does no play an important role in this process, says Radu Grosu from the Institute of Computer Engineering of TU Wien. For most neural networks, all the input is delivered at once, immediately resulting in a certain output. But in nature things are very different. Speech recognition, for example, is always time-dependent, as are simultaneous translations or sequences of movements reacting to a changing environment. Such tasks can be handled better using what we call RNN, or recurrent neural networks, says Ramin Hasani. This is an architecture that can capture sequences, because it makes neurons remember what happened previously. Hasani and his colleagues propose a novel RNN-architecture based on a biophysical neuron and synapse model that allows time-varying dynamics. In a standard RNN-model, there is a constant link between neuron one and neuron two, defining how strongly the activity of neuron one influences the activity of neuron two, says Ramin Hasani. In our novel RNN architecture, this link is a nonlinear function of time. The worm brain that can park a car Allowing cell activities and links between cells to vary over time opens up completely new possibilities. Ramin Hasani, Mathias Lechner and their coworkers showed theoretically that their architecture can, in principle, approximate arbitrary dynamics. To demonstrate the versatility of the new approach, they developed and trained a small neural network: We re-purposed a neural circuit from the nervous system of the nematode C. elegans. It is responsible for generating a simple reflexive behavior - the touch-withdrawal, says Mathias Lechner, who is now working at the Institute of Science and Technology (IST) Austria. This neural network was simulated and trained to control real-life applications. The success is remarkable: the small, simple network with only 12 neurons can (after appropriate training) solve challenging tasks. For instance, it was trained to manoeuvre a vehicle into a parking space along a pre-defined path. The output of the neural network, which in nature would control the movement of nematode worms, is used in our case to steer and accelerate a vehicle, says Hasani. We theoretically and experimentally demonstrated that our novel neural networks can solve complex tasks in real-life and in simulated physical environments. The new approach has another important advantage: it provides a better insight into the inner workings of the neural network. Previous neural networks, which often consisted of many thousands of nodes, have been so complex that only the final results could be analysed. Obtaining a deeper understanding of what is going on inside was hardly possible. The smaller but extremely powerful network of the Vienna team is easier to analyse, and so scientists can at least partially understand, which nerve cells cause which effects. This is a great advantage which encourages us to further research their properties, says Hasani. Of course, this does not mean that cars will be parked by artificial worms in the future, but it shows that artificial intelligence with a more brain-like architecture can be far more powerful than previously thought.', 'T05': \"New 'emotional' robots aim to read human feelings AFP The robot called Forpheus does more than play a mean game of table tennis. It can read body language to gauge its opponent's ability, and offer advice and encouragement. It will try to understand your mood and your playing ability and predict a bit about your next shot, said Keith Kersten of Japan-based Omron Automation, which developed Forpheus to showcase its technology. We don't sell ping pong robots but we are using Forpheus to show how technology works with people, said Kersten. Forpheus is among several devices shown at this week's Consumer Electronics Show which highlight how robots can become more humanlike by acquiring emotional intelligence and empathy. Although this specialization is still emerging, the notion of robotic empathy appeared to be a strong theme at the huge gathering of technology professionals in Las Vegas. Honda, the Japanese auto giant, launched a new robotics program called Empower, Experience, Empathy including its new 3E-A18 robot which shows compassion to humans with a variety of facial expressions, according to a statement. Although empathy and emotional intelligence do not necessarily require a humanoid form, some robot makers have been working on form as well as function. We're been working very hard to have an emotional robot, said Jean-Michel Mourier of French-based Blue Frog Robotics, which makes the companion and social robot called Buddy, set to be released later this year. He has a complex brain, Mourier said at a CES event. It will ask for a caress or it will get mad if you poke him in the eye. Other robots such as Qihan Technology's Sanbot and SoftBank Robotics' Pepper, are being humanized by teaching them to read and react to people's emotional states. Pepper is capable of interpreting a smile, a frown, your tone of voice, as well as the lexical field you use and non-verbal language such as the angle of your head, according to SoftBank. Robot in human shoes Developing emotional intelligence in robots is a difficult task, melding the use of computer vision to interpret objects and people and creating software that can respond accordingly. Empathy is the goal: the robot is putting itself in the shoes of the human, and that's about as hard as it gets, said Patrick Moorhead, a technology analyst with Moor Insights & Strategy. It's not just about technology, it's about psychology and trust. Moorhead said this technology is still in the early stages but holds promise in some areas, noting that there is strong interest in Japan amid a lack of caretakers for the elderly population. In some ways it can be a bit creepy if you're crying and the robot is trying to console you, he said. If you have no friends, the next best thing is a friend robot, and introverts might feel more comfortable talking to a robot.'Emotion chip' One CES exhibitor offered a promise of going further than the current devices by developing an emotion chip which can allow robots to process emotions in a manner similar to humans. There has been a lot of research on detecting human emotions. We do the opposite. We synthesize emotions for the machine, said Patrick Levy-Rosenthal, founder of New York-based Emoshape, which is producing its chip for partners in gaming, virtual and augmented reality and other sectors. It could be used to power a humanoid robot, or other devices. For example, an e-reader could better understand a text to infuse more emotion in storytelling. As for Forpheus, Kersten said the robot's ability to help people improve their table tennis skills could have numerous applications for sports, businesses and more. You could sense how people are feeling, if they are attentive or in a good state to drive, he said. Another key application could be in health care, he said: In an elderly patient facility, you can determine if someone is in distress and needs help.\", 'T06': \"AI system-on-chip runs on solar power Swiss Center for Electronics and Microtechnology - CSEM AI is used in an array of useful applications, such as predicting a machine's lifetime through its vibrations, monitoring the cardiac activity of patients and incorporating facial recognition capabilities into video surveillance systems. The downside is that AI-based technology generally requires a lot of power and, in most cases, must be permanently connected to the cloud, raising issues related to data protection, IT security and energy use. CSEM engineers may have found a way to get around those issues, thanks to a new system-on-chip they have developed. It runs on a tiny battery or a small solar cell and executes AI operations at the edge—i.e., locally on the chip rather than in the cloud. What's more, their system is fully modular and can be tailored to any application where real-time signal and image processing is required, especially when sensitive data are involved. The engineers will present their device at the prestigious 2021 VLSI Circuits Symposium in Kyoto this June. The CSEM system-on-chip works through an entirely new signal processing architecture that minimizes the amount of power needed. It consists of an ASIC chip with a RISC-V processor (also developed at CSEM) and two tightly coupled machine-learning accelerators: one for face detection, for example, and one for classification. The first is a binary decision tree (BDT) engine that can perform simple tasks but cannot carry out recognition operations. When our system is used in facial recognition applications, for example, the first accelerator will answer preliminary questions like: Are there people in the images? And if so, are their faces visible? says Stéphane Emery, head of system-on-chip research at CSEM. If our system is used in voice recognition, the first accelerator will determine whether noise is present and if that noise corresponds to human voices. But it can't make out specific voices or words—that's where the second accelerator comes in. The second accelerator is a convolutional neural network (CNN) engine that can perform these more complicated tasks—recognizing individual faces and detecting specific words—but it also consumes more energy. This two-tiered data processing approach drastically reduces the system's power requirement, since most of the time only the first accelerator is running. The integrated circuit can carry out complicated artificial-intelligence operations like face, voice and gesture recognition and cardiac monitoring. Credit: CSEM As part of their research, the engineers enhanced the performance of the accelerators themselves, making them adaptable to any application where time-based signal and image processing is needed. Our system works in basically the same way regardless of the application, says Emery. We just have to reconfigure the various layers of our CNN engine. The CSEM innovation opens the door to an entirely new generation of devices with processors that can run independently for over a year. It also sharply reduces the installation and maintenance costs for such devices, and enables them to be used in places where it would be hard to change the battery.\", 'T07': \"Transfer learning offers new insight into machine-learning error estimation Rachel Rose Omar Maddouri, a doctoral student in the Department of Electrical and Computer Engineering at Texas A&M University, is working with Dr. Byung-Jun Yoon, professor, and Dr. Edward Dougherty, Robert M. Kennedy Chair Professor, to evaluate machine-learning models using transfer learning principles. Dr. Francis Frank Alexander with Brookhaven National Labs and Dr. Xiaoning Qian from the Department of Electrical and Computer Engineering at Texas A&M University are also involved with the project. In data-driven machine learning, models are built to make predictions and estimations for what's to come in any given data set. One important field within machine learning is classification, which allows a data set to be assessed by an algorithm and then classified or broken down into classes or categories. When the data sets provided are very small, it can be very challenging to not only build a classification model based on this data but also to evaluate the performance of this model, ensuring its accuracy. This is where transfer learning comes into play. In transfer learning, we try to transfer knowledge or bring data from another domain to see whether we can enhance the task that we are doing in the domain of interest, or target domain, Maddouri explained. The target domain is where the models are built, and their performance is evaluated. The source domain is a separate domain that is still relevant to the target domain from which knowledge is transferred to make the analysis within the target domain easier. Maddouri's project utilizes a joint prior density to model the relatedness between the source and target domains and offers a Bayesian approach to apply the transfer learning principles to provide an overall error estimator of the models. An error estimator will deliver an estimate of how accurate these machine-learning models are at classifying the data sets at hand. What this means is that before any data is observed, the team creates a model using their initial inferences about the model parameters in the target and source domains and then updates this model with enhanced accuracy as more evidence or information about the data sets becomes available. This technique of transfer learning has been used to build models in previous works; however, no one has ever before used this transfer learning technique to propose novel error estimators to evaluate the performance of these models. For an efficient utilization, the devised estimator has been implemented using advanced statistical methods that enabled a fast screening of source data sets which enhances the computational complexity of the transfer learning process by 10 to 20 times. This technique can help serve as a benchmark for future research within academia to build upon. In addition, it can help with identifying or classifying different medical issues that would otherwise be very difficult. For example, Maddouri utilized this technique to classify patients with schizophrenia using transcriptomic data from brain tissue samples originally acquired by invasive brain biopsies. Because of the nature and the location of the brain region that can be analyzed for this disorder, the data collected is very limited. However, using a stringent feature selection procedure that comprises differential gene expression analysis and statistical testing for assumptions validity, the research team identified transcriptomic profiles of three genes from an additional brain region found to be highly relevant to the desired brain tissue as reported by independent research studies from other literature. This knowledge allowed them to utilize the transfer learning technique to leverage samples collected from the second brain region (source domain) to help with the analysis and significantly boost the accuracy of diagnosis within the original brain region (target domain). The data gathered from the source domain can be exploratory in the absence of information from the target domain, allowing the research team to enhance the quality of their conclusion.\", 'T08': \"What is 3G and why is it being shut down? An electrical engineer explains Mai Vu On Feb. 22, 2022, AT&T is scheduled to turn off its 3G cellular network. T-Mobile is scheduled to turn its off on July 1, 2022, and Verizon is slated to follow suit on Dec. 31, 2022. The vast majority of cellphones in service operate on 4G/LTE networks, and the world has begun the transition to 5G, but as many as 10 million phones in the U.S. still rely on 3G service. In addition, the cellular network functions of some older devices like Kindles, iPads and Chromebooks are tied to 3G networks. Similarly, some older internet-connected systems like home security, car navigation and entertainment systems, and solar panel modems are 3G-specific. Consumers will need to upgrade or replace these systems. So why are the telecommunications carriers turning off their 3G networks? As an electrical engineer who studies wireless communications, I can explain. The answer begins with the difference between 3G and later technologies such as 4G/LTE and 5G. Picture a family trip. Your spouse is on the phone arranging activities to do at the destination, your teenage daughter is streaming music and chatting with her friends on her phone, and her younger sibling is playing an online game with his friends. All those separate conversations and data streams are communicated over the cellular network, seemingly simultaneously. You probably take this for granted, but have you ever wondered how the cellular system can handle all those activities at the same time, from the same car? Communicating all those messages The answer is a technological trick called multiple access. Imagine using a sheet of paper to write messages to 100 different friends, one private message for each person. The multiple access technology used in 3G networks is like writing every message to each of your friends using the whole sheet of paper, so all the messages are written on top of each other. But you have a special set of pens with different colors that allows you to write each message in a unique color, and each of your friends has a special pair of glasses that reveals only the color intended for that person. However, the number of colored pens is fixed, so if you want to send messages to more people than the number of colored pens you have, you will need to start mixing colors. Now when a friend applies their special lenses, they will see a little bit of the messages to other friends. They won't see enough to read the other messages, but the overlap might be enough to blur the message intended for them, making it harder to read. The multiple access technology used by 3G networks is called Code Division Multiple Access, or CDMA. It was invented by Qualcomm founder Irwin M. Jacobs with several other prominent electrical engineers. The technique is based on the concept of spread spectrum, an idea that can be traced back to the early 20th century. Jacobs' 1991 paper showed that CDMA can increase the cellular capacity manyfold over systems at the time. CDMA lets all cellular users send and receive their signals at all times and over all frequencies. So if 100 users wish to initiate a call or use a cell service at around the same time, their 100 signals will overlap with each other over the entire cellular spectrum for the whole time they communicate. The overlapping signals create interference. CDMA solves the interference problem by letting each user have a unique signature: a code sequence that can be used to recover each user's signal. The code corresponds to the color in our paper analogy. If there are too many users on the system at the same time, the codes can overlap. This leads to interference, which gets worse as the number of users increases. Slices of time and spectrum Instead of allowing users to share the entire cellular spectrum at all times, other multiple access techniques divide access by time or frequency. Division over time creates time slots. Each connection can last over multiple time slots spread out in time, but each time slot is so short—a matter of milliseconds—that the cellphone user doesn't perceive the interruptions from alternating time slots. The connection appears to be continuous. This time slicing technique is time-division multiple access (TDMA). The division can also be done in frequency. Each connection is given its own frequency band within the cellular spectrum, and the connection is continuous for its duration. This frequency slicing technique is frequency division multiple access (FDMA). In our paper analogy, FDMA and TDMA are like dividing the paper into 100 strips in either dimension and writing each private message on one strip. FDMA would be, for example, horizontal strips, and TDMA would be vertical strips. With individual strips, all messages are separated. 4G/LTE and 5G networks use Orthogonal Frequency Division Multiple Access (OFDMA), a highly efficient combination of FDMA and TDMA. In the paper analogy, OFDMA is like drawing strips along both dimensions, dividing the whole paper into many squares, and assigning each user a different set of squares according to their data need. End of the line for 3G Now you have a basic understanding of the difference between 3G and the later 4G/LTE and 5G. You might still reasonably ask why 3G needs to be shut down. It turns out that because of those differences in the access technology, the two networks are built using completely different equipment and algorithms. 3G handsets and base stations operate on a wideband system, meaning they use the whole cellular spectrum. 4G/LTE and 5G operate on narrowband or multi-carrier systems, which use slices of the spectrum. These two systems need completely different sets of hardware, from the antenna on the cell tower down to the components in your phone. So if your phone is a 3G phone, it cannot connect to a 4G/LTE or 5G tower. For a long while, the cellular service providers have been keeping their 3G networks going while building a completely separate network with new tower equipment and servicing new handsets using 4G/LTE and 5G. Imagine bearing the cost of operating two separate networks at the same time for the same purpose. Eventually, one has to go. And now, as the carriers are starting to deploy 5G systems in earnest, that time has come for 3G.\", 'T09': \"Using AI to fill in the missing gaps in ancient texts Ingrid Fadelli A team of AI researchers at DeepMind, working with colleagues from the University of Venice, the University of Oxford and Athens University of Economics and Business, has developed an artificial intelligence (AI) application to help historians fill in the gaps of text missing from stone, metal or pottery artifacts. In their paper published in the journal Nature, the group describes how they built the app, how it can be used and how well it worked when tested against known texts. Charlotte Roueché, with King's College London, has published a News and Views piece in the same journal issue outlining the history of using new technology to better understand historical artifacts and the work done by the team on this new effort. During certain points in history, humans began using written text for purposes such as keeping accounts. Such accounts can give modern scholars clues as to how people in ancient societies went about their days. But that is only if the artifacts can be deciphered. Many have been eroded by weather or have been broken and are missing pieces. Modern scholars use a variety of tools to determine the content of the original text. This is almost always a long and tedious process. In this new effort, the team at DeepMind set out to develop a tool to help in such endeavors. The result was Ithaca, a machine-learning application that learns from other ancient texts to predict missing text. The researchers trained the application using 60,000 Greek texts from the years 700BC to 500AD. Each had already been extensively studied and reconstructed when necessary. The team then ran the app on the same texts prior to reconstruction. They then trained the app on another 8,000 well-studied texts to test it against the work done by human experts. The researchers found the system to be 62% accurate, which was better than the performance of historians. But the best results came from collaborations between the AI system and the historians; together, they were able to achieve 72% accuracy. The researchers also added another feature—the ability to attribute a text to a time and place using clues found in the text and from other sources. They found the system to be 71% accurate in determining the origin of the writer and could place the date of writing to within 30 years, on average.\", 'T10': \"Retina-inspired sensors for more adaptive visual perception Ingrid Fadelli To monitor and navigate real-world environments, machines and robots should be able to gather images and measurements under different background lighting conditions. In recent years, engineers worldwide have thus been trying to develop increasingly advanced sensors, which could be integrated within robots, surveillance systems, or other technologies that can benefit from sensing their surroundings. Researchers at Hong Kong Polytechnic University, Peking University, Yonsei University and Fudan University have recently created a new sensor that can collect data in various illumination conditions, employing a mechanism that artificially replicates the functioning of the retina in the human eye. This bio-inspired sensor, presented in a paper published in Nature Electronics, was fabricated using phototransistors made of molybdenum disulfide. Our research team started the research on optoelectronic memory five years ago, Yang Chai, one of the researchers who developed the sensor, told TechXplore. This emerging device can output light-dependent and history-dependent signals, which enables image integration, weak signal accumulation, spectrum analysis and other complicated image processing functions, integrating the multifunction of sensing, data storage and data processing in a single device. In 2018, Chai and his colleagues published their first paper on optoelectronic memories, where they introduced a resistive switching memory device, which could perform both photo-sensing and logic operations. One year later, the team presented a new optoelectronic resistive random-access memory device with three different capabilities. Specifically, this new device could sense the environment, store information in its memory, and perform neuromorphic visual pre-processing operations. In 2020, we examined the concept of near-sensor and in-sensor computing paradigms and provided our perspective in this field, Chai said. Our new study builds on all of our previous efforts. The intensity of natural light can vary significantly, with an overall range of 280 dB. When perceiving external light signals, the human retina adjusts the photosensitivity of its photoreceptors (i.e., rods and cones) according to the intensity of the signals. This ultimately allows the human eye to gradually adapt to different levels of illumination, to see well in both dark and bright environments, a capability known as visual adaptation. For example, when you enter a darkened movie theater from a bright hall, you can hardly see anything initially, but after a while in the theater, it becomes easier to see, Chai explained. This phenomenon is called scotopic adaptation. In contrast, if you come out of a dark movie theater on a sunny day, you feel very dazzled at first and it takes a while to see the surrounding scenery. This process is the opposite of scotopic adaptation, which is called photopic adaptation. The key objective of the recent work by Chai and his colleagues was to build a sensing device inspired by structure and function of the human retina. To do this, they first started researching the retina and then tried to devise strategies that would allow them to artificially replicate its visual adaptation capabilities. State-of-the-art image sensors based on silicon complementary metal-oxide-semiconductor (CMOS) technologies typically have a limited dynamic range of 70 dB. This range is significantly narrower than the lighting range of natural scenes (280 dB). To enable vision under a large illumination intensity range, researchers have explored the use of controlled optical apertures, liquid lenses, adjustable exposure times and de-noising algorithms in post-processing, Chai said. However, these approaches typically require complex hardware and software resources. Optoelectronic devices capable of visual adaptation and with a wide perception range at sensory terminals could have very valuable applications. For instance, they could help to improve the performance of computer vision tools, reduce the complexity of the hardware required to build robots or other sensing systems, and improve the accuracy of image recognition systems. In the past, other research teams introduced optoelectronic devices that can adapt to different illumination conditions. Nonetheless, most previously proposed devices can only replicate the retina's photopic adaptation mechanism. The scotopic adaptation process, on the other hand, has so far proved to be harder to simulate. There is still a long way to go before we can fully replicate the retina's visual adaptation function, Chai explained. To move towards this goal, we designed a phototransistor-type vision sensor using an ultrathin semiconductor, which can control the degree of scotopic adaptation and photopic adaptation in the same device by applying different gate voltages. In this way, we emulate the functions of photoreceptors and horizontal cells in the retina and successfully realized the bio-inspired in-sensor visual adaptation devices with an expanded perception range of 199 dB. The bio-inspired vision sensor developed by Chai and his colleagues is based on a phototransistor made of an ultrathin semiconductor material (i.e., molybdenum disulfide). The phototransistors they used have several charge trap states, states that can trap or de-trap electrons within the channel under different gate voltages. Ultimately, these states allow the researchers to dynamically modulate the conductance of their device. This in turn allows them to artificially replicate both the scotopic and photopic adaptation mechanisms of the human retina, enlarging the perception range of their sensor in response to different lighting conditions. Our sensor has several advantages and characteristics, Chai said. Firstly, the visual adaptation function is realized in a single device, which substantially reduces its footprint. Second, it can achieve multiple functions with a single device, including light sensing, memory and processing. Finally, it can be used to perform scotopic and photopic adaptation to different background light intensities, simply by controlling its gate voltages. Chai and his colleagues evaluated their bio-inspired sensor in a series of tests and found that it can effectively emulate the function of the human retina, achieving remarkable results in both scotopic and photopic adaptation. In addition, in contrast with previously proposed solutions, it has a significantly higher perception range (i.e., 199 dB). Our sensor can enrich machine vision functions, reduce hardware complexity and realize high image recognition efficiency, Chai said. All of these benefits have great application prospects in the fields of automatic driving, face recognition and industrial manufacturing in complex lighting environments. In their next studies, the researchers plan to improve the performance of their sensors further, while also using them to fabricate large-scale systems composed of an array of sensors. Ideally, they would like to build this sensor array on a flexible or hemispherical substrate, to achieve a wider field-of-view. An area for improvement is our sensor's adaptation time, as it is still not short enough to enable machine vision applications, Chai added. Our target is to reduce the adaptation time to the microseconds level. The sensor array scale also needs further improvement. Our near-term goal of array scale is greater than 100×100. Finally, the heterogeneous integration of vision sensors and the post-processing units with Si-based control circuits is a very important step to move towards practical applications.\", 'T11': \"Data scientist builds a detailed network map of 'The Witcher' Ingrid Fadelli The Witcher, a fantasy novel series by Andrzej Sapkowski, has become increasingly popular, following the release of several videogames and a spin-off series by Netflix. The latest season of the show, uploaded on Netflix in December 2021, was watched by users worldwide for 2.2 billion minutes in its debut week alone. Milán Janosov, lead scientist at Datopolis with a Ph.D. in Network Science from Central European University recently tried to summarize the plot and character relationships in The Witcher using network science. In a paper published on Nightingale, arXiv and ResearchGate, he introduced the first visual network map outlining the hidden patterns, storylines and character relationships in the fantasy series. I started reading 'The Witcher' early last year, shortly after I got hooked to the Netflix show, and the storyline just sucked me in, Janosov told TechXplore. It was a somewhat similar experience to watching 'Game of Thrones' a few years ago, which had also inspired one of my research articles. When I was about to finish watching the new season of Witcher, I started to wonder how I could get more out of this. Although The Witcher videogames are also highly popular and iconic, Janosov was more drawn to the storylines and relationships outlined in the books and Netflix series. On a quest to understand the iconic series' world more in depth, he thus set out to create a social map of 'The Witcher.' The first step for his research was to collect data that he could then use to create the network map. He started by looking at the Netflix show's subtitles, but soon realized that he would need more than that and decided to analyze the whole text of the book series, too. To build a network, I also needed a complete list of the characters who appeared in the series, Janosov said. After collecting these initial pieces of information, my job was fairly simple. I wrote a computer program that screened through every single sentence of all the books and took a note every time it matched a character's name into a sentence. Using his computer program, Janosov derived the mentions for every character in sentences. This allowed him to determine how close or far two characters were, in terms of how often they were mentioned in similar parts of the texts (e.g., whether two characters were mentioned in the same sentence, two sentences apart, and so on). As it turns out, these proximities are pretty good indicators of whether two characters have actually met or were featured in the same plots, Janosov said. The social map of The Witcher. Characters are represented by nodes, their size corresponding to their degree centrality, and their colors encode the network communities to which they belong. The network links are proportional to the number of times two characters were mentioned within a five-sentence distance from each other in the novel. The most significant 50 individuals are labeled. Credit: Milán Janosov. After looking at the proximity between character mentions, Janosov defined the elements in his network. More specifically, he decided to represent every character with a node, linking nodes when characters were mentioned in the same context or part of the text. While context is relatively easy to interpret for humans, for a computer, it is not that simple, Janosov explained. So to capture the context of the characters mentioned, I assumed that two characters were mentioned in the same context as they were not mentioned further than five sentences from each other. While the number five is somewhat arbitrary, it was chosen for the sake of simplicty (and OCD-friendliness), because three, four or even six sentence-distances lead to very similar results too, also staying consistent for example with the typical paragraph lengths in written text. Janosov's paper is a valuable example of how network science can be used to reveal hidden patterns in large amounts of unstructured data, such as texts, novels, or movie scripts. After reading books or other texts that are thousands of pages long, humans can get a general idea of how a story is structured. However, they will generally be unable to memorize all the characters and remember all details of the plot. If they were to draw a map of the story, therefore, this map would most likely be biased. In contrast, network science tools can help to summarize a saga or book series in a quantitive and objective way. I was surprised and excited to see the different plots clustered into network communities, Janosov said. You know that kind of Eureka moment when suddenly everything starts to make sense—who met whom, who is together, where the main conflicts and smaller spin-off plots fold out, etc., almost like in a detective movie. At this point,the skeptic may ask—why would we care so much about a fantasy novel? While the example of 'The Witcher' is certainly fun, it indeed does not seem to bear that crucial practical importance at first. While the network map of The Witcher resulting from this study and other maps that Janosov created in the past are unique and interesting, his work is merely an example of how network science could be implemented in the real-world. In fact, similar data analysis tools could also be used to summarize other networks in the real world. In our daily lives, we are surrounded by social networks: our friends on social media, our colleagues at work, friends from school, family, sports and hobbies, and many more, Janosov said. All these social systems are intertwined by networks of which we almost always only have a partial and subjective understanding. To overcome this lack of knowledge and sparsity of information, network science comes really handy as it provides a set of tools and a framework of thinking that can help us better understand these social networks we participate in daily, just as it helped to clear the fog around 'The Witcher.' Network science tools like the ones employed by Janosov could also be applied (or are already in use) in a series of real-world settings. For instance, they could be used by HR specialists to design better work environments or enhance collaboration between co-workers, by scientific organizations to optimize the sharing of research funding across different research groups, or even to analyze and improve international trade and telecommunications. As the Academy Awards are coming next month, I am now thinking to revisit my previous research capturing the role of luck in the success of films and music, to see how much luck counts this year, Janosov added.\", 'T12': \"Deep-learning diagnoses: AI detects COVID-19 from smartwatch sensors Molly Sharlach, Princeton University Combining questions about a person's health with data from smartwatch sensors, a new app developed using research at Princeton University can predict within minutes whether someone is infected with COVID-19. This new breed of diagnostic tool stems from research led by Niraj Jha, a professor of electrical and computer engineering at Princeton University. His team is developing artificial intelligence (AI) technology for COVID-19 detection, as well as diagnosis and monitoring of chronic conditions including depression, bipolar disorder, schizophrenia, diabetes and sickle cell disease. NeuTigers, a company founded to commercialize Jha's work, applied to the U.S. FDA, under the agency's provision for software as a medical device, for clearance for its COVIDDeep product. Shayan Hassantabar, a Ph.D. student in Jha's group, is the lead author of a paper in IEEE Transactions on Consumer Electronics describing the development and testing of COVIDDeep. The software integrates smartwatch sensor readings of heart rate, skin temperature and galvanic skin response with blood pressure and oxygen saturation levels, as well as a questionnaire on COVID symptoms. Jha's research group at Princeton has long focused on adapting a type of AI called deep learning, which is typically energy-intensive, to function on low-power electronic devices such as phones and watches instead of centralized cloud computing centers. This approach, known as edge AI, has the added benefit of helping to preserve users' privacy and increase security. A key innovation is pared-down neural networks (the neu of NeuTigers) that mimic human brain development. It's a very generalizable framework, said Jha. Smart health care is just one application. We are also applying it to cybersecurity and other internet-of-things applications. Similar to preventive medical interventions, machine learning models could spot aberrant patterns and help fix software vulnerabilities before a cyberattack ever occurs, he said. In recent years, Jha's team has explored edge AI for health care applications such as noninvasive detection of diabetes and mental health disorders from smartwatch and smartphone sensor data. In fall 2017, former pharmaceutical executive Adel Laoui audited Jha's class on Machine Learning for Predictive Data Analytics and was intrigued by the technology. Laoui, who had experience developing and deploying new technologies for disease management, approached Jha after the course ended. After further discussions with Jha's Ph.D. students, they launched NeuTigers in June 2018. We saw a lot of intersections in our interests, said Jha. Smart health care with the help of edge AI was taking off, so it was an opportune moment for a startup in this area. Adel had a lot of connections with angel investors, and so it ramped up very quickly. Several patented technologies from Jha's lab have been licensed to NeuTigers, including methods for diagnosis of diabetes and mental health conditions, and for security vulnerability detection in internet-of-things systems. When the COVID-19 pandemic was declared in March 2020, Jha wondered whether his team's deep learning approaches could be used to diagnose the virus—especially in people with the potential to spread COVID-19 with no apparent symptoms, a major problem for controlling the disease. The hypothesis was that the disease leaves a unique signature on the physiological signals emanating from our body, said Jha. This hypothesis seems to have been true, at least for the few diseases we had looked at, so my idea was to see if we could diagnose COVID-19 this way. Jha and Laoui got in touch with Ignazio Marino, a professor of surgery at Thomas Jefferson University in Philadelphia and the executive director of the Jefferson Italy Center. In May 2020, at the tail end of northern Italy's initial cluster of COVID-19 in Europe, NeuTigers CTO Vishu Ghanakota traveled to Pavia, Italy, to deliver medical-grade smartwatches, software applications and training materials to Marino's colleagues at San Matteo Hospital. The clinical researchers collected data from 87 individuals, 30 of whom had tested negative for COVID by PCR; another 30 tested positive and were symptomatic, and 27 tested positive and were asymptomatic. The data included 60 minutes of smartwatch sensor readings on heart rate, skin temperature and galvanic skin response (a measure of sweat gland activity), divided into 15-second intervals. Separately, the clinicians measured participants' blood pressure and oxygen saturation levels, and answered a questionnaire indicating whether each participant had shortness of breath, cough, fever or any of eight other symptoms. The Princeton researchers, led by Ph.D. student Hassantabar, used a subset of this data to train neural network models to predict a patient's COVID-19 status, and another subset to test the resulting models. The team found that their models were 98.1% accurate at detecting COVID-19. One method Hassantabar used to boost the models' accuracy was the addition of synthetic data obtained based on the probability distribution of the real data—a broadly applicable technique that Jha's group first used for other applications. Another method he used was based on the grow-and-prune neural network synthesis paradigm developed in Jha's group. The researchers have since validated the method with a larger field trial in France, and health organizations in the United States and Algeria have piloted COVIDDeep among their staff. To enable more widespread adoption of COVIDDeep, NeuTigers is working to make it compatible with some types of Samsung, Fitbit and Apple smartwatches, which will also integrate blood pressure and pulse oximeter measurements. Manually entering clinical data into a smartphone app could be another useful screening approach in many settings, especially since smartphones are far more common worldwide than smartwatches, said Marino, who oversaw clinical data collection for the study. On the other hand, said Laoui, using a smartwatch alone may be preferable for many users, and the researchers are also working to adapt the neural network models to function with a smartwatch's more limited computing power. I think this could be far superior [to rapid antigen tests], because the accuracy of a rapid test that you do by yourself at home is limited, said Marino. Pushing the swab up in the nose is obviously a discomfort, and I don't know if people are going to do that as accurately as they need to. But if you have a device on your wrist that is not invasive and is totally independent from a physical human maneuver, I think that is much better. Marino also expressed hope that the technology could improve early diagnosis of widespread conditions such as diabetes. There are millions of people in the United States that have early diabetes that probably is very treatable, and they don't know, he said. Having new information that leads to early diagnosis could lead to a very successful treatment, he added. Laoui is interested in exploring the methods' use for diagnosis and monitoring of other diseases such as cardiovascular disorders and sepsis infections, both increasing problems for the world's aging population. We're going to have a library of disease models embedded in the watch, and from time to time we're going to run the sensors' information through these disease models, which will be personalized, said Laoui. If there is something wrong or something unusual, we are going to inform you in a meaningful way. I believe this new era of smart health care will be powered by edge AI applications and will redefine healthcare delivery and consumer well-being.\", 'T13': \"Injecting fairness into machine-learning models Adam Zewe, Massachusetts Institute of Technology If a machine-learning model is trained using an unbalanced dataset, such as one that contains far more images of people with lighter skin than people with darker skin, there is serious risk the model's predictions will be unfair when it is deployed in the real world. But this is only one part of the problem. MIT researchers have found that machine-learning models that are popular for image recognition tasks actually encode bias when trained on unbalanced data. This bias within the model is impossible to fix later on, even with state-of-the-art fairness-boosting techniques, and even when retraining the model with a balanced dataset. So, the researchers came up with a technique to introduce fairness directly into the model's internal representation itself. This enables the model to produce fair outputs even if it is trained on unfair data, which is especially important because there are very few well-balanced datasets for machine learning. The solution they developed not only leads to models that make more balanced predictions, but also improves their performance on downstream tasks like facial recognition and animal species classification. In machine learning, it is common to blame the data for bias in models. But we don't always have balanced data. So, we need to come up with methods that actually fix the problem with imbalanced data, says lead author Natalie Dullerud, a graduate student in the Healthy ML Group of the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. Dullerud's co-authors include Kimia Hamidieh, a graduate student in the Healthy ML Group; Karsten Roth, a former visiting researcher who is now a graduate student at the University of Tubingen; Nicolas Papernot, an assistant professor in the University of Toronto's Department of Electrical Engineering and Computer Science; and senior author Marzyeh Ghassemi, an assistant professor and head of the Healthy ML Group. The research will be presented at the International Conference on Learning Representations. Defining fairness The machine-learning technique the researchers studied is known as deep metric learning, which is a broad form of representation learning. In deep metric learning, a neural network learns the similarity between objects by mapping similar photos close together and dissimilar photos far apart. During training, this neural network maps images in an embedding space where a similarity metric between photos corresponds to the distance between them. For example, if a deep metric learning model is being used to classify bird species, it will map photos of golden finches together in one part of the embedding space and cardinals together in another part of the embedding space. Once trained, the model can effectively measure the similarity of new images it hasn't seen before. It would learn to cluster images of an unseen bird species close together, but farther from cardinals or golden finches within the embedding space. The similarity metrics the model learns are very robust, which is why deep metric learning is so often employed for facial recognition, Dullerud says. But she and her colleagues wondered how to determine if a similarity metric is biased. We know that data reflect the biases of processes in society. This means we have to shift our focus to designing methods that are better suited to reality, says Ghassemi. This image shows two distinct PARADE embeddings for bird color. On the left, both example images are mapped to clusters with birds of the same plumage. On the right in the class label embedding, due to de-correlation, the images are separated from the region of space with other birds of the same plumage, but are still well-clustered, indicating that PARADE can find other attributes to distinguish these species clusters. Credit: Massachusetts Institute of Technology The researchers defined two ways that a similarity metric can be unfair. Using the example of facial recognition, the metric will be unfair if it is more likely to embed individuals with darker-skinned faces closer to each other, even if they are not the same person, than it would if those images were people with lighter-skinned faces. Second, it will be unfair if the features it learns for measuring similarity are better for the majority group than for the minority group. The researchers ran a number of experiments on models with unfair similarity metrics and were unable to overcome the bias the model had learned in its embedding space. This is quite scary because it is a very common practice for companies to release these embedding models and then people finetune them for some downstream classification task. But no matter what you do downstream, you simply can't fix the fairness problems that were induced in the embedding space, Dullerud says. Even if a user retrains the model on a balanced dataset for the downstream task, which is the best-case scenario for fixing the fairness problem, there are still performance gaps of at least 20 percent, she says. The only way to solve this problem is to ensure the embedding space is fair to begin with. Learning separate metrics The researchers' solution, called Partial Attribute Decorrelation (PARADE), involves training the model to learn a separate similarity metric for a sensitive attribute, like skin tone, and then decorrelating the skin tone similarity metric from the targeted similarity metric. If the model is learning the similarity metrics of different human faces, it will learn to map similar faces close together and dissimilar faces far apart using features other than skin tone. Any number of sensitive attributes can be decorrelated from the targeted similarity metric in this way. And because the similarity metric for the sensitive attribute is learned in a separate embedding space, it is discarded after training so only the targeted similarity metric remains in the model. Their method is applicable to many situations because the user can control the amount of decorrelation between similarity metrics. For instance, if the model will be diagnosing breast cancer from mammogram images, a clinician likely wants some information about biological sex to remain in the final embedding space because it is much more likely that women will have breast cancer than men, Dullerud explains. They tested their method on two tasks, facial recognition and classifying bird species, and found that it reduced performance gaps caused by bias, both in the embedding space and in the downstream task, regardless of the dataset they used. Moving forward, Dullerud is interested in studying how to force a deep metric learning model to learn good features in the first place. How do you properly audit fairness? That is an open question right now. How can you tell that a model is going to be fair, or that it is only going to be fair in certain situations, and what are those situations? Those are questions I am really interested in moving forward, she says.\", 'T14': \"Aqua-Fi: Underwater WiFi developed using LEDs and lasers King Abdullah University of Science and Technology Aquatic internet that sends data through light beams could enable divers to instantly transmit footage from under the sea to the surface. The internet is an indispensable communication tool, connecting tens of billions of devices worldwide, and yet we struggle to connect to the web from under water. People from both academia and industry want to monitor and explore underwater environments in detail, explains the first author, Basem Shihada. Wireless internet under the sea would enable divers to talk without hand signals and send live data to the surface. Underwater communication is possible with radio, acoustic and visible light signals. However, radio can only carry data over short distances, while acoustic signals support long distances, but with a very limited data rate. Visible light can travel far and carry lots of data, but the narrow light beams require a clear line of sight between the transmitters and receivers. Now, Shihada's team has built an underwater wireless system, Aqua-Fi, that supports internet services, such as sending multimedia messages using either LEDs or lasers. LEDs provide a low-energy option for short-distance communication, while lasers can carry data further, but need more power. Scuba divers could send sea life shots in real time using an aquatic internet service. Credit: KAUST; Xavier Pita The Aqua-Fi prototype used green LEDs or a 520-nanometer laser to send data from a small, simple computer to a light detector connected to another computer. The first computer converts photos and videos into a series of 1s and 0s, which are translated into light beams turning on and off at very high speeds. The light detector senses this variation and turns it back into 1s and 0s, which the receiving computer converts back into the original footage. The researchers tested the system by simultaneously uploading and downloading multimedia between two computers set a few meters apart in static water. They recorded a maximum data transfer speed of 2.11 megabytes per second and an average delay of 1.00 millisecond for a round trip. This is the first time anyone has used the internet underwater completely wirelessly, says Shihada. In the real world, Aqua-Fi would use radio waves to send data from a diver's smartphone to a gateway device attached to their gear. Then, much like a booster that extends the WiFi range of a household internet router, this gateway sends the data via a light beam to a computer at the surface that is connected to the internet via satellite. Aqua-Fi will not be available until the researchers overcome several obstacles. We hope to improve the link quality and the transmission range with faster electronic components, explains Shihada. The light beam must also remain perfectly aligned with the receiver in moving waters, and the team is considering a spherical receiver that can capture light from all angles. We have created a relatively cheap and flexible way to connect underwater environments to the global internet, says Shihada. We hope that one day, Aqua-Fi will be as widely used underwater as WiFi is above water.\", 'T15': \"Cyberattack can steal data via cooling fan vibrations Peter Grad, Tech Xplore Israeli researchers uncovered a novel way that hackers could steal sensitive data from a highly secured computer: by tapping into the vibrations from a cooling system fan. Lead cyber-security researcher Mordechai Guri at Ben-Gurion University of the Negev said data encoded by hackers into fan vibrations could be transmitted to a smartphone placed in the vicinity of the targeted computer. We observe that computers vibrate at a frequency correlated to the rotation speed of their internal fans, Guri said. Malware can control computer vibrations by manipulating internal fan speeds, he explained. These inaudible vibrations affect the entire structure on which the computer is placed. The covertly transmitted vibrations can be picked up by a smartphone resting on the same surface as the computer. Since accelerometer sensors in smartphones are unsecured, they can be accessed by any app without requiring user permissions, which make this attack highly evasive, he said. Guri demonstrated the process, named AiR-ViBeR, with an air-gapped computer setup. Air-gapped computer systems are isolated from unsecured networks and the internet as a security measure. The research team said three measures would help secure a computer system against such an assault. One would be to run the CPU continuously at maximum power consumption mode, which would keep it from adjusting consumption. Another would be to set fan speeds for both CPU and GPU at a single, fixed rate. The third solution would be to restrict CPUs to a single clock speed. The Ben-Gurion University cybersecurity team specializes in what are termed side-channel attacks. Rather than exploiting software or coding vulnerabilities, side-channel attacks zero in on the manner in which a computer accesses hardware. This is the very essence of a side-channel attack, Guri said of AiR-ViBer. The malware in question doesn't exfiltrate data by cracking encryption standards or breaking through a network firewall; instead, it encodes data in vibrations and transmits it to the accelerometer of a smartphone. AiR-ViBer relied on vibration variances sensed by an accelerometer capable of detecting motion with a resolution of 0.0023956299 meters per square second. There are other means of capturing data through side channels. They include electromagnetic, magnetic, acoustic, optical and thermal. In 2015, for instance, Guri's team introduced BitWhisper, a thermal covert channel that allowed a nearby computer to establish two-way communication with another computer by detecting and measuring changes in temperature. A year earlier, his team demonstrated malware that extracts data from air-gapped computers to a nearby smartphone through FM signals emitted by the screen cable. He subsequently showed that he could exfiltrate data using cellular phone frequencies generated from buses connecting a computer's RAM and CPU.\", 'T16': \"How it takes just six seconds to hack a credit card Newcastle University Working out the card number, expiry date and security code of any Visa credit or debit card can take as little as six seconds and uses nothing more than guesswork, new research has shown. Research published in the academic journal IEEE Security & Privacy, shows how the so-called Distributed Guessing Attack is able to circumvent all the security features put in place to protect online payments from fraud. Exposing the flaws in the VISA payment system, the team from Newcastle University, UK, found neither the network nor the banks were able to detect attackers making multiple, invalid attempts to get payment card data. By automatically and systematically generating different variations of the cards security data and firing it at multiple websites, within seconds hackers are able to get a 'hit' and verify all the necessary security data. Investigators believe this guessing attack method is likely to have been used in the recent Tesco cyberattack which the Newcastle team describe as frighteningly easy if you have a laptop and an internet connection. And they say the risk is greatest at this time of year when so many of us are purchasing Christmas presents online. This sort of attack exploits two weaknesses that on their own are not too severe but when used together, present a serious risk to the whole payment system, explains Mohammed Ali, a PhD student in Newcastle University's School of Computing Science and lead author on the paper. Firstly, the current online payment system does not detect multiple invalid payment requests from different websites. This allows unlimited guesses on each card data field, using up to the allowed number of attempts - typically 10 or 20 guesses - on each website. Secondly, different websites ask for different variations in the card data fields to validate an online purchase. This means it's quite easy to build up the information and piece it together like a jigsaw. The unlimited guesses, when combined with the variations in the payment data fields make it frighteningly easy for attackers to generate all the card details one field at a time. Each generated card field can be used in succession to generate the next field and so on. If the hits are spread across enough websites then a positive response to each question can be received within two seconds - just like any online payment. So even starting with no details at all other than the first six digits - which tell you the bank and card type and so are the same for every card from a single provider - a hacker can obtain the three essential pieces of information to make an online purchase within as little as six seconds. How the Distributed Guessing Attack works To obtain card details, the attack uses online payment websites to guess the data and the reply to the transaction will confirm whether or not the guess was right. Different websites ask for different variations in the card data fields and these can be divided into three categories: Card Number + Expiry date (the absolute minimum); Card Number + Expiry date + CVV (Card security code); Card Number + Expiry date + CVV. Because the current online system does not detect multiple invalid payment requests on the same card from different websites, unlimited guesses can be made by distributing the guesses over many websites. However, the team found it was only the VISA network that was vulnerable. MasterCard's centralised network was able to detect the guessing attack after less than 10 attempts - even when those payments were distributed across multiple networks, says Mohammed. At the same time, because different online merchants ask for different information, it allows the guessing attack to obtain the information one field at a time. Mohammed explains: Most hackers will have got hold of valid card numbers as a starting point but even without that it's relatively easy to generate variations of card numbers and automatically send them out across numerous websites to validate them. The next step is the expiry date. Banks typically issue cards that are valid for 60 months so guessing the date takes at most 60 attempts. The CVV is your last barrier and theoretically only the card holder has that piece of information - it isn't stored anywhere else. But guessing this three-digit number takes fewer than 1,000 attempts. Spread this out over 1,000 websites and one will come back verified within a couple of seconds. And there you have it - all the data you need to hack the account. Protecting ourselves from fraud An online payment - or card not present transaction - is dependent on the customer providing data that only the owner of the card could know. But unless all merchants ask for the same information then, says the team, jigsaw identification across websites is simple. So how can we keep our money safe? Sadly there's no magic bullet, says Newcastle University's Dr Martin Emms, co-author on the paper. But we can all take simple steps to minimise the impact if we do find ourselves the victim of a hack. For example, use just one card for online payments and keep the spending limit on that account as low as possible. If it's a bank card then keep ready funds to a minimum and transfer over money as you need it. And be vigilant, check your statements and balance regularly and watch out for odd payments. However, the only sure way of not being hacked is to keep your money in the mattress and that's not something I'd recommend!\", 'T17': \"Unfixable security flaw found in Intel chipset Peter Grad, Tech Xplore The bad news: A security research firm has found that Intel chipsets used in computers over the past five years have a major flaw that allows hackers to bypass encryption codes and quietly install malware such as keyloggers. The worse news: There is no complete fix for the problem. The security firm Positive Technologies announced late last week that the vulnerability, hard-coded in the boot ROM, exposes millions of devices using Intel architecture to industrial espionage and leaks of sensitive information that cannot be detected as they happen. Because the flaw occurs at the hardware level, it cannot be patched. The firm said that a hacker would need direct access to a local network or machine, thus somewhat limiting the possibility of attack. They also noted that one barrier to attack is an encrypted chipset key inside the one-time programmable (OTP) memory, although the unit that initiates such encryption is itself open to attack. Researchers made it clear the threat is a serious one. Since the ROM vulnerability allows seizing control of code execution before the hardware key generation mechanism … is locked, and the ROM vulnerability cannot be fixed, we believe that extracting this encryption key is only a matter of time, Positive Technologies researchers said. When this happens, utter chaos will reign. They warned of forged hardware IDs, extracted digital content and decryption of data on hard drives. Intel's most recent line of chips,10th Gen processors, are not vulnerable to this threat. Intel, which acknowledged it was aware of the problem last fall, issued a patch last Thursday that partially addresses the problem. A spokesman for the company explained that while they cannot secure hardcoded ROM in existing computers, they are trying to devise patches that will quarantine all potential system attack targets. The flaw is located in Intel's Converged Security Management Engine (CSME), which handles security for firmware on all Intel-powered machines. In recent years, Intel has confronted a few serious security flaws such as the Meltdown and Spectre processor vulnerabilities and the CacheOut attack. The latest crisis comes at a time of increasing fierce competition with AMD, developer of the popular Ryzen chip. But perhaps the most serious blow is to Intel's longstanding reputation of excellence. The latest flaw, according to Mark Ermolov, lead specialist of OS and hardware security at Positive Technologies, strikes at the heart of Intel's most vital asset: trust. The scenario that Intel system architects, engineers, and security specialists perhaps feared most is now a reality, Ermolov said. This vulnerability jeopardizes everything Intel has done to build the root of trust and lay a solid security foundation on the company's platforms. More details of Intel's efforts to address the vulnerability can be found on the company's support page: intel.com/content/www/us/en/support/articles/000033416/technologies.html\", 'T18': \"Backscatter breakthrough runs near-zero-power IoT communicators at 5G speeds everywhere Georgia Institute of Technology The promise of 5G Internet of Things (IoT) networks requires more scalable and robust communication systems—ones that deliver drastically higher data rates and lower power consumption per device. Backscatter radios—passive sensors that reflect rather than radiate energy—are known for their low-cost, low-complexity, and battery-free operation, making them a potential key enabler of this future although they typically feature low data rates and their performance strongly depends on the surrounding environment. Researchers at the Georgia Institute of Technology, Nokia Bell Labs, and Heriot-Watt University have found a low-cost way for backscatter radios to support high-throughput communication and 5G-speed Gb/sec data transfer using only a single transistor when previously it required expensive and multiple stacked transistors. Employing a unique modulation approach in the 5G 24/28 Gigahertz (GHz) bandwidth, the researchers have shown that these passive devices can transfer data safely and robustly from virtually any environment. The findings were reported earlier this month in the journal Nature Electronics. Traditionally, mmWave communications, called the extremely high frequency band, is considered the last mile for broadband, with directive point-to-point and point-to-multipoint wireless links. This spectrum band offers many advantages, including wide available GHz bandwidth, which enables very large communication rates, and the ability to implement electrically large antenna arrays, enabling on-demand beamforming capabilities. However, such mmWave systems depend on high-cost components and systems. The Struggle for Simplicity Versus Cost Typically, it was simplicity against cost. You could either do very simple things with one transistor or you need multiple transistors for more complex features, which made these systems very expensive, said Emmanouil (Manos) Tentzeris, Ken Byers Professor in Flexible Electronics in Georgia Tech's School of Electrical and Computer Engineering (ECE). Now we've enhanced the complexity, making it very powerful but very low cost, so we're getting the best of both worlds. Our breakthrough is being able to communicate over 5G/millimeter-wave (mmWave) frequencies without actually having a full mmWave radio transmitter—only a single mmWave transistor is needed along much lower frequency electronics, such as the ones found in cell phones or WiFi devices. Lower operating frequency keeps the electronics' power consumption and silicon cost low, added first author Ioannis (John) Kimionis, a Georgia Tech Ph.D. graduate now a member of technical staff at Nokia Bell Labs. Our work is scalable for any type of digital modulation and can be applied to any fixed or mobile device. The researchers are the first to use a backscatter radio for gigabit-data rate mmWave communications, while minimizing the front-end complexity to a single high-frequency transistor. Their breakthrough included the modulation as well as adding more intelligence to the signal that is driving the device. We kept the same RF front-end for scaling up the data rate without adding more transistors to our modulator, which makes it a scalable communicator, Kimionis said, adding that their demonstration showed how a single mmWave transistor can support a wide range of modulation formats. First author John Kimionis explains that the backscatter breakthrough only requires a single mmWave transistor and much lower frequency electronics, such as the ones found in cell phones or WiFi devices. Credit: John Kimionis, Nokia Bell Labs Powering a Host of 'Smart' IoT Sensors The technology opens up a host of IoT 5G applications, including energy harvesting, which Georgia Tech researchers recently demonstrated using a specialized Rotman lens that collects 5G electromagnetic energy from all directions. Tentzeris said additional applications for the backscatter technology could include rugged high-speed personal area networks with zero-power wearable/implantable sensors for monitoring oxygen or glucose levels in the blood or cardiac/EEG functions; smart home sensors that monitor temperature, chemicals, gases, and humidity; and smart agricultural applications for detecting frost on crops, analyzing soil nutrients, or even livestock tracking. The researchers developed an early proof of concept of this backscatter modulation, which won third prize at the 2016 Nokia Bell Labs Prize. At the time, Kimionis was a Georgia Tech ECE doctoral researcher working with Tentzeris in the ATHENA lab, which advances novel technologies for electromagnetic, wireless, RF, millimeter-wave, and sub-terahertz applications. Key Enabler of Low Cost: Additive Manufacturing For Kimionis, the backscatter technology breakthrough reflects his goal to democratize communications. Throughout my career I've looked for ways to make all types of communication more cost-efficient and more energy-efficient. Now, because the whole front end of our solution was created at such low complexity, it is compatible with printed electronics. We can literally print a mmWave antenna array that can support a low-power, low-complexity, and low-cost transmitter. Tentzeris considers affordable printing crucial to making their backscattering technology market viable. Georgia Tech is a pioneer in inkjet printing on virtually every material (paper, plastics, glass, flexible/organic substrates) and was one of the first research institutes to use 3-D printing up to millimeter-frequency ranges back in 2002.\", 'T19': \"A new genetic algorithm for traffic control optimization Ingrid Fadelli , Tech Xplore Researchers at the University of Technology Sydney and DATA61 have recently developed a new method for optimizing the timing of signals in urban environments under severe traffic conditions. Their approach, presented in a paper pre-published on arXiv, entails the use of genetic algorithms (GAs), a popular computer science technique for solving optimization problems. The idea of this research work came from various drives with my car in the city of Sydney, which is often affected by traffic incidents, causing a large amount of delay and increased road congestion, Tuo Mao, one of the researchers who carried out the study, told TechXplore. This made me wonder: How can we solve this problem with the aid of advanced computer science techniques? Traffic control signals are the most widespread tools to control and manage road traffic in densely populated urban environments. A traffic signal's settings, also known as signal control plan, can affect road traffic significantly, particularly when disruptions first arise. So far, the majority of proposed solutions for traffic control optimization are designed to work under normal traffic conditions. This is because optimizing a traffic light's control plans after an incident has occurred or when traffic is at a peak is a particularly challenging task, especially if multiple lanes or an entire road section are affected. Contrarily to most previous works, Mao and his colleagues set out to achieve traffic signal control optimization under severe traffic conditions using GAs. GAs are a computer science technique inspired by the biological evolution observed in humans, which is designed to naturally select the most optimal solutions among an initial set of possibilities. GAs are commonly used in optimization problems (e.g., finding the best phase duration that would minimise travel time in an intersection) by using bio-inspired functions such as individual mutation, crossover, and selection of best individuals to carry on the best genes of a population—in our case, best signal phases, Mao said. We thought that GAs would be a fantastic solution to solve this problem and decided to use them to generate the optimized traffic signal plans for the incident affected area. The GA developed by Mao and his colleagues essentially explores all possible traffic signal control plans for a given intersection (e.g. the green time for right turn signals, go straight signals, etc.). Its key objective is to minimize the total travel time in an area affected by a road accident by identifying the best combination of signal phases across all intersections within that area. We first generate a large number of traffic control plans, including different phase durations evenly distributed in a large numerical space, which constitute the first generation of individuals from the entire population, Mao explained. Then we apply selection, crossover and mutation in order to introduce more randomness in exploring the space of all possibilities, and select only the best candidates to carry on the optimization in a next generation. Subsequently, the approach devised by Mao and his colleagues evolves the original population for a specific number of generations until the majority of individuals within that population are similar, and it has reached an optimal solution. The GA's final outcome is an optimized traffic signal control plan for all traffic lights in areas affected by road accidents. While past studies have proposed several other traffic signal control optimization techniques, most of these are based on traffic modeling and knowledge-based expert (i.e., heuristic) systems. These systems passively react to observed traffic conditions and are hence unable to actively propose solutions for reducing congestion caused by road accidents. Our method has three key advantages, Mao explained. Firstly, it considers non-recurrent traffic incidents, as we input the incident to the model actively after someone reported it, therefore the traffic signal control plan is aware of the incident and can respond faster. Secondly, it considers the rerouting behavior of drivers by applying a dynamic traffic assignment, which considers the road capacity drop caused by the traffic incidents. Finally, our method is efficient for exploring many possibilities of signal control plans. The researchers evaluated their technique using a four-intersection network designed in AIMSUN, a renowned traffic modeling platform. They constructed three different scenarios in which the GA had to optimize traffic signal timings under both normal conditions and with severe traffic. In these tests, they observed that when traffic signal control plans can be adapted to a change of route by drivers after a traffic accident has occurred, congestion tends to dissipate faster. When using our method, we improved drivers' total travel time by 40.76% compared to applying no response at all (i.e. no control over the signal phasing), Mao said. Our research could provide suggestions for traffic management centres on how to act when a fresh incident happens, as a part of a routine for managing a better traffic response. In the future, the GA developed by Mao and his colleagues could aid the development of more effective traffic control systems. According to the researchers, by advancing their technique's data streaming capabilities and computational performance they could ultimately allow it to automatically optimize traffic signals, actively responding to live road incidents. We are currently applying the method to a more complicated network and even a larger network from the city of Sydney, Mao said. We are also researching to further shorten the computation time and further increase efficiency by coupling the GA with machine learning, which could speed-up the convergence rate towards the best solutions.\", 'T20': \"Software developed to help programmers prototype graphic user interfaces Ulsan National Institute of Science and Technology A new artificial intelligence (AI) system has been developed to help ordinary untrained people to design and create applications and software for smartphones and personal computers. With the help of this system, non-designers can quickly and easily create a user-friendly mobile app. A research team, led by Professor Sungahn Ko in the School of Electrical and Computer Engineering at UNIST has developed a deep learning-based artificial intelligence (AI) system that can provide design recommendations regarding the best layouts through the assessment of graphical user interfaces (GUIs) of the mobile application. The graphical user interface (GUI) is a form of user interface that allows users to interact with electronic devices using graphical icons and other visual indicators. And thus, it is important to create an intuitive, convenient, and attractive user interface and user experience. Indeed, GUIs play a pivotal role in attracting potential customers, yet ordinary untrained people may face challenges while designing user-friendly GUIs. This was solved through artificial intelligence (AI). As smartphones are becoming ubiquitous and pervasive in our daily lives, many things, like shopping, making travel inquiries, and SNS activities can be done from anywhere at any time using mobile apps. As a result, more and more people dream of starting their own businesses or vitalizing businesses using mobile apps. However, developing intuitive and user-friendly applications is a painstaking process for users with no relevant experience and guidance. This is particularly because the visual arrangement of icons and texts becomes more important due to the nature of the mobile environment, such as small screen size. Professor Ko solved this issue with the use of deep-learning-based artificial intelligence (AI) and iterative design process. The new AI system is capable of studying the strengths and weaknesses of the existing GUI design patterns, evaluating the created GUIs for mobile apps, and suggesting alternatives. To this end, the research team conducted semi-structured interviews, based on which they built a GUI prototyping assistance tool, called GUIComp by integrating three different types of feedback mechanisms: recommendation, evaluation, and attention. According to the research team, this tool can be connected to GUI design software as an extension, and it provides real-time, multi-faceted feedback on a user's current design. Additionally, the research team conducted two user studies, in which they asked participants to create mobile GUIs with or without GUIComp, and requested online workers to assess the created GUIs. The experimental results show that GUIComp facilitated iterative design and the participants with GUIComp had better a user experience and produced more acceptable designs than those who did not. We applied deep learning techniques for the design recommendation process and eye-tracker calibration, says Chunggi Lee (School of Electrical and Computer Engineering, UNIST) , the first author of the study. In particular, we used K-nearest neighbor Algorithm and Stacked Autoencoder (SAE) to provide real-time, multifaceted feedback on a user's current design. We have put much effort into visualization to help resolve issues that users with no experience face during the designing process of user-friendly GUIs, says Professor Ko. By securing quality data, this is expected to be applied to the educational sector, such as web development and painting. The findings of this study have been published in ACM Conference on Human Factors in Computing Systems.\"}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import JSON file with article texts, indexed with G01, T01 etc \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "f = open('flattexttopics.json')\n",
    "topics = json.load(f)\n",
    "\n",
    "print(topics)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the twentieth century, it became clear that national economies can make progress only by investing in human capital. Knowledge increases human productivity and creativity, and society can prosper only through human self awareness, understanding of the world around them and by increasing the quality of their lives. In the information age, progress was made through information and technology, with particular emphasis on critical thinking. But in the conceptual economy of the 21st century, influenced by three key developments; over-supply, outsourcing, and automation, it has become clear that critical thinking, even with the help of high technology, is no longer sufficient to make a living or to be competitive in the labor market. In the u0027Value age,u0027 these capabilities need to be complemented by highly conceptual and highly sensitive traits. The integration of critical and creative thinking is a fundamental factor in personal productivity and the essential condition for achieving sustainable economic development. The self assessment of Algebra University College students was designed to investigate the extent to which students integrated critical and creative thinking into their own circumstances and to explore whether there is a correlation between the type of thinking they are more inclined to and their study programs.  \n",
      "\n",
      "The predicted exponential growth of mobile data traffic demands suitable solutions for coping with the high amount of data. Increasing number of Wi-Fi enabled smartphones and Wi-Fi access becoming more widely available is opening the possibility of offloading a large chunk of data traffic from 3G/ 4G mobile networks to Wi-Fi IEEE 802.11. However, the quality of experience must be preserved no matter which network layer is currently engaged. Therefore, the scope of this paper was to determine the usability of the free of charge Wi-Fi available networks and to compare it with the legacy 3G\\4G networks.  \n",
      "\n",
      "This paper explains appliance of social network analysis and data visualization techniques in analysis of information propagation. Context of information (news) propagation through social network is an extremely dynamic and complex area to study. Due to topic actuality and a very small number of works on the similar topic this paper required a comprehensive and systematic approach. Thus, for practical reasons this work is based on the usage of Social Network Analysis (SNA) and visualization of social networking data obtained through Facebook covering 145 + public pages linked to 2.6 million fans. The main hypothesis is based on the premise whether is possible to find any similarities between the real-life social, economic and political entities/processes and online information propagation. The process consists of the development of the underlying model, the retrieval of data, data processing and consequential analysis u0026 visualization which has been elaborated in detail along with the comments related to the methods of application.  \n",
      "\n",
      "We construct an explicit minimal strong Grobner basis of the ideal of vanishing polynomials in the polynomial ring over Z/m for mu003e=2. The proof is done in a purely combinatorial way. It is a remarkable fact that the constructed Grobner basis is independent of the monomial order and that the set of leading terms of the constructed Grobner basis is unique, up to multiplication by units. We also present a fast algorithm to compute reduced normal forms, and furthermore, we give a recursive algorithm for building a Grobner basis in Z/m[x\"1,x\"2,...,x\"n] along the prime factorization of m. The obtained results are not only of mathematical interest but have immediate applications in formal verification of data paths for microelectronic systems-on-chip.  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host 'guacamole.univ-avignon.fr'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "The 'no-check-certificate' link as described in the GUI API documentation is used, \n",
    "which causes a warning. The DBLP data is still accessible despite the SSL warning. \n",
    "    \n",
    "There are several search parameters you can use: size, text queries, and boolean \n",
    "queries. Below are a few example queries from the API documentation, remove hash \n",
    "before query to use (make sure to only have one query active). \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# example of a text query\n",
    "query = {'q':'algebra', 'size':'4'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves doc with id 1494645067\n",
    "# query = {'q':'_id:1494645067', 'size':'1'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves docs which refer to 1584898773\n",
    "# query = {'q':'references:1584898773', 'size':'1'}\n",
    "\n",
    "\n",
    "# example of a boolean query which retrieves all documents which title \n",
    "# contains geometric or with a field of subject name equal to \"\n",
    "# Computer science\" and another including algebra but none equal to Graph\n",
    "# query = {'q':'(( fos.name: \"Computer science\" AND fos.name:*algebra* AND NOT fos.name:Graph ) OR title:*geometric* )', 'size':'10'}\n",
    "\n",
    "# request response from GUI API\n",
    "url = 'https://inex:qatc2011@guacamole.univ-avignon.fr/dblp1/_search?'\n",
    "corpus = requests.get(url, params=query, verify=False)\n",
    "dump = corpus.json()\n",
    "\n",
    "\n",
    "# extract abstracts from ElasticSearch hits\n",
    "hits = dump['hits']['hits']\n",
    "abstracts = []\n",
    "\n",
    "for hit in hits:\n",
    "    print(hit['_source']['abstract'], \"\\n\")\n",
    "\n",
    "\n",
    "#print('Data frame: \\n', df)\n",
    "\n",
    "#print('Dump: \\n', dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
